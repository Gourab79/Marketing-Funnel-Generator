[
  {
    "id": "1lpk8ib",
    "title": "[D] Self-Promotion Thread",
    "body": "Please post your personal projects, startups, product placements, collaboration needs, blogs etc.\n\nPlease mention the payment and pricing requirements for products and services.\n\nPlease do not post link shorteners, link aggregator websites , or auto-subscribe links.\n\n\\--\n\nAny abuse of trust will lead to bans.\n\nEncourage others who create new posts for questions to post here instead!\n\nThread will stay alive until next one so keep posting after the date in the title.\n\n\\--\n\nMeta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",
    "source": "post"
  },
  {
    "id": "n11sdl3",
    "title": "",
    "body": "# Your Creation, Your Proof. Get It Free.\n\nAs a creator, I learned the hard way: your only real responsibility is **proving you made it**. If you're not a big corporation with endless resources for global copyright registration, there's a simple solution.\n\nUse our **free global copyright verification service(i-STAM)** to instantly verify your **images, PDFs, audio, and video files** via our app or web. Website [https://www.i-stam.com](https://www.i-stam.com)\n\n**Here\u2019s how it works:**\n\n* **10 Free Registrations:** Log in and get 10 free points to start.\n* **Need More?** Just send me a direct message (DM) and I'll gladly provide more free points, as many as you need.\n\n**A few quick notes:**\n\n1. **Homegrown:** I built this myself, so the UI/design is basic but effective.\n2. **Permanent Record:** Once registered, your content is saved permanently and can't be changed.\n3. **Mindful Use:** Please use it responsibly to help manage server costs.\n\n(Check the website for full instructions before using the app or web.)",
    "source": "comment"
  },
  {
    "id": "1loqe5e",
    "title": "[D] Monthly Who's Hiring and Who wants to be Hired?",
    "body": "**For Job Postings** please use this template\n\n>Hiring: \\[Location\\], Salary:\\[\\], \\[Remote | Relocation\\], \\[Full Time | Contract | Part Time\\]    and \\[Brief overview, what you're looking for\\]\n\n**For Those looking for jobs** please use this template\n\n>Want to be Hired: \\[Location\\], Salary Expectation:\\[\\], \\[Remote | Relocation\\], \\[Full Time | Contract | Part Time\\]  Resume: \\[Link to resume\\] and \\[Brief overview, what you're looking for\\]\n\n&#x200B;\n\nPlease remember that this community is geared towards those with experience.",
    "source": "post"
  },
  {
    "id": "n0ta1xu",
    "title": "",
    "body": "Want to be Hired: Remote, Salary Expectation:$15/hr, Remote, Contract | Part Time, Resume:\u00a0[https://drive.google.com/file/d/1cAX1kbTSOYgJz3nDGZ2wiQJDoHi4B-j1/view?usp=sharing](https://drive.google.com/file/d/1cAX1kbTSOYgJz3nDGZ2wiQJDoHi4B-j1/view?usp=sharing)\u00a0, looking for GenAI, AI Agents, RAG, NLP, Data Extraction, Chatbot, AI Automation related work.",
    "source": "comment"
  },
  {
    "id": "n0pkp4o",
    "title": "",
    "body": "Want to be Hired: Remote Salary Expectation: $200-$350 / month, Remote, Internship/Part Time  \nResume: [https://drive.google.com/file/d/1Cs7MYGWFFaeKXbYePmyXna3X18PqPpxF/view?usp=sharing](https://drive.google.com/file/d/1Cs7MYGWFFaeKXbYePmyXna3X18PqPpxF/view?usp=sharing)   \nand being interested in the mathematics of DL as well as the implementation, I would love to work in projects where I can implement papers and work on new models",
    "source": "comment"
  },
  {
    "id": "n0u0ov6",
    "title": "",
    "body": "Want to be Hired:\n\nLocation: Egypt\n\nSalary Expectation: Minimum of 5-10 USD per hour\n\nOpen to Remote and Relocation,\n\nOpen to Full Time, Contract, Part Time, and Freelancing\n\nResume:\u00a0[mohamed.sourcing@gmail.com](mailto:mohamed.sourcing@gmail.com)\n\nPublished Project: Multidimensional neural networks as an alternative to the transformers and the attention mechanism\n\n[https://github.com/mohamed-services/mnn/blob/main/paper.md](https://github.com/mohamed-services/mnn/blob/main/paper.md)",
    "source": "comment"
  },
  {
    "id": "1lqgbdk",
    "title": "[D] AI/ML interviews being more like SWE interviews",
    "body": "Have people noticed that AI/ML/DS job interviews now feel more SWE-like? For example, relying more on data structures and algorithms leetcode questions. I\u2019ve noticed in my professional friend groups more people are being asked these questions during the coding interview.",
    "source": "post"
  },
  {
    "id": "n12mu7q",
    "title": "",
    "body": "AI engineer job is just SWE but with AI. And the trending nowadays is just integrating LLMs into existing system.",
    "source": "comment"
  },
  {
    "id": "n12kwb6",
    "title": "",
    "body": "AI/ML/DS is no longer about creative research ideas, but about execution.",
    "source": "comment"
  },
  {
    "id": "n12n02s",
    "title": "",
    "body": "None of the AI research positions I have interviewed for had leetcode, you are probably applying to an \"AI Engineer\" or however they call it nowadays that is SWE for AI (and then it's not surprising they test you for coding)",
    "source": "comment"
  },
  {
    "id": "n12m8hz",
    "title": "",
    "body": "ML has a big SWE component to it. Leetcode type questions is an easy, low-effort way to do a first pass filtering. After that first round, they will start with the actual ML/DS interview.",
    "source": "comment"
  },
  {
    "id": "n13oqu6",
    "title": "",
    "body": "Always have been. The overwhelming majority of people in hiring positions have no idea how to screen candidates, this has been the status quo since data science started as a discipline.",
    "source": "comment"
  },
  {
    "id": "n13o4vr",
    "title": "",
    "body": "Yes!!! I hate it, I have no desire to be a SWE or MLE, I like research and publishing, it's so frustrating when the focus in on leetcode or SWE principles. There are very few RS positions available and they are gatekept by PhDs, and even then they often have leetcode style questions, it's a joke.",
    "source": "comment"
  },
  {
    "id": "n13oh5c",
    "title": "",
    "body": "From personal experience, a big and hard part of ML in industry is infrastructure, tooling and platforms to train, serve and monitor models. Usually even in very large companies very few people are involved in purely just modelling, and a lot of people are involved in building infrastructure for the modelling to take place and eventually go to production reliably.",
    "source": "comment"
  },
  {
    "id": "n12wtz0",
    "title": "",
    "body": "I\u2019ve not had leetcode as much, but I have had a lot of design discussions and ML case study type discussions in interviews. At a certain point MLE requires a lot more large scale design over just ML concepts.",
    "source": "comment"
  },
  {
    "id": "n13m6qz",
    "title": "",
    "body": "gotta be able to actually do the work, not just talk all fancy about it",
    "source": "comment"
  },
  {
    "id": "n14sjza",
    "title": "",
    "body": "The field has changed.\n\n2-3 years ago, our daily routine was defining metrics, collecting data, check quality, finetuning a BERT or a ResNet to perform all sort of NLP/ CV tasks, check the wandb dashboard and dealing with training issue, and iterate, and also deploy the models. ML engineer/ applied researcher is very decentralized.\n\nNow it is a one-model-fit-all scenario. You can prompt to solve almost all NLP and CV problem. It is the era of centralization. You just need some top labs to do data curation, model training, eval and deployment that serve millions of developers. The low supply makes the bar extremely high.\n\n  \nThe research field has been changing too. You will see a lot of maths in older papers pre LLM, and now they're mostly technical report, or prompt engineer paper.",
    "source": "comment"
  },
  {
    "id": "n12p7cz",
    "title": "",
    "body": "Not it\u2019s still about that but also about this",
    "source": "comment"
  },
  {
    "id": "n14dktq",
    "title": "",
    "body": "It's never about creative research ideas at 99% of the places. It's about using AI as part of the software. Think distributed system -- you use it to make your code run faster, but you don't innovate on the actual distributed system protocol. \n\nIn the remaining 1% of places (i.e., leading industrial reserach labs), there is still quite good research going on, though there is a general shift toward applied stuff and closed models.",
    "source": "comment"
  },
  {
    "id": "n13xsam",
    "title": "",
    "body": "Yes, sadly it\u2019s true.",
    "source": "comment"
  },
  {
    "id": "n12ztzl",
    "title": "",
    "body": "Meta FAIR RS ask/ leetcode, also openai research scientists. It\u2019s not hard per se its just medium ish problems but it\u2019s expected you know average lc.\n\nI have yet to see a top company (meta fair, deepmind, salesforce research, amazon research, tiktok, openai, anthropic, etc) RS not ask leetcode but the bar is not high on those",
    "source": "comment"
  },
  {
    "id": "n146ot5",
    "title": "",
    "body": "What discipline? It's like 20 different disciplines, depending on the job / department / company / manager.",
    "source": "comment"
  },
  {
    "id": "n14fqyi",
    "title": "",
    "body": "even in the top research labs the larger research projects taking most of the compute resources are engineering projects on scaling up the training of large models.",
    "source": "comment"
  },
  {
    "id": "n131knp",
    "title": "",
    "body": "Hi! For automated reasoning positions at AWS, the leetcode phase is super simple. Only two questions of the easy level. They are more focused in the interview and deep (theoretical) questions for which you need a PhD level in the topic",
    "source": "comment"
  },
  {
    "id": "n131vcf",
    "title": "",
    "body": "I didn't do leetcode for Amazon and Microsoft, but I guess it depends on the team",
    "source": "comment"
  },
  {
    "id": "n135e6v",
    "title": "",
    "body": "Good to know!",
    "source": "comment"
  },
  {
    "id": "n132ax5",
    "title": "",
    "body": "oh yea team based hire maybeee and perhaps the company wide policy is more advisory",
    "source": "comment"
  },
  {
    "id": "1lqjgjz",
    "title": "[D] AAAI-2026 2 phase review discussion",
    "body": "AAAI-26' Two-phase reviewing for the Main Track:\n\n[https://aaai.org/aaai-launches-ai-powered-peer-review-assessment-system/](https://aaai.org/aaai-launches-ai-powered-peer-review-assessment-system/)\n\nPhase 1: Two reviews supplemented by one AI-generated, non-decisional review.\n\nPhase 2: Additional reviews for papers not rejected in Phase 1.\n\n**Author response after Phase 2, only for papers not rejected in Phase 1.**\n\nSo the phase 1 will be reviewed by AI? and it will decide whether ur paper is accepted for phase 2 or rejected? Is it correct? Or the AI will just check the formatting and minor factors?\n\nEdit : They also said (but why the use of AI)  \nThe pilot program will thoughtfully integrate LLM technology at two specific points in the established review process:\n\n1. Supplementary First-Stage Reviews: LLM-generated reviews will be included as one component of the initial review stage, providing an additional perspective alongside traditional human expert evaluations.\n2. Discussion Summary Assistance: LLMs will assist the Senior Program Committee (SPC) members by summarizing reviewer discussions, helping to highlight key points of consensus and disagreement among human reviewers.\n\n",
    "source": "post"
  },
  {
    "id": "n13b1b4",
    "title": "",
    "body": "Whaaat?",
    "source": "comment"
  },
  {
    "id": "n13eiam",
    "title": "",
    "body": ">>The pilot program will provide supplementary information in the form of AI-generated reviews and summaries that do not contain any ratings or recommendations. AI-generated supplementary reviews will not play any formal role in the review process, except being visible to the assigned reviewers (after they submit their own reviews), area chairs, and appropriate members of the Program Committee during the paper discussion phase. In addition, AI-generated summaries of reviewer discussions will also be used to assist Senior Program Committee members in their decision making.",
    "source": "comment"
  },
  {
    "id": "n13pfu0",
    "title": "",
    "body": "Lol what a joke. At least they are transparent about it",
    "source": "comment"
  },
  {
    "id": "n142z5g",
    "title": "",
    "body": "what a horrible decision. This degrades the quality of AAAI in my eyes.",
    "source": "comment"
  },
  {
    "id": "n13je73",
    "title": "",
    "body": "I couldn\u2019t believe this was real so I looked it up: https://aaai.org/aaai-launches-ai-powered-peer-review-assessment-system/\n\nWhat are we doing to ourselves, seriously?",
    "source": "comment"
  },
  {
    "id": "n13qj4v",
    "title": "",
    "body": "Thank you sm for the clarification. Still, this does not really make sense to me \n\n  \nIn usual cases meta reviewers (many if not all) don't see the rebuttals of the authors properly and depend upon the initial ones. umm! understandable coz it can be due to higher submissions and low quality reviews (meta-reviewers are also tired), authors are also the reviewers   \nSo, how do we expect them to check all the llm generated reviews.   \nAlso many reviewers do use AI to review, so it will be double AI (crying) if the author is unlucky.",
    "source": "comment"
  },
  {
    "id": "n13r800",
    "title": "",
    "body": "Is this created by the marketing team?\n\n> Cutting-Edge Methods with Rigorous Safeguards\n\nWhat exactly are these \"cutting-edge methods\"? What specific safeguards are in place?\n\nThere is zero transparency here. Yet graduate students are expected to exhaust themselves trying to publish papers where even minor mistakes can end their careers.",
    "source": "comment"
  },
  {
    "id": "1lqedrt",
    "title": "[D] Paper with code is completely down",
    "body": "Paper with Code was being spammed (https://www.reddit.com/r/MachineLearning/comments/1lkedb8/d\\_paperswithcode\\_has\\_been\\_compromised/) before, and now it is compoletely down. It was also down a coupld times before, but seems like this time it has lasted for days. (https://github.com/paperswithcode/paperswithcode-data/issues)\n\n",
    "source": "post"
  },
  {
    "id": "n126pdx",
    "title": "",
    "body": "Is there anything we can do to help these people?",
    "source": "comment"
  },
  {
    "id": "n13nztz",
    "title": "",
    "body": "Who owns paperswithcode? Both of the major contributors I could find appear to be very inactive",
    "source": "comment"
  },
  {
    "id": "n12shss",
    "title": "",
    "body": "Aww not again T.T",
    "source": "comment"
  },
  {
    "id": "n12889a",
    "title": "",
    "body": "I think restoring a clone? Their data is open I think, so making a mirror website might be possible",
    "source": "comment"
  },
  {
    "id": "n13e2nk",
    "title": "",
    "body": "Its up again!",
    "source": "comment"
  },
  {
    "id": "n14litb",
    "title": "",
    "body": "Its down again",
    "source": "comment"
  },
  {
    "id": "1lqhoe2",
    "title": "[D] Are NLP theory papers helpful for industry research scientist roles?",
    "body": "Currently I'm quite interested in NLP theory, and have some questions about how to make them count for RS roles in industry roles at top AI labs.  \n(1) Does the number of papers help? My impression is that having many papers that are \"purely theoretical\" may not help that much, and AI labs will only count the number of \"relevant papers\" (and exclude those that are less relevant).   \n(2) If the theory paper also yields strong empirical results, is it important to frame it as an empirical paper (and maybe put the theory in the appendix)? This could compensate for any perceived weakness with theoretical work.  \n(3) What topics in language/vision models are particularly relevant in industry? Efficiency of LLMs is one priority; MoE, sparse attention & structured sparsity, are two approaches to efficient LLMs.",
    "source": "post"
  },
  {
    "id": "n13hdza",
    "title": "",
    "body": "1) Number of papers is more of a screening metric that doesn't matter once you reach the interview stage. After that, as you said the relevant papers (and relevant experience from internships, open-source work, etc.) are more important.  \n2) Having both strong theory and empirical results is, in my opinion, better than only strong empirical results. I wouldn't go out of my way to \"hide\" the theory. When you discuss your papers on an interview or during a research talk you can still frame the work however you think is best aligned with the team you're interviewing for. \n\n3) There are many relevant topics across the entire pipeline from data collection/curation to (agentic) model deployment/inference, but it totally varies from team to team. One team might care a lot about efficiency while another is only interested in exploring reasoning/RL techniques. If you want to best position yourself for one of these roles, I'd advise to focus on a particular niche where you can distinguish yourself from the average applicant. Also keep in mind that the field is moving so fast that the hot topics today might no longer be hot 6 months from now, so just do something you're interested in and think you can do well, and it will probably work out better than just chasing trends and drowning in the competition",
    "source": "comment"
  },
  {
    "id": "n13ua8k",
    "title": "",
    "body": "If # papers is a screening metric, roughly would they expect to clear this stage? Thanks.",
    "source": "comment"
  },
  {
    "id": "n13x2a8",
    "title": "",
    "body": "As a disclaimer, I haven't personally been on the hiring side - this is my anecdotal experience going through the process myself (and hearing from friends / colleagues). I also don't want to discourage you in any way. Give it a shot and see what happens!\n\nOverall, it's hard to say because #papers is not the only factor being considered.  E.g. if you come from a top uni/group with a well-known advisor, you'll more easily land interviews than if you're at a no-name lab. Likewise, having prior internship experience at top groups (GDM, Meta, Nvidia, etc.) is extremely important/helpful. Also, the state of the market is crucial; currently, senior researchers are in high demand but PhD grads are a dime a dozen so you need a bit more luck and patience.\n\nAs a rough guideline though, 3-5 first-author top conference papers seems to be the minimum requirement for competitive roles nowadays. If you have more, that's great, but realistically you only get to talk about your best/most relevant 1-3 papers during the interview process. If you have less, you can still make it, but you probably shouldn't just apply through job portals, also consider non-RS roles like MLE/RE, and find other ways to signal value like internships and open-source.\n\nLastly, you can usually skip the screening stage through networking (having personal connections at companies, cold emailing, talking to recruiters at conferences) or having recruiters directly approach you if you have an online presence (linkedin, X, blog, etc.), which in my experience is the most straightforward way of landing interviews.",
    "source": "comment"
  },
  {
    "id": "n13xulv",
    "title": "",
    "body": "Thanks, that's helpful. Do they need to be at top conferences, or do good journals (eg. TMRL, JMLR, TACL) count too? Can it be a mixture of good conference/journal publications?",
    "source": "comment"
  },
  {
    "id": "1lq9dh5",
    "title": "[D] Machine Learning Cheat Sheet Material",
    "body": "* [Linear Algebra Cheat Sheet](https://macro.com/app/pdf/5aa2375d-a8f6-4430-93f9-a7e4aba55690)\n* [Super VIP Cheatsheet: Artificial Intelligence](https://macro.com/app/pdf/5be153e6-6dd3-4eef-adbf-554d53afa3ed)\n* [VIP Cheatsheet: Transformers and Large Language Models (LLMs)](https://macro.com/app/pdf/d8770868-9cbe-4bf8-abe0-2988f39344d9)\n* [VIP Cheatsheet: Deep Learning](https://macro.com/app/pdf/ab4efb6c-6e71-4836-85bc-4841e26312c1)\n* [Super VIP Cheatsheet: Machine Learning (ML)](https://macro.com/app/pdf/a8b3033b-c823-4715-ab2c-24ed9eca98ef)\n* [Machine Learning Cheat Sheet](https://macro.com/app/pdf/79b5f468-d65c-4c03-b9b6-7c117581e677)\n* [ML Cheatsheet Documentation](https://macro.com/app/pdf/65f5ae92-7f08-4869-8d53-cc81ed0fabc2)\n* [Machine Learning: UC Berkeley Intro to ML Course Notes](https://macro.com/app/pdf/ea86a4d6-433a-4eeb-bf40-985b871afcc8)\n* [Machine Learning: A Probabilistic Perspective](https://macro.com/app/pdf/a36b8fd4-f70e-4a41-b18f-9436c2806019)",
    "source": "post"
  },
  {
    "id": "1lqmzlh",
    "title": "[P] Looking for Thesis Ideas: Extracting Pipelines",
    "body": "Hi everyone,\n\nI\u2019m currently starting my Master\u2019s thesis, and I\u2019ll be working on the topic of automatically extracting pipelines from scientific research papers. The core idea is to extract how researchers actually process, model, and evaluate their data, by turning the \u201cMethods\u201d section into a structured, step-wise list/workflow.\n\nSomeone under the same advisor already did a project on this (using LLMs + GROBID + semantic retrieval), so I\u2019m looking for ways to extend or approach the problem differently, ideally in a way that\u2019s both research-worthy and practically useful.\n\nOne initial idea I\u2019ve brainstormed include was\n- Extracting pipeline steps and Reconstructing directed graphs of those (not just flat lists).\n\nHave any of you worked on something similar? Or seen cool papers/tools that deal with pipeline reconstruction, reproducibility, or scientific method understanding?\n\nWould love to hear:\n- Ideas you think are worth exploring\n- Common pain points in method reporting that could be addressed\n\nThanks in advance!\n",
    "source": "post"
  },
  {
    "id": "1lqoxbd",
    "title": "[D] Looking for Advice from an ML Engineer or AI Architect \u2013 Facing Some Challenges",
    "body": "I\u2019m working on a project in the conversational AI space and have hit a few roadblocks related to how the system handles context, tone, and scalability. I\u2019d really appreciate a quick chat with someone who has experience in building or architecting intelligent assistants or similar ML systems.\n\nIf you're open to a brief conversation or can point me in the right direction, I\u2019d be super grateful. Happy to share more details in DMs.",
    "source": "post"
  },
  {
    "id": "n14qs09",
    "title": "",
    "body": "Why not start with building out your current solution and then iterating on it/showing us where your perceived bottleneck is?\n\nMost of it is just organisation related work, it should become apparent once you draw it out.",
    "source": "comment"
  },
  {
    "id": "1lppvk8",
    "title": "[D] How will LLM companies deal with CloudFlare's anti-crawler protections, now turned on by default (opt-out)?",
    "body": "Yesterday, [Cloudflare had announced](https://blog.cloudflare.com/content-independence-day-no-ai-crawl-without-compensation/) that their protections against AI crawler bots will be turned on by default. Website owners can choose to opt out if they wish by charging AI companies for scraping their websites (\"pay per crawl\").\n\nThe era where AI companies simply recursively crawled websites with simple GET requests to extract data is over. Previously, AI companies simply disrespected robots.txt - but now that's not enough anymore.\n\nCloudflare's protections against crawler bots are now pretty sophisticated. They use generative AI to produce scientifically correct, but unrelated content to the website, in order to waste time and compute for the crawlers (\"[AI Labyrinth](https://blog.cloudflare.com/ai-labyrinth/)\").  This content is in pages that humans are not supposed to reach, but AI crawler bots should reach - invisible links with special CSS techniques (more sophisticated than `display: none`), for instance. These nonsense pages then contain links to other nonsense pages, many of them, to keep the crawler bots wasting time reading completely unrelated pages to the site itself and ingesting content they don't need.\n\nEvery possible way to overcome this, as I see it, would significantly increase costs compared to the simple HTTP GET request recursive crawling before. It seems like AI companies would need to employ a small LLM to check if the content is related to the site or not, which could be extremely expensive if we're talking about thousands of pages or more - would they need to feed every single one of them to the small LLM to make sure if it fits and isn't nonsense?\n\nHow will this arms race progress? Will it lead to a world where only the biggest AI players can afford to gather data, or will it force the industry towards more standardized \"pay-per-crawl\" agreements?",
    "source": "post"
  },
  {
    "id": "n0wog2f",
    "title": "",
    "body": "Scrapers will always win. At the end of the daythe content has to be accessible by people. So cloudflare is inherently disadvantaged in the arms race. And honestly you can't expect to have your cake and eat it too. If you want people to be able to easily access your content then it has to be easily accessible. If it's easily accessible by people then it's easily scrapable. You can try to build in these protections and safeguards but at the end of the day a motivated actor will figure out how to exploit that inherent weakness in your defense.",
    "source": "comment"
  },
  {
    "id": "n0wolpe",
    "title": "",
    "body": "I thought cloudflare is trying to raise capital \n\nLLM companies will pay cloudflare \nBe it a subscription fee , shares or buying out the company",
    "source": "comment"
  },
  {
    "id": "n0xyc1m",
    "title": "",
    "body": "Just one more place Google has a huge advantage.  Not going to prohibit Google from crawling your site as you kind of have to be in the Google search index.",
    "source": "comment"
  },
  {
    "id": "n0wy8q7",
    "title": "",
    "body": "That reminded me:\n\n- Why can't we make good bear proof trash containers?\n- Because there is considerable overlap between smartest bears and stupid people.\n\n\nThe game is futile. If people can tell the difference between valid content and a honey pot, the AI crawler will surely be able to do the same.",
    "source": "comment"
  },
  {
    "id": "n0ygjrm",
    "title": "",
    "body": "Companies will require governments to require citizens digital authentication for websites at each connection, something like this",
    "source": "comment"
  },
  {
    "id": "n0xlo3s",
    "title": "",
    "body": "If we can\u2019t imagine this happening 15 years ago, when Google first started doing the one click, how are we supposed to imagine this working now?\n\nI literally cannot imagine, cloudflare suing OpenAI and winning. Just like NYT or wtvr new source it was, they had a legitimate case for copyright yet nothing happened.",
    "source": "comment"
  },
  {
    "id": "n0xacl0",
    "title": "",
    "body": "behavioural cloning on mouse movement for the are you human check, selenium -> screengrab -> OCR.\n\ncheaper than using an LLM to post process the scrape.",
    "source": "comment"
  },
  {
    "id": "n0xgrro",
    "title": "",
    "body": "Get only works for static pages anyway. Most modern crawlers like crawl4ai or firecrawl actually render the pages to get the dynamic content like a normal user and cloudflare can't do shit.",
    "source": "comment"
  },
  {
    "id": "n0xz75p",
    "title": "",
    "body": "I guess people will have to improve sample efficiency. I've done experiments on ideas in this direction. I'm sure there are people who have been trying for 20 years, or for whom it's their primary research interest. I don't think my, maybe not ad-hoc stuff, but the stuff I came up with in a week worked badly, so presumably there are a bunch of ideas that work great.\n\nThe big problem for LLMs though, is when something is actually obscure. Then you're in hallucination land even with the best models, and overcoming that can't be done simply with more data. It needs something else, maybe having the model prepare 'tomorrow I will make requests about x, study these repositories' and then the model developers have some script that automatically generates things the model can practice on relating to things in that repository, until it's well prepared and knows every detail of it.",
    "source": "comment"
  },
  {
    "id": "n0zal4r",
    "title": "",
    "body": "Queue browser extensions that scrape pages people are actually looking at, under the guise of removing ads or something.\u00a0",
    "source": "comment"
  },
  {
    "id": "n11n1rg",
    "title": "",
    "body": "The industry has moved past pretraining on internet data. If we didn't get a single byte more from web crawls it wouldn't change the trajectory one bit.",
    "source": "comment"
  },
  {
    "id": "n11vy6v",
    "title": "",
    "body": "From what they said, there is no labyrinth, they just throw out an HTTP 402 code. The web was already made to handle this sort of thing, there was just never a concrete reason since the whole microtransaction driven concept from the early 2000s never took off.",
    "source": "comment"
  },
  {
    "id": "n122nln",
    "title": "",
    "body": "Is there any way for a human to look through this? And barring the fact that IP profiling might stop real users.",
    "source": "comment"
  },
  {
    "id": "n0wwqa3",
    "title": "",
    "body": "This method seems potentially dangerous to website owners. If you get a scraper stuck looking at useless pages, it can get stuck in some infinite loop, especially unsophisticated scraper, and end up costing you more, not less. \n\nHackers can always adapt, but at what point does this all become too sleazy, or just not worth it financially for public companies? This isn't exactly the classic cybersecurity cat-and-mouse. \n\nOn the other hand,  I have a hard time believing pay to scrape will catch on. Most likely, if this succeeds, there will just be less scraping.",
    "source": "comment"
  },
  {
    "id": "n0zf8is",
    "title": "",
    "body": "Arms race",
    "source": "comment"
  },
  {
    "id": "n0wvj3a",
    "title": "",
    "body": "I disagree.  Have you ever tried to do comprehensive content scrape for Microsoft, Google, or Meta for the public content they don't want to get scraped?  It's easy to scrape small scale, but the becomes impossible as you scale up.\n\nSimilarly Cloudflare turns the tables in the arms race. They have the scale scale, legal, and technology advantages smaller anti-scrapers never had. \n\n1. Any big player, OpenAI, Microsoft, Meta, Google, ... will be shut down. Legal threats are most effective against them and  restrict them already. They scrape in massive scale, and will be detected quickly.  \n2. Cloudflare has scale and tech advantage against scrappy small scrapers who don't care about legal threats. Their volume, and patterns and figgerprints are easier to detect analyzing millions of sites. \n\n( Let's be aware of perfect solution fallacy, in this case  \"If some  scrapers get past some time it does not work.\")",
    "source": "comment"
  },
  {
    "id": "n0zohb8",
    "title": "",
    "body": ">At the end of the daythe content has to be accessible by people\n\nThe AI Labyrinth link above describes that CloudFlare would only deploy this decoy material when they detect unauthorized scraping. It isn't as crude as just including hidden links on every page (which they also discuss as easily ignored by said bots).",
    "source": "comment"
  },
  {
    "id": "n10a717",
    "title": "",
    "body": "the race never ends until someone drops out. As long as improvements are made to protect against scraping it's a good thing.",
    "source": "comment"
  },
  {
    "id": "n12i6d1",
    "title": "",
    "body": "Users don't need to be served content nearly at the same rate as scrapers. If you can limit bot access to the level of normal user it effectively kills large scale scraping, or at least makes it a very long and inefficient way of data acquisition, discouraging it.\n\nEmphasis on IF, this might not be effective for long, but it certainly will take some load off their servers for a bit.",
    "source": "comment"
  },
  {
    "id": "n10ien5",
    "title": "",
    "body": "You have to create a problem first, before you can charge for the solution.",
    "source": "comment"
  },
  {
    "id": "n0zt8cc",
    "title": "",
    "body": "I don't understand how Google market cap is relatively much lower compared to the top 4.",
    "source": "comment"
  },
  {
    "id": "n0zsv2v",
    "title": "",
    "body": "the objective isn't to stop it completely, but to rate limit it.",
    "source": "comment"
  },
  {
    "id": "n0yhp9o",
    "title": "",
    "body": "Yeah but if both the bear and a human open up a trash can, the bear will eat the trash while the human will probably pinch their nose and walk away. Filling hidden links with AI generated slop to both trap crawlers and poison the models that are training on content they return won\u2019t hurt users as much as it will hurt models. I think the main distinction I have is that you can\u2019t just trap them, you also have to create the poisoning risk.",
    "source": "comment"
  },
  {
    "id": "n10gmlw",
    "title": "",
    "body": "I love this metaphor and thank you for sharing it. In this case, though, it seems more like a (\\*human) imperceptible faint odor of fish that is always just around the next corner.",
    "source": "comment"
  },
  {
    "id": "n0xg1cg",
    "title": "",
    "body": "Completely missed the point huh? The costs to this a setup like this would be insane",
    "source": "comment"
  },
  {
    "id": "n0ybvbz",
    "title": "",
    "body": "This is Cloudflare, so the scraper would get served pages from the CDN's server not yours.",
    "source": "comment"
  },
  {
    "id": "n0wyovy",
    "title": "",
    "body": "Less scraping is an unfavorable outcome for both LLM companies and their end users, so I find it hard to believe they will just accept this. Most data is already scraped, but you always need new data.",
    "source": "comment"
  },
  {
    "id": "n0wxh2u",
    "title": "",
    "body": "yeah its a fair point they have the resources to make it more difficult or expensive but my impression (as an non expert) has been that the legal side of things tends to favour scraping if it's publicly accessible information. id say where my threshold is for avoiding the perfect solution fallacy is whether or not i personally can feasibly do it. maybe i'm more experienced in this area than average but idk i've just never seen anything that can appear on google not be scrapable. i mean the reality is that many places want to be scraped (e.g. by google just look at SEO and paid ads)",
    "source": "comment"
  },
  {
    "id": "n0wzkee",
    "title": "",
    "body": "Isn't it likely that OpenAI, for instance, have a team that is supposed to find ways to prevent their crawlers from being detected or blocked? I agree that smaller companies may struggle immensely, but large AI companies seem to have the resources to find workarounds.",
    "source": "comment"
  },
  {
    "id": "n0ycm07",
    "title": "",
    "body": "> I disagree. Have you ever tried to do comprehensive content scrape for Microsoft, Google, or Meta for the public content they don't want to get scraped? It's easy to scrape small scale, but the becomes impossible as you scale up.\n\n\nset up daemons to run on a couple hundred residential IPs to scrape, configure them to rotate the IPs on the modems when blocked or at an interval. This is child's play for a company with the resources of OAI or Anthropic and hundreds of employees with their own connections.",
    "source": "comment"
  },
  {
    "id": "n0zl3r9",
    "title": "",
    "body": "Yes I have. The challenge at the end of the day is to determine what they're looking at/running etc to determine you're a bot. Then spoofing that. They can never win. They can just try more and more things. But they'll never win.",
    "source": "comment"
  },
  {
    "id": "n13hbtv",
    "title": "",
    "body": "Bro meta bragged about using millions of pirated books to train llama and didnt get any consequences there. Big players are NOT afraid of legat threats in today's world, they are simply too big.",
    "source": "comment"
  },
  {
    "id": "n10flqi",
    "title": "",
    "body": "Because \"move fast and break things\" does not scale horizontally. They absolutely should have more market share, but Gemini app related launches have been Jr Dev levels of absurd at times.  There was a period of time a month or so ago where chat history entries were actually being deleted if you engaged with the chat in some way. I only know it happened with exporting research to document, because I don't even attempt to interact with Gemini like I would ChatGPT. But, I assume it was happening with other chats as well. If Claude or ChatGPT let that happen it would be viewed as a catastrophic failure and breech of user trust. Gemini hasn't even established a high enough bar for that to be out of line.\n\nEdit: This is alongside various \"unable to connect to server\" errors, along with terrible defaults for error handling from a basic UI/UX perspective. I can gauge how long my NotebookLM podcast is going to be based on when and how badly the Material spinner starts glitching. These are the small things that get lost in the sprawl, but I assume it permeates the API and cloud layers as well. Wasn't one of the more recent outages literally in part due to not having exponential backoff?",
    "source": "comment"
  },
  {
    "id": "n0zorqp",
    "title": "",
    "body": "So the article OP linked actually covers the \"poison model\" thing. CloudFlare explicitly doesn't want to do this, so all the served content is actual real scientific content, not fake slop. Any AI trained on it wouldn't incorporate misinformation, they just wouldn't get information about the website in question.",
    "source": "comment"
  },
  {
    "id": "n10gwch",
    "title": "",
    "body": "Thank you. So many of the responses do not take scale into account. Just \"I could easily whip up a script or prompt\". If a human is doing this, it defeats the purpose.",
    "source": "comment"
  },
  {
    "id": "n10hznf",
    "title": "",
    "body": "If we were talking about some humanity driven NGO, sure. But, there is no overall alignment there for companies that have built their product off of the back of public data and then turn around and charge for it by default. Don't get me wrong, I absolutely love LLMs and the large companies that have enabled their success. I just don't trust that the instant they start facing model collapse or recursive ingestion (whatever the correct formal term is), they won't push this very narrative.",
    "source": "comment"
  },
  {
    "id": "n0wzec6",
    "title": "",
    "body": "Just like Google vs black-hat SEOs, Cloudflare can have team to change things daily and evolving AI Labyrinth   poisoning content.",
    "source": "comment"
  },
  {
    "id": "n10akig",
    "title": "",
    "body": "content doesn't have to be public in a legal sense",
    "source": "comment"
  },
  {
    "id": "n0x27lv",
    "title": "",
    "body": "More resources can't overcome the limits of  legally available technological means and scale required. \n\nCriminal botnets  can use techniques OpenAI never can use and Cloudflare fights  against them daily.\n\nCloudflare knows the IP addresses belonging to data centers, and residential IP proxies around the world. OpenAI can't rent and rotate addresses fast enough to hide the scale they need without going completely criminal.",
    "source": "comment"
  },
  {
    "id": "n0yw7sl",
    "title": "",
    "body": "Even this approach would be detected almost immediately with modern anomaly detection and log analysis methods... which Cloudflare is almost certainly doing.",
    "source": "comment"
  },
  {
    "id": "n0zauso",
    "title": "",
    "body": "More like a couple hundred thousand daemons. And the scraping behavior is modeled after the person using the computer, because they opted into that to get a game or something.\u00a0",
    "source": "comment"
  },
  {
    "id": "n0zs6mz",
    "title": "",
    "body": "ip address rate limiting?",
    "source": "comment"
  },
  {
    "id": "n10lmyz",
    "title": "",
    "body": "that Google are bad at software engineering is... surprising to say the least.",
    "source": "comment"
  },
  {
    "id": "n0zt2lf",
    "title": "",
    "body": "Right, and they state that their intent is to prevent misinformation. It\u2019s odd to me that they\u2019re both attempting to thwart AI bots but also not be too mean to them. But what\u2019s to stop anyone else who doesn\u2019t have that intention? I view it as much stronger than just the labyrinth.",
    "source": "comment"
  },
  {
    "id": "n0x04x9",
    "title": "",
    "body": "i just dont believe that it won't be a prompt away to work around",
    "source": "comment"
  },
  {
    "id": "n0ybr1u",
    "title": "",
    "body": "You say that like it would be the first time a large corporation has done something incredibly illegal... If poisoning and killing millions of people hasn't stopped other corporations in the past, you think developing sophisticated means of hiding their illegal access to content that is essential for their product is going to stop them if the benefit is worth more than the cost of being found out? You just do it all under a shell company. Pay someone else to take the fall if needed, pass the data off to your own company. Corporations have been using tactics like this for nefarious purposes forever and continue to do so to this day. It's a little naive to think they'll let something so damaging slow them down.\n\nBut to be honest, they might have enough data already. Generated training data can be of the same or even higher quality as training data compared to data scraped off the internet at this point. Too little too late to be honest. And if they do still need to scrape, you think they're beyond shaking hands with entities in China or wherever that are untouchable legally and essentially impossible to trace back OpenAI or whoever? There are so many ways around these hurdles. Cloudflare's attempts are akin to locking up your luggage at the airport. It's a deterrent; it might stop someone from a committing a crime of opportunity or *slow down* someone motivated, but it won't *stop* anyone who is truly motivated to steal (from) your luggage.",
    "source": "comment"
  },
  {
    "id": "n0zsksx",
    "title": "",
    "body": "can you not game the anomaly detection itself? if it's the pattern you can vary that.\n\nif it's about rate limiting the ip addresses, e. g. you can recycle those on those 200 residential in the example provided.\n\njust playing devil's advocate to learn more.",
    "source": "comment"
  },
  {
    "id": "n10bdzf",
    "title": "",
    "body": "It actually isn't detected if they're just random residential IPs and not on the same ASN or anything and you use a sane request pattern. It's really not hard to scrape a site.",
    "source": "comment"
  },
  {
    "id": "n114007",
    "title": "",
    "body": "vpns are a very very basic tool in a scrapers toolset.",
    "source": "comment"
  },
  {
    "id": "n145vlb",
    "title": "",
    "body": "Deep mind is bad at software engineering because they don\u2019t ask leetcode lol",
    "source": "comment"
  },
  {
    "id": "n0zvrez",
    "title": "",
    "body": ">  It\u2019s odd to me that they\u2019re both attempting to thwart AI bots but also not be too mean to them\n\nI don't see it as odd. The data will likely go into some model at some point. It won't make the models obviously worse (assuming the fake data is a small proportion of the overall training material on that subject), but could result in folks getting incorrect responses more often. So, if the data's going to be used in something released to the public down the line anyway, you might as well have it be real data, just irrelevant.\n\n> But what\u2019s to stop anyone else who doesn\u2019t have that intention? \n\nI don't understand what you mean. What's to stop someone else poisoning crawler results? Nothing, except they'd need the global reach of CloudFlare to do it on an automated and vast scale.",
    "source": "comment"
  },
  {
    "id": "n0zrk9c",
    "title": "",
    "body": "a prompt away? \nin what way? how is the llm even related to this?",
    "source": "comment"
  },
  {
    "id": "n144ra8",
    "title": "",
    "body": "If cloudflare forces scrapers to rely on LLMs they already won because that makes scraping *extremely* expensive",
    "source": "comment"
  },
  {
    "id": "n10eo3m",
    "title": "",
    "body": "This is programmatically difficult. Let alone a hyper-derivative like a prompt. Spicy take.",
    "source": "comment"
  },
  {
    "id": "n0zs1kt",
    "title": "",
    "body": "this isn't just for training.\n\nrag contexts e.g. perplexity AI-style web searches",
    "source": "comment"
  },
  {
    "id": "n10axv2",
    "title": "",
    "body": "Yea, you can. As people have pointed out here cloudflare errs on the side of public availability and not blocking. All of the people assuming that their bot detection is omnipotent have clearly never tried scraping a cloudflare site. It's not that hard. You can scrape a larger site with a single IP if you have some patience.",
    "source": "comment"
  },
  {
    "id": "n11fi4w",
    "title": "",
    "body": "but the vpn up addresses are all blacklisted or rate limited to unusable scraping levels?",
    "source": "comment"
  },
  {
    "id": "n0zvz7r",
    "title": "",
    "body": "> The data will likely go into some model at some point.\n\nThen what\u2019s the point of even trying to thwart the bots?",
    "source": "comment"
  },
  {
    "id": "n10wtlm",
    "title": "",
    "body": "There are poisoning attacks that have been identified that can have a much greater impact on the model performance than the volume of data would imply. Some context that helps when understanding this:\n\n- There is research that shows large variance in how much a model learns from a certain document, chunk, even token. There is even research that shows certain data elements have very little or negative value in training.\n- While it's a myth that \"we don't know how these models work\", the detailed mechanics are much too large to interpret and the most promising approach right now is to use AI models to interpret the details of AI neural networks to understand their inner workings at detail + scale. Until that field matures, it is likely that these types of attacks can still be effective.",
    "source": "comment"
  },
  {
    "id": "n0zs5l3",
    "title": "",
    "body": "\"How do I scrape x website without being detected as a bot?\"",
    "source": "comment"
  },
  {
    "id": "n10h9l2",
    "title": "",
    "body": "It's not actually worst case just use pyautogui to use your computer to open a browser and click around to access the site you want to scrape.",
    "source": "comment"
  },
  {
    "id": "n11d698",
    "title": "",
    "body": "That's true. I feel like something has to give there. AI is the future whether people like it or not. If AI can't access your website people won't be accessing it either. I hardly browse the web any more. Why would I? Other than a select few specific cases. All the toddlers growing up with AI will probably hardly know what a web browser is, or at least their children won't. Web sites aren't at all an efficient format. Riddled with ads, SEO hacking, half of the content or more in 10 years time will be AI generated anyway... So you're going to go browsing to read something that AI you could just ask could write for you almost instantly?\n\nThat's going to be the thing... Anyone who doesn't get on board is going to lose out in a big way. Eventually if AI can't find it, it might as well not exist. Good luck with your scraper protection then.",
    "source": "comment"
  },
  {
    "id": "n10n3n4",
    "title": "",
    "body": "is perplexity continuously scraping the Internet? or does it only reach out when a search is performed.?",
    "source": "comment"
  },
  {
    "id": "n10nucs",
    "title": "",
    "body": "what's the max download rate per ip?",
    "source": "comment"
  },
  {
    "id": "n11fypj",
    "title": "",
    "body": "get more? There are providers that use residential ips and automatically rotate them out.",
    "source": "comment"
  },
  {
    "id": "n11299v",
    "title": "",
    "body": "The point is to not allow *new* data in, data that the site owner didn't consent to being used. You replace that with old data that the model almost certainly already has in the training set. It won't improve the model, but it won't poison it either.",
    "source": "comment"
  },
  {
    "id": "n112fzk",
    "title": "",
    "body": "...what? I'm not sure what you're even talking about. Of course other people could put up random crap to poison the scrapers. Those other people won't have the same reach that CloudFlare does.",
    "source": "comment"
  },
  {
    "id": "n104qhz",
    "title": "",
    "body": "YES! that will do /s",
    "source": "comment"
  },
  {
    "id": "n10m1lt",
    "title": "",
    "body": "I mean without the human element, which I assume is necessary for anything truly at scale. It just seems like this is something that someone could do with some site. But, is it something that a large company could implement, automatically, regularly, with limited human input? I am 100% just giving an knee-jerk take my own, so much more interested in learning. But, why something like pyautogui over Selenium, etc?",
    "source": "comment"
  },
  {
    "id": "n11fdzs",
    "title": "",
    "body": "\"interesting\" times we live in?\nthe rate of change these past 5 years has been mesmerising.",
    "source": "comment"
  },
  {
    "id": "n14qtg9",
    "title": "",
    "body": "If AI can\u2019t reach your site through legit channels, users will bounce to whatever source feeds their agent directly. The unlock is shifting from passive pages to explicit feeds: expose a clean API or GraphQL endpoint, tag snippets with schema.org, and charge per token or hit. News outlets I work with are testing AWS Data Exchange\u2019s pay-per-call gateway and Substack\u2019s paid RSS so the model always gets fresh, licensed facts. I\u2019ve also seen folks bolt on Google\u2019s PAIR content license, but Mosaic slots ads into chat responses so you still earn off open endpoints without nagging paywalls. In practice, once the data lives in a structured, monetized pipe, no one cares if the raw HTML stays crawler-proof-agents query, you get paid, and readers never notice. Sites that ignore this shift will be ghost towns whether Cloudflare shields them or not.",
    "source": "comment"
  },
  {
    "id": "n14sg9v",
    "title": "",
    "body": "how much data can you download per ip per day?\n\ncurious about the amount of rotation and total number of ips required.",
    "source": "comment"
  },
  {
    "id": "n11nbg3",
    "title": "",
    "body": "Sorry I bothered then. You said you didn't see how a small proportion of training data could have an impact. I attempted to explain.",
    "source": "comment"
  },
  {
    "id": "n104w5p",
    "title": "",
    "body": "Lol it actually will though?",
    "source": "comment"
  },
  {
    "id": "n10sdpu",
    "title": "",
    "body": "I'm just saying as worst case. Easiest case you just spoof the google bot crawler and do normal get requests. Pretty sure most websites want to be on Google so yeah",
    "source": "comment"
  },
  {
    "id": "n14k7mh",
    "title": "",
    "body": "> You said you didn't see how a small proportion of training data could have an impact.\n\nI did not say that. I said that a small amount of fake information provided by CloudFlare wouldn't make them *obviously* worse, as in, the product owners wouldn't immediately identify it had been poisoned. It would make it *subtly* worse.",
    "source": "comment"
  },
  {
    "id": "n10nd3w",
    "title": "",
    "body": "omg what has software engineering become?\na conglomerate of hustlers.",
    "source": "comment"
  },
  {
    "id": "1lqe31s",
    "title": "[D] What Tool to Use to Create Illustrations Like This?",
    "body": "Recently, I\u2019ve seen many researchers adopt this style of illustration to present an architectural view of their method or approach. These visuals are clean, professional, and visually appealing, perfect for research papers and presentations.\n\nI've tried replicating this style using [draw.io](http://draw.io), but I haven\u2019t been able to achieve the same level of quality or aesthetics.\n\nCould anyone suggest tools or software commonly used to create such research illustrations?\n\nI'm particularly interested in tools that are:\n\n1. Suitable for academic or technical diagrams\n\n2. Capable of producing high-quality, publication-ready visuals\n\n3. Flexible for custom styling or layouts\n\nAny recommendations would be greatly appreciated!\n\nPlease check Illustration here: [https://imgur.com/a/VWiKD3Q](https://imgur.com/a/VWiKD3Q)",
    "source": "post"
  },
  {
    "id": "n12foo0",
    "title": "",
    "body": "Adobe illustrator with a generous helping of comic sans.",
    "source": "comment"
  },
  {
    "id": "n13qswf",
    "title": "",
    "body": "My understanding of clean, professional and understandable is exact opposite of this example",
    "source": "comment"
  },
  {
    "id": "n12312v",
    "title": "",
    "body": "sadly, powerpoint is great for this. Vector graphics, easy to drag and group elements. Configurable Snap-to guides. Custom shapes/patterns. Layering like photoshop. Powerpoint has it all",
    "source": "comment"
  },
  {
    "id": "n12h1ni",
    "title": "",
    "body": "I've had varying levels of success doing this with PGF/TiKZ, Inkscape, MS Powerpoint, Google Drawings, and I know people also use Adobe Illustrator.",
    "source": "comment"
  },
  {
    "id": "n131y9h",
    "title": "",
    "body": "I use overleaf + draw.io for posters",
    "source": "comment"
  },
  {
    "id": "n13mqzz",
    "title": "",
    "body": "The specific tool you use is going to be less important than being comfortable working in it, having it produce scalable vector images, and thinking through which *specific* points you need the illustration for -- it's getting long in the tooth and is centered around presenting statistics, but Tufte's *Visual Display of Quantitative Information* is a great read for thinking about how much information you *should* try to convey in a figure.\n\nI would also suggest something like Powerpoint for overall block/systems architecture diagrams, but [LibreOffice Impress](https://www.libreoffice.org/) also works just as well, and will also produce SVG/EPS/anything but JPG",
    "source": "comment"
  },
  {
    "id": "n12xgy1",
    "title": "",
    "body": "This example is horrible though",
    "source": "comment"
  },
  {
    "id": "n14dh3v",
    "title": "",
    "body": "Comic Sans (the font in your example) is certainly an interesting choice! I'm not sure I'd personally choose it for technical/academic diagrams but some people may like it I guess.\n\nIf you need live-edit LaTeX equations, block diagrams (like boxes+arrows), PDF export then you might be interested in Vexlio (I am the developer). [https://vexlio.com](https://vexlio.com)",
    "source": "comment"
  },
  {
    "id": "n12tn1b",
    "title": "",
    "body": "I second this",
    "source": "comment"
  },
  {
    "id": "n14foto",
    "title": "",
    "body": "I also use PowerPoint . But layering ? How ? Do you mean \u201csend backwards \u201c?",
    "source": "comment"
  },
  {
    "id": "1lpvn4q",
    "title": "[P] The tabular DL model TabM now has a Python package",
    "body": "Hi! My colleagues have recently published a Python package for [TabM](https://github.com/yandex-research/tabm) \\-- a **simple and powerful DL architecture** for solving predictive tasks on **tabular data** (classification, regression, etc.).\n\nIn a nutshell, TabM efficiently imitates an ensemble of MLPs (see the image below). This basically means that TabM has the power of an ensemble, but at the same time remains practical and scalable. Among the recent highlights: \ud83c\udfc6 **TabM has been successfully used on Kaggle**, including the winning solutions! The package provides the PyTorch implementation of TabM, as well as PyTorch layers and functions for building custom TabM-like models.\n\nInstallation:\n\n```\npip install tabm\n```\n\n- [Paper](https://arxiv.org/abs/2410.24210)\n- [Package](https://github.com/yandex-research/tabm)\n- [Colab example](https://colab.research.google.com/github/yandex-research/tabm/blob/main/example.ipynb)\n\n\n[TabM model illustration](https://preview.redd.it/pl3oth89qgaf1.png?width=2432&format=png&auto=webp&s=37ed08404f3eee2e2a72dc41aa796b17ed6ae32b)",
    "source": "post"
  },
  {
    "id": "n114cge",
    "title": "",
    "body": "Is this dropout?",
    "source": "comment"
  },
  {
    "id": "1lql7zx",
    "title": "[D] Why DragGAN is not going viral as other image models",
    "body": "I remember how impressed I was when I first saw its demo videos. But after two years, it hasn\u2019t reached the level of popularity I expected. Why is that? Just because natural language isn't involved? Its customized image manipulation features seem really useful to me\u2014though I\u2019m not an expert or an active user in this domain. Or has it already become part of the workflow with diffusion/LLM-based image models?",
    "source": "post"
  },
  {
    "id": "n13wdnj",
    "title": "",
    "body": "A couple reasons off the top of my head. GANs as image generators are useful for interactive stuff, but they lack the fidelity and stability of LDMs which is why the hype has mostly been concentrated on the latter. \n\nThere's also the fact that DragGAN is basically just a fancy Photoshop plugin. It's a cute idea, sure, but its scope as a tool is pretty limited. I wouldn't expect it to attract the kind of mass appeal you see with some of the more well-known models.",
    "source": "comment"
  },
  {
    "id": "1lppyht",
    "title": "[D] How to become fluent at modifying/designing/improving models?",
    "body": "By fluency I mean:\n\n1. Read a paper and and without much problem implement the techniques mentioned, whether it's building something from scratch using the paper as guidance (even in the absence of code), or modifying existing models.\n2. Having an idea and being able to translate that into designing new architectures or modifying existing models.\n3. Improving models.\n\nThink of people like [Phil Wang](https://github.com/lucidrains) who is very prolific at reproducing papers and or improving them. I'm very curious to know in your experience what made it \"click\" that unlocked your ability to be productive with these things. I suspect the boring answer is \"just reproduce papers, bro\", but I was hoping to learn about people's own experience/journey on this and if you guys have any specific insight/tricks that can be useful for others to know about. Like maybe you have a good workflow for this or a good pipeline that makes you 10x more productive, or you have some niche insight on designing/modifying/improving models that people don't usually talk about etc.",
    "source": "post"
  },
  {
    "id": "n0x8wo0",
    "title": "",
    "body": "For me it finally started to click when I actually started building stuff\u2014even if it was hacky or half-working at first. You get way more out of trying to implement even a toy version than you do passively reading.\n\nWhen something doesn\u2019t work, I dig into the layer or module causing the issue, and that\u2019s where the real learning happens. Also helps to keep a few reference repos around that are clean and well-annotated\u2014gives you a mental map of how things are structured.\n\nOne tip: don\u2019t just copy and run code. Try to swap in a new loss function or tweak an architecture and see what breaks. That\u2019s how you go from \u201cI kinda get it\u201d to \u201cI can tweak it with confidence.\u201d",
    "source": "comment"
  },
  {
    "id": "n0yq3l4",
    "title": "",
    "body": "You should do X to become good at X.",
    "source": "comment"
  },
  {
    "id": "n11ma8n",
    "title": "",
    "body": "lots of lots of practice",
    "source": "comment"
  },
  {
    "id": "n0x1w7m",
    "title": "",
    "body": "[deleted]",
    "source": "comment"
  },
  {
    "id": "n0xh3j6",
    "title": "",
    "body": "That's good advice, thank you! What is sort of unfortunate though, but also expected, is that I feel like for every domain and every specific type of architecture there's a bag of tricks that you can only learn by implementing, tweaking and experimenting with models. But once you go to another entirely different model, you have to sort of learn from anew about all the bag of tricks that is needed. Seemingly this means that you would need to pour in a lot of time into working with any kind of model if you want to get to the levels of someone like Phil Wang. In reality, I guess, nowadays the underlying techniques overlaps with lots of popular models, such as attention and diffusion, and architectures such as LLM, so knowledge/intuition that you gain from working with these models can be highly transferable.\n\nDo you know if there are any repos or resources on good \"bag of tricks\" for different types of models that people have accumulated somewhere? Sadly, I guess intuition about models can be hard to put down into words.",
    "source": "comment"
  },
  {
    "id": "n0ximdp",
    "title": "",
    "body": "Sorry if I'm wrong about this, but I highly suspect this must be a bot promoting the site mentioned, like 95% of the user messages are about that site and the account was created 1 month ago lol.",
    "source": "comment"
  },
  {
    "id": "n0xj368",
    "title": "",
    "body": "I don't know if it is really as much a bag of tricks as it is having a strong foundation.\n\nI think to achieve anything in ML you need to be able to both deep dive in your chosen area, and draw from other areas when it makes sense.  On the data science side of the spectrum, there are so many people crawling the space, finding something truly novel is tough.\n\nAnd for me, unnecessary - I shop for models like I am buying parts at the hardware store.  My specialization is interactive intelligence - I am on the other side of ML compared to the data scientists.\n\nMy focus is on what they do more than how they do it.  And that is how I believe I am able to take a broader view than individuals participating in the grid search.   My knowledge is broad but shallow.  That is where I enjoy being.\n\nAn adversarial networking here, a mixture of experts there, etc.  That is enough to keep up with, considering I also deal with brining them to life.\n\nSo, my experimentation with models is always pretty quick and dirty these days - mostly I want to know what hyperparameters are most sensitive - I already have data and task selected for benchmarking - I think that is how most of us approach it.  \n\nIf you are going to deep dive into the models - I suggest mastering filtering out the noise.  We all have capacity constraints.\n\nI don't know of any way to learn what I know other than years of study and research and experimentation.  I find myself watching nature now and thinking about whether survival techniques animals use are transferrable to systems.  When I tutor my 5 year old I am careful not to overfit him with my biases.  :-)\n\nSo, I don't think it is really a trick anymore.  It is kind of just the way I view the world.  And the knowledge becomes transferrable across domains.",
    "source": "comment"
  },
  {
    "id": "n0yih1d",
    "title": "",
    "body": "And it's a pretty good bot, too. Look, it even replies. The future of advertisement.",
    "source": "comment"
  },
  {
    "id": "n0xk5lt",
    "title": "",
    "body": "[deleted]",
    "source": "comment"
  },
  {
    "id": "n10kzqf",
    "title": "",
    "body": "I am an outsider that ended up tangential to the space in software dev for a couple Ml infra companies, but never in a way that actually clicked with my inner drive or \"chosen area\". Out of respect for it, I tried to quit it and assume I was just experiencing a new Dunning-Kruger. But, my mind kept coming back to it in the types of situations you described, \"unrelated\" disciplines. Yet, the more I formally dug into the field, the less unrelated many fields became. Especially when it comes to viewing it as a source of, as you said, \"interactive intelligence\". Which is what drew me to programming in the first place. Make the tool that makes tools to help people.\n\nIf this is something that resonates, without taking up much of your time, would you have a few concrete recommendations for taking some early steps into making things concrete (high signal reproducible work, solid text/audio/visual resources, things that first helped you cross that line yourself)? Otherwise, if I'm off base, no worries at all.",
    "source": "comment"
  },
  {
    "id": "n10q2x5",
    "title": "",
    "body": "Newton invented calculus so he could understand physics, not because he loved math.\n\nAs much as I respect the work the data scientists are doing, I think many of them cannot see the forest through the trees.\n\nSome of the best people in AI/ML come from disciplines outside CS.  They bring unique insight.\n\nAnd half of interactive intelligence is software engineering.\n\nThe algorithm doesn't always work, but in my systems it does what it is expected and needs to do 100% of the time.\n\nThat is not something my data science friends are necessarily concerned with.\n\n\\_\\_\n\nHigh signal reproducible work - seed your random variables :-)\n\nyou would have to clarify why you are seeking that.  Are you trying to publish something?  If so, why?  Are you trying to build something?  why?\n\ntext - GPT 4.5 is supposed to be the best at writing.\n\nI haven't found a good AI for visuals. lol\n\n\\_\\_\\_\\_\n\nI recently realized I crossed the line when my boss gave me one of the most visible, complex and technically challenging projects at our Fortune 50, and simultaneously had a Warton Fellow tell me that my side project was novel and probably can get DoD funding and he was willing to make introductions.\n\nThat was a pretty good signal for me.  :-)\n\nBetter than leetcode.  But here - let me show you a merge sort. That's important.  lol\n\nHow did I get here - long long story - 240 undergrad credits, 2 masters degrees...and a long list beyond that.\n\nBut clarify what you want to do - make the switch into a more ML focused role?  Create a portfolio?  DM me if you like.",
    "source": "comment"
  },
  {
    "id": "1lqb0uq",
    "title": "[D] UofT PhD Ranking",
    "body": "In terms of academia prestige (for future prof positions), where would you place UofT ML PhD? Is it better RoI to do it at a T10 American school (UIUC, Georgia Tech, UT Austin, UWash, etc) for name recognition considering the advisors are equivalent? Also, how does UofT PhD fare against Oxbridge DPhil these days?",
    "source": "post"
  },
  {
    "id": "n11c3dz",
    "title": "",
    "body": "Supervisor fit and personal motivation for the topic matter more than institution. University reputation is an imperfect proxy for the research strengths, networking, and supervisor guidance you'll receive.\u00a0",
    "source": "comment"
  },
  {
    "id": "n11c9ph",
    "title": "",
    "body": "Toronto has produced some very respectable and successful names in ML and it\u2019s applications, biggest example being Hinton, who recently won the Nobel Prize in Physics for breakthroughs in ANNs\n\nNo worries about academic prestige there, look into departments that are the best for you specifically",
    "source": "comment"
  },
  {
    "id": "n11c4vi",
    "title": "",
    "body": "At this point it depends more on your advisor and research skills, they're all good universities.",
    "source": "comment"
  },
  {
    "id": "n12lo4g",
    "title": "",
    "body": "This site is a good source for seeing how PhD institution interacts with hiring: https://jeffhuang.com/computer-science-open-data/\n\nUofT is good, lots of professors are from UofT \u2014 but nowhere near as many as are from MIT, Stanford, Berkeley, CMU. By raw numbers on this list (which are not normalized by number of graduates, unfortunately) they're basically on par with Georgia Tech.",
    "source": "comment"
  },
  {
    "id": "n11coeq",
    "title": "",
    "body": "Just my 5 cents but I'm at UofT for a ML PhD and most people here agree the schools you listed and Toronto are virtually equivalent. So agree with everyone else that advisor topic etc. matters more. Also worth noting US is going through funding problems and stuff that Canada has mostly been insulated from",
    "source": "comment"
  },
  {
    "id": "n123io2",
    "title": "",
    "body": "UofT ML PhD student here. Name recognition of the university has close to zero bearing when it comes to being offered a profship. Your best work during your PhD and the recommendation from your PhD supervisor are the two most important aspects. Instead of university recognition, I would say that the name recognition of your professor matters a lot more than that of the university. UofT had people like Stephen Cook and Geoff Hinton at one point. Their recommendation would've landed you in any place that you'd hope.",
    "source": "comment"
  },
  {
    "id": "n13d0vd",
    "title": "",
    "body": "UofT/ Vector institute are well known globally to AI researchers. I would say above the programs you mentioned except for Oxford/Cambridge. Realistically the name wont help you much, quality of your research/supervisor will be the biggest impact on your opportunities post graduation",
    "source": "comment"
  },
  {
    "id": "n147spi",
    "title": "",
    "body": "Toronto is a top school for AI/ML.   Atm the US is a hostile place for graduate students, particularly international students.",
    "source": "comment"
  },
  {
    "id": "n11qjsc",
    "title": "",
    "body": "Curious if you were specializing in something and explored what's available at UofT in terms of research opps.\n\nI'm debating doing an MSc at most (PM who's taking grad level courses at Stanford and loving it)... but want to focus on causal reasoning and neurosymbolic methods to enhance performance on generative tasks.",
    "source": "comment"
  },
  {
    "id": "n12z83h",
    "title": "",
    "body": "Can I DM?",
    "source": "comment"
  },
  {
    "id": "n12yhvc",
    "title": "",
    "body": "[deleted]",
    "source": "comment"
  },
  {
    "id": "n11s2o6",
    "title": "",
    "body": "[deleted]",
    "source": "comment"
  },
  {
    "id": "n1231o0",
    "title": "",
    "body": "Minor nitpick - Hinton never studied at UofT, he did his PhD at the University of Edinburgh. Of course, a lot of his PhD students at UofT went on to do cool stuff.",
    "source": "comment"
  },
  {
    "id": "n146mcg",
    "title": "",
    "body": "Hinton taught there. Sutskevar and a bunch of other famous AI researchers studied at Toronto under Hinton. \n\nI\u2019d say ten years ago it was the best place to be to be studying deep learning. Today, I\u2019m not sure, but probably still top tier",
    "source": "comment"
  },
  {
    "id": "n134xe9",
    "title": "",
    "body": "AI hasn't been effected as much. But if you are from a group that has been historically excluded from STEM careers in the U.S. or a postcolonial country , the funding for the types of programs that specifically help those communities has dried up in the U.S. And the worst elements in U.S. society have been emboldened to purge or harass universities at will. If you have the chance to avoid for what is a lateral position in Canada, go for it",
    "source": "comment"
  },
  {
    "id": "n136ibr",
    "title": "",
    "body": "What a stupid and random ranking, haha",
    "source": "comment"
  },
  {
    "id": "n11vogl",
    "title": "",
    "body": "I don't know your goals, which should guide your decision. UofT is well respected.\u00a0\n\n\nYou already know how to get the answer for academia. U of T does have a healthy percentage of PhDs continuing onto tenured positions and you can track where they end up.\n\n\nBut pay attention to how schools are connected to industry. That's more important than you may realize. Companies generally build connections to Universities and people within them. Your advisors past students can help you as well. That personal connection gives a leg up beyond the standard recruiting efforts.\u00a0\n\n\nHowever, connecting with an advisor is also important as is interest in the topic. You won't do well without either. You need to have genuine interest if you don't want to burn out. Also, an advisor you connect with is very important for that next position. They often help with that process even outside of academia.\u00a0\n\n\nAlso, have you been accepted into all of these?\u00a0",
    "source": "comment"
  },
  {
    "id": "n1256gr",
    "title": "",
    "body": "To give you more confidence in your decision: each of the universities you've listed is very highly respected in ML. You should have no concerns there. There is more variance in individual researchers within each institution than there are between the universities themselves. So it is worth thinking carefully about who you will work with and how each city matches with your interests (you'll be spending 3-5 years there, growing roots, friends, and connections - so it's also worth thinking about these lifestyle factors).",
    "source": "comment"
  },
  {
    "id": "n137dcf",
    "title": "",
    "body": "[deleted]",
    "source": "comment"
  },
  {
    "id": "n137yz0",
    "title": "",
    "body": "If you're picking your PhD institutions because of some magazine rankings, you should rethink your motivation for a PhD.\u00a0\nSo many of the top ML researchers nowadays went to a school you call irrelevant, how is that possible? Just of the top of my head: LeCun(Paris), Silver(Alberta),\u00a0 Hinton(Edinburgh), Schmidhuber(TUM), Bengio(McGill), Hassabi(UCL)\nYou should go with the best research fit and not just where some magazine ranking tells you to go.\u00a0",
    "source": "comment"
  },
  {
    "id": "n13k7ay",
    "title": "",
    "body": "[deleted]",
    "source": "comment"
  },
  {
    "id": "n1420e0",
    "title": "",
    "body": "mostly agree w/ u but I think both of you are taking kind of an extreme opinion on this subject\n\n- the people listed like Le Cun etc. are literally _one of kind_ from their institutes, but institutes like CMU regularly produce Le Cun-ish people,\n- However relying on rankings is definitely not optimal, I think there should be some prof-Uni matrix or such... just university rankings can be misleading...",
    "source": "comment"
  },
  {
    "id": "n13npw2",
    "title": "",
    "body": "But why should you go to a uni just because they have many big labs that publish well, instead of looking at the best labs in the field you want to get into and then pick the one that publishes best in that field? Why should, for example someone that wants to do research in Causal Inference for ML, go to Berkeley or MIT because they have Levine/ Abeel and Tedrake with an insane publishing output in RL for Robotics, pushing them up in Rankings, when you could go to better labs for Causality at Columbia, Harvard, ETHZ or MPI for IS?",
    "source": "comment"
  },
  {
    "id": "n14kzv9",
    "title": "",
    "body": "I don't think I am taking an extreme stance. All I am saying is pick the lab/ Advisor that fits best for what you want to do, because they are the top person in that field. Obviously for a lot of people that's going to be at CMU, Berkeley, MIT or Stanford so a lot of people should go there. But don't just go there because they top some ranking, actually know why you want to go there from a research perspective.\u00a0\nOn another note, I could not think of one person as influential as LeCun in ML coming out of CMU, who were the ones you were thinking of?",
    "source": "comment"
  },
  {
    "id": "n14pkb1",
    "title": "",
    "body": "Andrew Ng did his undergrad at CMU :)",
    "source": "comment"
  },
  {
    "id": "n14qaib",
    "title": "",
    "body": "Okay that's fair. But I wasn't really talking about undergrad. I don't think your research career is going to depend on who taught you basic calc and intro to CS in undergrad, haha",
    "source": "comment"
  },
  {
    "id": "n14tbyp",
    "title": "",
    "body": "ok next: Jure Leskovec did his PhD at CMU",
    "source": "comment"
  },
  {
    "id": "1lqampf",
    "title": "[D] Applicability of a Biomedical based AI/ML PhD to other AI/ML fields",
    "body": "Hey all,\n\nI am a first year PhD student in a top biomedical program in the US. One of the labs I am most interested in studies how to more effectively use AI/ML to enhance the drug discovery and development process. Although I current have only a limited knowledge of coding (really just experience with R and a little C++) the PI has told me he'd be happy to have me join the group. Still, I wonder about the applicability of this niche expertise. Does having done a PhD in biomedical focused AI/ML allow for the possibility of being hired in say finance AI/ML? What about AI/ML research in big tech? Or would you say it is only applicable in Big Pharma/biomed startup research?\n\nThanks for your insights.",
    "source": "post"
  },
  {
    "id": "n11837t",
    "title": "",
    "body": "I've done a number of biomedical AI/ML projects including generative small molecules for drug discovery, microscopy image analysis, amino acid sequence encoding for function and structure prediction. \n\nI get recruiters messaging me about roles for non biomedical fields. Meta, for example, recently reached out to improve their ad recommendations ML.",
    "source": "comment"
  },
  {
    "id": "n11wx9m",
    "title": "",
    "body": "My PhD is in bio-focused ML from a top-five uni in the world! While after my PhD most of the places that I interviewed with were biotechs, I also got approached by companies from different domains. There are people from my cohort, who went to fintechs. IMO it depends on how one pitches their work and how they do networking during their PhDs.",
    "source": "comment"
  },
  {
    "id": "n13pca3",
    "title": "",
    "body": "I'm an undergrad student working in Biomedical Computer Vision. I will probably get a WACV paper published this year. Would you say that doing a PhD is necessary to get a research role in this field?",
    "source": "comment"
  },
  {
    "id": "1lphfhf",
    "title": "[D] Request for Career Advice \u2013 ML PhD non hot topic",
    "body": "I\u2019m currently a PhD student in Machine Learning, working on a research topic that isn\u2019t considered \u201chot\u201d in the current academic or industrial landscape. Despite this, I\u2019ve managed to publish as the lead author at ICML, NeurIPS. And  twice at ECML. I also have two co-authored publications at ECAI.\n\nI\u2019ve noticed that many  PhD students in the U.S. seem to have much stronger publication records, often in trendier areas. This makes me question how competitive I really am in the current job market\u2014especially given the wave of layoffs and increasing demand for very specialized expertise in industry.\n\nThat said, I do have a strong foundation in core ML, Deep Learning, and LLMs (although LLMS aren\u2019t the direct focus of my PhD research).\n\nGiven all of this, I\u2019m trying to realistically assess: \u2022 What are my current chances of landing a demanding, high-quality job in industry or research after my PhD? \u2022 What could I do now to improve those chances? \u2022 Goal is FANNG.\n\nI\u2019d greatly appreciate any  feedback.\n\nEdit: My research focuses on anomaly detection, a less trendy area compared to the current popularity of large language models and reinforcement learning.",
    "source": "post"
  },
  {
    "id": "n0vugvp",
    "title": "",
    "body": "Imagine you are computer vision expert in ~2005 and publish a lot on the hot topic of custom made descriptors for feature matching for augmented reality. You pity those unfortunate souls trying to make convolutional networks work. Dead end previous century approach. And they are using gaming videocards, poor things...",
    "source": "comment"
  },
  {
    "id": "n0w3or5",
    "title": "",
    "body": "I had similar credentials a few years ago with pubs in ICML, NeurIPS focusing on topics like high dimensional statistics, numerical linear algebra, and some other theory stuff. I did a short postdoc in deep learning and language models adjacent theory, and managed to get some work pulished. Fast forward to now, and I'm in a Faang working on LLM applied research. My point is, you are already competent in deeply technical stuff and research relevant to latest hot topics. You just need to find a way  to have a smooth transition using the credentials you have.",
    "source": "comment"
  },
  {
    "id": "n0v4mjt",
    "title": "",
    "body": "Intern and network. Research scientist jobs are competitive even for PhD at MIT unless you have a famous advisor with strong industry connections.",
    "source": "comment"
  },
  {
    "id": "n0vfdbh",
    "title": "",
    "body": "> I\u2019ve noticed that many PhD students in the U.S. seem to have much stronger publication records\n\nYou have two lead author papers at good conferences. Were they workshop sessions? Posters? Trying to understand what you consider to be a stronger record.",
    "source": "comment"
  },
  {
    "id": "n0v7jka",
    "title": "",
    "body": "You're a lot more competitive than the poor sobs graduating with COLT/STOC/FOCS papers.\n\nPut out code and data for all your papers, make websites and demos (if you haven't already).\n\nFor industry: start leetcoding, reach out to connections to give brownbag talks (many orgs in FAANG have these and they'd love to have you). Take pictures at your talks and tweet about them.\n\nFor academia: your advisor's letter is most important. Reaching out to connections is important. Send cold emails, have less shame and be less fearful of saving face for the year you're on the market.\n\nThat being said, a lot of academia is in a hiring freeze right now, including for postdocs.",
    "source": "comment"
  },
  {
    "id": "n0uviv5",
    "title": "",
    "body": "Would be better if the author gives more details on what topic it is. AFAIK,  there is less opportunities in less trendy research",
    "source": "comment"
  },
  {
    "id": "n0wgvbe",
    "title": "",
    "body": "Is computer vision is still trendy or how does it look like in the industry right now...?",
    "source": "comment"
  },
  {
    "id": "n10m4k4",
    "title": "",
    "body": "This may be a good or bad thing, but people have gotten into FAANG (deepmind, FAIR, etc.) for less (a lot less). For example, 2 coworkers at my lab work at deepmind, both with a grand total of 0 Neurips, ICML, ICLR papers. Your publication record is very strong, and 2 top AI conference publications vs 5 or whatever WILL NOT MATTER at all (unless, of course, one of those papers is a super well-cited/well-known paper).\n\nThat being said, there are 2 big problems you are going to face. First is your research direction. Teams at FAANG (and probably everywhere else) first and foremost make sure the people they are hiring are studying the same or very similar things. They will rather have someone who has thought deeply about what they are currently exploring/researching than someone with x top AI conference publications (peer review is noisy and getting worse by the year anyways). Second, is networking. Many people who work at these companies interned there or have lab alumni working there, or have faculty well known by industry.\n\nSo, I think the best advice is find a way to connect your clear expertise in anomaly detection into something more popular/current, and try to intern or publish with faculty that have the industry connections.",
    "source": "comment"
  },
  {
    "id": "n14t7xu",
    "title": "",
    "body": "i dunno. it sounds like you're doing great",
    "source": "comment"
  },
  {
    "id": "n0x1wg6",
    "title": "",
    "body": "Thank you very much for the tips. I really appreciate you sharing your story.",
    "source": "comment"
  },
  {
    "id": "n0yy5gi",
    "title": "",
    "body": "By \u201cfamous\u201d advisor, do you mean famous in the current hot topic? My interests have recently shown promising ML applications but I wouldn\u2019t consider the professors working in the field famous in the language model space for example",
    "source": "comment"
  },
  {
    "id": "n0x0nfz",
    "title": "",
    "body": "I have four first-author publications: one each at ICML and NeurIPS, and two at ECML.\nUnfortunately, ECML is generally considered a mid-tier conference.\nI\u2019ve seen many PhD students in the U.S. with 5 or more first-author papers, all published at top-tier venues like NeurIPS, ICML, ICLR, or KDD.",
    "source": "comment"
  },
  {
    "id": "n0wcs9u",
    "title": "",
    "body": "\"You're a lot more competitive than the poor sobs graduating with COLT/STOC/FOCS papers.\"---\n\n\nIs this sattire or do random ML people in this thread think COLT is of similar stature to STOC/FOCS?",
    "source": "comment"
  },
  {
    "id": "n0x1fnm",
    "title": "",
    "body": "Thank you very much for the tips.\nI actually started using LeetCode recently, and given the current situation, I\u2019ve already solved around 500+ problems to stay competitive.",
    "source": "comment"
  },
  {
    "id": "n0uwlh2",
    "title": "",
    "body": "Hi, it is anomaly detection.",
    "source": "comment"
  },
  {
    "id": "n0wfo2w",
    "title": "",
    "body": "No idea what he refers to, but I think the emphasis is on these conferences being theoretical",
    "source": "comment"
  },
  {
    "id": "n0xf0gq",
    "title": "",
    "body": "Tongue in cheek, these are all solid venues, it's just that theoreticians struggle on the market despite being good.",
    "source": "comment"
  },
  {
    "id": "n0xx8po",
    "title": "",
    "body": "ngl I think you'll do well. If you can, I'd talk to Yue Zhao at USC or Jiawei Han at UIUC.",
    "source": "comment"
  },
  {
    "id": "n0v1a6p",
    "title": "",
    "body": "I'd argue anomaly detection falls into \"sorta trendy\".\nWe need novel ways to find wild flaws in systems, artifacts in physical engineering projects, etc.\n\nDepending on what subfield, this could be tied into casual inference which has gotten popular recently.\n\nThis isn't strictly in the latest LLM craze, but I think you'd do well looking for work in anomaly detection in modern tech.",
    "source": "comment"
  },
  {
    "id": "n0v2400",
    "title": "",
    "body": "anomaly detection using ML or anomaly detection in ML models? Both areas honestly are in need and will be always like that. Try to not compare yourself to other people espically who are coming from big labs. They collaborate and get so much help. You need to focus on the quality of your research and your technical and social skills. You have a great publicaiton record!",
    "source": "comment"
  },
  {
    "id": "n0uwryh",
    "title": "",
    "body": "Isnt it dtill very relevant?.. to make sure LLMs dont go hay wire?",
    "source": "comment"
  },
  {
    "id": "n0v3rn9",
    "title": "",
    "body": "Time series?",
    "source": "comment"
  },
  {
    "id": "n0zl3yv",
    "title": "",
    "body": "Why would you be worried? This is a very classic problem that could be useful in industry. With your track record, the only thing you really need to do is apply / reach out to previous lab mates for referrals.\u00a0\n\nSource: Big tech research intern with 0 papers in neurips, icml, iclr",
    "source": "comment"
  },
  {
    "id": "n0xvf2d",
    "title": "",
    "body": "Whoever gave you this idea?",
    "source": "comment"
  },
  {
    "id": "n0xuwvo",
    "title": "",
    "body": "Anomaly detection using ML.\n\nThanks for the feedback.\nI\u2019ll stay focused and continue preparing for standard interviews (like LeetCode, system design, and so on).",
    "source": "comment"
  },
  {
    "id": "n0uyalu",
    "title": "",
    "body": "Personally, I don\u2019t quite see it that. There  are only a few specialized roles that involve working on it.",
    "source": "comment"
  },
  {
    "id": "n0xvjeu",
    "title": "",
    "body": "I'm on the hiring side.",
    "source": "comment"
  },
  {
    "id": "n0y52jb",
    "title": "",
    "body": "I wouldnt be so sure... these jobs gets renamed very fast.. i think llm safety engineers might be involved in this... But yours will cover a lot more broader domain ryt?.. I think this will always be in demand in monitoring and merrics team or companies.. Also a main thing a phd teaches you how to research which is very applicable to get ground on any domain",
    "source": "comment"
  },
  {
    "id": "n0xvvoa",
    "title": "",
    "body": "So it's completely anecdotal?\n\n\nThe reason I ask is that all my friends I know who had an actual theory heavy PhD (primarily published in focs/stoc) seem to be doing quite well \ud83d\ude05",
    "source": "comment"
  },
  {
    "id": "n0xwz9h",
    "title": "",
    "body": "> anecdotal\n\nNo bruh I have a preprint analyzing faculty hiring across CS subfields just like and subscribe to my Substack /s\n\n> doing quite well\n\nOf course they'll do well, they're smart.\n\nI said theoreticians struggle on the market.\n\nMy theory-focused PhD students had significantly fewer faculty offers than my non-theory-focused ones, and they were of similar quality.\n\nYou don't have to believe me and I don't really care.",
    "source": "comment"
  },
  {
    "id": "n0yye6w",
    "title": "",
    "body": "Do you have advice for a theory-focused student on how to approach the job market better?",
    "source": "comment"
  },
  {
    "id": "n0z0ap3",
    "title": "",
    "body": "I know theorists hate this, but you need to impress upon non-theorists the practical implications of your work. What does it enable? \n\nPut a tldr on your website below each paper. Have a good website and ask friends for feedback on it. Release slides for tutorials etc. you may have given, organize workshops/tutorials if you can.\n\nAsk your advisor to reach out to schools you are particularly interested in.\n\nShowcase independence of thought from your advisor and coauthors.\n\nAlways be leetcoding, there is a lot of randomness in the faculty job market. Industry is a good option.\n\nEmbrace the hustle but stay healthy, we have it easier than musicians and other creatives trying to get signed to a label.",
    "source": "comment"
  },
  {
    "id": "1lq66ra",
    "title": "[D] Understanding DDIM : Accelerated Sampling Case",
    "body": "Hello,\n\nI have been going through DDIM paper and have some queries on how the sampling is accelerated (appendix C.1)\n\nThe authors assume that the forward can be decomposed as\n\n[Forward decomposition](https://preview.redd.it/n0yvok1liiaf1.png?width=520&format=png&auto=webp&s=0cbce45652fccf8f10441b25238e8fd8136c7e37)\n\nand backward\n\n[Backward decomposition](https://preview.redd.it/f5gtpdrmiiaf1.png?width=437&format=png&auto=webp&s=2d1f597df36d5dcdab955a167e8fb588a866184d)\n\nwhere tau is subsequence of timesteps \\[1, T\\].\n\nFirst thing I want to point out is that, index \"i\" should start from 2 and from 1. (Am I right in saying this ?)\n\nIf you look into the decomposition, in the forward for the timesteps that are not in the subsequence, we are directly writing x\\_{t}|x\\_{0} and for the timesteps that are in subsequence we write x\\_{tau\\_{i-1}}|x\\_{tau\\_{i}},x\\_{0}.\n\nSo to mimic in the reverse we write for the timesteps that are not in subsequence x\\_{0}|x\\_{t} and for timesteps in the subsequence we write x\\_{tau\\_{i-1}}|x\\_{tau\\_{i}}.\n\nThe above explaination looks good in intuitive sense but when I take an example and write the decomposition, the intutition doesn't come at all.\n\n[Example](https://preview.redd.it/6zn8fux3piaf1.png?width=705&format=png&auto=webp&s=6628a8167fc4e6a7458054d9872a1caf9a338292)\n\nHere the third term in backward p(x\\_{3}|x\\_{4},x\\_{5}) = p(x\\_{0}|x\\_{3}) and fifth p(x\\_{1}|x\\_{2},x\\_{3},x\\_{4},x\\_{5}) = p(x\\_{0}|x\\_{1}) doesn't make sense at all.\n\nCan someone explain how does the backward decomposition work ?\n\nNote : I don't know if this is the correct place to ask these type of questions, but I felt that other subs are not suited for this.\n\nThanks.",
    "source": "post"
  },
  {
    "id": "1lqgtym",
    "title": "[R] A New Approach to AI-Driven R&D: Sharing a Generative Reasoning Framework for Community Stress-Testing",
    "body": "**the Stochastic Kernel Mixture v2.1: A Production-Ready Framework for Generating Synthetic Optimization Landscapes is at the bottom for your critique**\n\n\nA few days ago, I briefly posted an early version of a conceptual prompting framework I called Simulated Parallel Inferential Logic, however I deleted it due to formatting issues on the reasoning canvas. An old iteration of the framework is still available on https://www.reddit.com/r/PromptEngineering/comments/1lnryyf/simulated_parallel_inferential_logic_spil_an/. I've since developed an automated tool to implement the methodology, which I\u2019ve named the Cognitive Forge. It\u2019s a meta-prompting framework that creates bespoke, multi-perspective reasoning engines to tackle complex problems.\n\nI plan to post the full framework, the Cognitive Forge prompt, and a \"how-to\" guide to GitHub tomorrow for everyone to use. My hope is that it can be a valuable tool for the community.\n\nHow It's Different from Standard Multi-Agent Systems\n\nThe Forge operates on a different principle than most agentic systems. Instead of using a static team of pre-defined agents (e.g., \"coder agent\"), it dynamically generates a bespoke team of expert personas tailored to the specific problem. This enables a process focused on forcing a creative synthesis between competing worldviews on a persistent \"Reasoning Canvas,\" all audited by a \"Scientist\" persona for logical consistency. The framework can also recursively analyze its own outputs to drill down into specific sub-problems, allowing for an iterative deepening of an idea.\n\nA Use Case for Critique: Generating a Novel ML Algorithm Blueprint\nTo demonstrate the process, I used the Cognitive Forge to perform a complete, simulated R&D cycle. The AI was tasked with analyzing a real-world ML problem (generating synthetic data for in-context optimizers) and producing a detailed specification for a novel, production-ready solution.\n\nImportant Clarification: The AI did not run code or execute physical benchmarks. It performed a conceptual stress test, using its own logical reasoning to identify failure modes in a theoretical algorithm and then designing engineering solutions to mitigate them.\n\nThe result is the attached white paper for the \"Stochastic Kernel Mixture v2.1\" algorithm. It is a blueprint generated entirely by the AI-driven reasoning process. The entire workflow, from ingesting the problem to producing this final document, took less than an hour.\n\nMy Request to You\nI am not an expert in this specific ML sub-field. I am asking for your rigorous critique of this AI-generated specification.\n * Is the proposed algorithm (v2.1) genuinely novel and theoretically sound?\n * Are the identified failure modes and proposed \"hardening\" solutions logical and realistic from an engineering perspective?\n * Based on this blueprint, do you believe this is a viable path for accelerating R&D?\nMy primary goal is to validate whether this generative reasoning process can reliably produce high-quality, expert-level technical proposals. I look forward to your feedback and insights.\nContact:\n * Public Discourse: http://x.com/The_HumanEngine\n * Secure Correspondence: TheHumanEngine@protonmail.me\n * Author: Architectus Ratiocinationis\n\n\n# Stochastic Kernel Mixture v2.1: A Production-Ready Framework for Generating Synthetic Optimization Landscapes\n\n**The Cognitive Forge Project**\n\nJuly 3, 2025\n\nAbstract\n\nThe training of large-scale, in-context optimization models is critically dependent on access to vast and diverse datasets of functions with a priori known optima. We introduce the Stochastic Kernel Mixture algorithm (v2.1), a constructive, search-free method for generating these functions by directly modifying a Gaussian Process covariance kernel. This paper details two key innovations: \n\n1) A principled, artifact-mitigation technique, Importance-Sampled Orthogonal Features, that significantly improves the statistical fidelity of scalable sampling. \n\n2) A complete, production-ready ecosystem designed around the algorithm, featuring a resilient MLOps pipeline and a novel \"Latent Space Atlas\"\u2014a user-facing tool for the intuitive, visual exploration and control of landscape geometry. \n\nWe present the full blueprint, from the refined mathematical formulation to the deployable system architecture, designed to accelerate the next generation of AI-driven scientific discovery.\n\n1. Introduction\nThe paradigm of \"learning to optimize,\" where models learn optimization as a supervised task, promises to revolutionize computationally expensive discovery processes. A fundamental prerequisite, however, is a data generation engine capable of producing millions of varied and complex optimization landscapes with known ground truth.\n\nExisting methods often fail, either through a lack of diversity or a lack of scalability. To solve this, the \"Stochastic Kernel Mixture\" algorithm was previously proposed as a method that constructs optima directly within the kernel.\n\nThis paper presents the mature, production-ready version of this system. We detail a significant refinement to the core algorithm that mitigates statistical artifacts. More importantly, we present the full architectural blueprint for a deployable, user-centric tool designed to bring this powerful generative capability to researchers and engineers.\n\n2. The Stochastic Kernel Mixture Method (v2.1)\nOur approach encodes the desired function properties directly into a custom GP kernel, k_final, which is then used to draw a single function sample.\n\n2.1. Core Formulation: Additive Kernel Mixtures\nThe kernel is a sum of a base component and a peak component:\nk_{\\text{final}}(x, y) = k_{\\text{base}}(x, y) + A \\cdot k_{\\text{peak}}(x, y; x^*, \\theta)\n * k\\_{\\\\text{base}}: A Mat\u00e9rn kernel controls the baseline smoothness.\n * k\\_{\\\\text{peak}}: A localized, anisotropic RBF kernel constructs a peak with specific geometric properties (\\\\theta) at the location x^\\*.\n * A: A stochastic amplitude controls the peak's prominence.\n\n2.2. Generative Control via VAE\nTo make generating diverse peak shapes intuitive, the parameter vector \\\\theta is controlled by a pre-trained Variational Autoencoder (VAE). This provides a low-dimensional latent space Z, allowing a user to generate complex peak geometries by manipulating a simple latent code z.\n\n2.3. Refinement: Mitigating Spectral Artifacts\nTo ensure high statistical fidelity when using scalable sampling methods like Random Fourier Features (RFF), we refine the process with Importance-Sampled Orthogonal Features. This two-stage technique first generates a set of Orthogonal Random Features to reduce Monte Carlo variance, then applies importance re-weighting to more accurately match the kernel's true spectral density. This principled approach significantly reduces artifacts at their source.\n\n3. A Production-Ready Ecosystem\nA powerful algorithm is only useful if it's deployable and reliable. We designed a complete ecosystem around the v2.1 algorithm to meet these requirements.\n\n3.1. MLOps Pipeline for Scalable Generation\nThe system is designed as a resilient, microservices-based pipeline:\n * API & Job Queue: A REST API receives requests, which are placed onto a message queue (e.g., RabbitMQ).\n * Stateless Workers: A scalable cluster of containerized workers (managed by Kubernetes) consumes jobs.\n * Resilient Storage & QA: Workers perform atomic writes to cloud storage (e.g., S3). A monitoring service automatically runs a battery of statistical tests on a fraction of samples to ensure output quality.\n\n3.2. The Latent Space Atlas: An Interface for Discovery \ud83d\uddfa\ufe0f\nTo solve the \"black box\" nature of the VAE generator, we designed the \"Latent Space Atlas,\" a web-based user interface for intuitive control:\n * It features a gallery of pre-computed landscapes for inspiration.\n * A 2D visualization of the latent space Z allows users to explore different regions, with sliders for direct, tactile control over the most important dimensions.\n * A real-time panel renders a preview of the corresponding peak shape, enabling rapid iteration.\n\n4. Adversarial Analysis & Vulnerability Identification\nThe conceptual algorithm was subjected to a systematic vulnerability assessment to ensure its robustness. This analysis revealed three classes of critical failure modes.\n\n * 4.1 Geometric Instability: The stability of the algorithm depends on the inversion of the kernel matrix. It was determined that pathological combinations of kernel hyperparameters and auxiliary point placements could create a near-singular matrix, leading to numerically meaningless results.\n\n * 4.2 Engineering & Implementation Fragility: The algorithm's implicit precision requirements were tested. On systems using 32-bit floating-point precision, key calculations could suffer from catastrophic cancellation or underflow, producing silently incorrect results.\n\n * 4.3 Statistical Bias & Exploitation: The data generation process was found to imprint subtle, exploitable artifacts. A meta-learning model could potentially learn these signatures (e.g., uniform derivative noise, predictable curriculum stages) instead of the intended optimization task.\n\n5. The Hardened Specification: CDC-GP-H v2.1\nIn response to the identified vulnerabilities, a hardened specification was developed. This version incorporates the following mandatory mitigations:\n\n * 5.1 Stability Guardrails:\n   * Condition Number Check: Before matrix inversion, the matrix's condition number is calculated. If it exceeds a high threshold (e.g., 10^{12}), the operation is aborted with a NumericalInstabilityError.\n   * Adaptive Nugget: The stabilizing \"nugget\" added to the matrix diagonal is now adaptive, scaling with the trace of the matrix for robust stabilization.\n\n * 5.2 Robust Implementation Requirements:\n   * 64-Bit Precision Mandate: The algorithm must run in a 64-bit floating-point environment to prevent precision-related failures. The implementation must check for this at runtime.\n\n * 5.3 Bias & Exploit Mitigation:\n   * Intermixed Curriculum: Discrete training stages are replaced with an intermixed curriculum where parameters for each function are drawn from randomized distributions.\n   * Randomized Noise Signature: The covariance of any \"soft\" derivative noise is randomized for each function to prevent overfitting to a uniform noise texture.\n\n6. Conclusion & Path Forward\nThe conceptual algorithm, while theoretically elegant, is insufficient for production use. This work has specified Stochastic Kernel Mixture v2.1, a hardened successor that incorporates non-negotiable mitigations against identified instabilities and biases. This specification provides a trustworthy foundation for generating the large-scale synthetic datasets required to train next-generation optimization models. The path forward is to implement the algorithm according to this blueprint and utilize it to generate a benchmark dataset, accompanied by a full datasheet as templated in the appendix.\n\n### **7. Appendix: Refined Pseudocode (v2.1)**\n\n```pseudocode\nfunction generate_function_v2_1(x_points, z_latent_code, fidelity_param=1.0):\n    \"\"\"\n    Generates a function sample with reduced spectral artifacts.\n    fidelity_param of 1.0 means no filtering; lower values apply optional filtering.\n    \"\"\"\n    \n    # 1. Setup & Kernel Construction\n    theta_params = g_vae.decode(z_latent_code) \n    amplitude_A = sample_from_log_normal_dist()\n    k_final, p_k_final = construct_final_kernel_and_density(k_base, k_peak, A, theta_params)\n\n    # 2. Refined Feature Generation (Importance-Sampled Orthogonal Features)\n    num_rff = calculate_required_features(k_final)\n    omega_features = generate_orthogonal_random_features(num_rff, dimension=D)\n    importance_weights = calculate_importance_weights(omega_features, p_k_final)\n    \n    # 3. Sample Function\n    function_values_raw = sample_gp_with_weighted_orf(\n        k_final, omega_features, importance_weights, x_points\n    )\n\n    # 4. Optional Post-Hoc Filtering\n    if fidelity_param < 1.0:\n        function_values_filtered = apply_spectral_filter(\n            function_values_raw, strength=(1.0 - fidelity_param)\n        )\n        final_function_values = function_values_filtered\n    else:\n        final_function_values = function_values_raw\n\n    # 5. Output Rich Metadata for Monitoring\n    metadata = build_metadata(...)\n    \n    return final_function_values, metadata\n```",
    "source": "post"
  },
  {
    "id": "n1345m4",
    "title": "",
    "body": "The biggest gap I see is verifying that the kernel mixture actually spans useful function families instead of just neat math. From my own stab at training in-context optimizers, diversity dies the moment the latent VAE bottleneck shrinks, so before shipping, I\u2019d run a leave-one-region-out coverage test: sample tens of thousands of z codes, cluster on gradient spectra, then flag clusters with fewer than N members for manual inspection. On numerical stability, the adaptive nugget is smart but you\u2019ll still hit nasty spikes when the peak and base kernels overlap too aggressively; adding a soft floor on length-scale during sampling kept my condition numbers sane without killing shape variety. Also, think about dataset watermarking early-models will memorize the generator\u2019s quirks; I inject a salted hash into low-magnitude coefficients and track leakage downstream. I\u2019ve leaned on Weights & Biases for lineage, Ray Serve for scaling, and Mosaic when I need quick monetization experiments, so the ops stack looks doable. The biggest gap remains establishing empirical coverage guarantees.",
    "source": "comment"
  },
  {
    "id": "n13x9ss",
    "title": "",
    "body": "You are asking people to review your AI-slop...depressing",
    "source": "comment"
  },
  {
    "id": "1lq4q34",
    "title": "[P] Open-Source: Scaled & Automated Paired Testing for Bias (NYC LL144 & Beyond)",
    "body": "## Proven Impact\nPaired testing (identical requests, one varying factor) exposed systemic discrimination in:\n- Housing: 8,000 HUD audits \u2192 Fair Housing Act\n- Hiring: 10,000+ applications \u2192 proved racial bias\n\n## The Problem\nManual testing can't keep pace with modern discrimination - whether in:\n- AI systems\n- Human bureaucracies\n- Hybrid decision systems\n\n## Why Current Solutions Fail\n\ud83d\udd34 **Traditional audits** - Artificially limited scale  \n\ud83d\udd34 **AI governance tools** - Only look at code, not real-world behavior  \n\ud83d\udd34 **Human system audits** - Easily gamed by temporary compliance  \n\n## How We Fix It\n\u2705 Tests **any** decision system: AI models, government offices, HR  \n\u2705 Fully automated paired testing at **million-scale**  \n\u2705 No internal access needed - measures real outputs  \n\u2705 Turns resistance into **proof of guilt**  \n\u2705 **CC0 public domain** findings  \n\n## The Accountability Engine\n1. Run massive tests on:\n   - Hiring algorithms\n   - Visa systems\n   - Loan approvals\n   - Any decision interface\n2. Publish **immutable** CC0 findings\n3. Force systems to:\n   - **Fix the bias**, or\n   - **Prove their bias** by refusing\n\n## Active Targets\n\ud83c\udde7\ud83c\uddf7 Brazil's AI Act (AEDTs)  \n\ud83c\uddfa\ud83c\uddf8 US regulatory needs  \n\ud83c\uddea\ud83c\uddfa EU GDPR enforcement  \n\ud83c\udfdb\ufe0f Traditional bureaucratic systems  \n\n## Why This Changes Everything\n**Old model:**  \n\"Trust us, we fixed it after that last scandal\"  \n*(Who watches the watchers? No one, by design.)*\n\n**Our model:**  \n\"Continuous, automated proof of fairness - or lack thereof\"  \n*(We watch them watching, always, by their replies.)*\n\n> \"The perfect audit reveals bias whether the decision-maker is silicon or flesh.\"\n\nGet Involved if interested (lmk if I'm mad). \n**GitHub:** [watching_u_watching](https://github.com/genaforvena/watching_u_watching)  ",
    "source": "post"
  },
  {
    "id": "1lplwz3",
    "title": "[D] Will the relationship between Meta's FAIR and Super Intelligence Labs be like that of Google Brain and DeepMind previously?",
    "body": "I really don\u2019t get the point of setting up a new AI lab at Meta.  \nWell, maybe it\u2019s related to the semi-acquisition of Scale AI and creating a group dedicated to Alexandr Wang.  \nBut doesn\u2019t the merger of Google Brain and DeepMind suggest it\u2019s better not to split your resources in the AI war?\n\nAlso would there be possible feud out there?\n\n",
    "source": "post"
  },
  {
    "id": "n0w27i9",
    "title": "",
    "body": "FAIR will  likely be consumed by the genAI and super intelligence labs.",
    "source": "comment"
  },
  {
    "id": "n0w6p1h",
    "title": "",
    "body": "It might have something to do with Yann Lecun's (FAIR's former director and Chief AI Scientist at Meta) focus on World Models and his critical comments about transformers. So maybe FAIR and Super Intelligence Labs will head into different directions research-wise.",
    "source": "comment"
  },
  {
    "id": "n0zfh45",
    "title": "",
    "body": "Yes. there will be a turf war on compute. \n\nAt least FAIR can work on architectures that aren't scale-ups of autogressive models",
    "source": "comment"
  },
  {
    "id": "n13hln6",
    "title": "",
    "body": "Semi-acquisition of Scale AI for Meta's AI future, plus leveraging Alexandr Wang's expertise, is a strategic move to stay competitive.",
    "source": "comment"
  },
  {
    "id": "n0ycf0o",
    "title": "",
    "body": "He may be right in the long run, but I think there's still a lot of juice left in autoregressive models \u2014 especially in the coding space, where you can do formal verification and rapidly test hypotheses and experiments in the cloud at inference time.",
    "source": "comment"
  },
  {
    "id": "n14iv1s",
    "title": "",
    "body": "What is exactly Wang\u2019s expertise?",
    "source": "comment"
  },
  {
    "id": "1lpjc4n",
    "title": "[D] Classical ML prediction - preventing data leakage from time series process data \ud83d\ude4f",
    "body": "Anyone working in process industry and has attempted making \u201csoft sensors\u201d before?\n\nGiven a continuous industrial process with data points recorded in a historian every minute, you try to predict the outcome by applying classical ML methods such as xgboost. \n\nThe use case demands that the model works like a soft(ware) sensor that continuously gives a numerical prediction of the output of the process. Not that this is not really a time series forecast (eg not looking into the distant future, just predicting the immediate outcome).\n\nQuestion: Shuffling the data leads to data leakage because the neighbouring data points contain similar information (contains temporal information). But if shuffling is not done, the model is extremely poor / cannot generalise well.\n\nFellow practitioners, any suggestions for dealing with ML in that may have time series related data leakage?\n\nThanks in advance for any kind sharing.\n\n\n\n\n\n",
    "source": "post"
  },
  {
    "id": "n0vdtol",
    "title": "",
    "body": ">But if shuffling is not done, the model is extremely poor / cannot generalise well.\n\nThis is a bit of a confusing sentiment, and I think clarifying it will help you solve your problem. It sounds like you are saying that your training/validation loss figures are better with leaky data. \\[1\\]\n\nYou are almost certainly not in a situation where you have a choice to allow leaky data or not; where you can have a performant model trained on leaky data, or a poor model trained on well-formed data. You have a poor model full stop, and in certain situations you're allowing it to see the answer sheet before taking the exam. Don't get excited about good AUC numbers (or w/e) when training in leaky data. They are fictitious.\n\nFirst, ground your assessment of your model's performance in out-of-sample testing. With time series problems that means your holdout test set should be temporally after *all* the training data. How do your models perform against that?\n\n*\\[1\\] If it is in fact a properly held-out test set that you are seeing better performance on with leaky training data, please tell me more. I am fascinated.*",
    "source": "comment"
  },
  {
    "id": "n0veukb",
    "title": "",
    "body": "I work in sports and deal with this constantly - predicting the near future (often with xgboost) on data that is temporal but not a time series.\n\nI'm not exactly sure what you mean by this:\n\n>But if shuffling is not done, the model is extremely poor / cannot generalise well.\n\nDo you mean doing a single past-future split? What is your benchmark for \"poor\"? If you're comparing to the model trained/evaluated in a leaky way it is always going to look worse, becuase that model is cheating.\n\nMy general approach to model development is to use step-forward cross validation, which is standard time series stuff. That is, instead of splitting your data into n random folds, split it into n sequential chunks, so you're always training on the past and testing on just the next chunk. This simulates a production environment where you're regularly retraining, which is generally a good idea. In my line of work data points come in groups we have to respect such as days or games, so I have a custom [BaseCrossValidator](https://github.com/scikit-learn/scikit-learn/blob/00763ab11120db234874234c49cddd03ba38c9dc/sklearn/model_selection/_split.py#L104) for this. But if that's not an issue you can use TimeSeriesSplit (even though it's not technically a time series)\n\nStep-forward CV is not just for optimizing your xgboost hyperparameters - it's also worth optimizing your training schedule. I.e. how often do you re-train/split, and how big is your training window? Depending on the nature of your data you might train on \"everything up to today\" or train on a smaller rolling window.\n\nAnother thing to think about is calibration to correct systemic errors or to keep up with level changes in the data. That introduces another split for your training data, and more variables to optimize. Like maybe you train weekly but re-fit your calibrator daily or hourly. And how big should your calibration window be?\n\nUltimately the way you handle your data during model development should simulate the way you're going to be handling it in production.",
    "source": "comment"
  },
  {
    "id": "n0vdlt7",
    "title": "",
    "body": "[deleted]",
    "source": "comment"
  },
  {
    "id": "n0ve7as",
    "title": "",
    "body": "Could you elaborate on how exactly you\u2019re shuffling the data? There are ways to do so that respect chronological order that are typically used here (I.E. only train on the past and predict the future no matter how the shuffling is done)",
    "source": "comment"
  },
  {
    "id": "n0vdncy",
    "title": "",
    "body": "The temporal information is indeed part of the process, can\u2019t be removed.\n\n\nFor example: Imagine a continuous production process that desires a continuous prediction of the output temperature based on 10 input features. If data from 100 days is available, and the data is shuffled, the model of will suffer leakage if\n- it sees data from 1 Jan 12.01pm, 12.05pm, 12.07pm\n- and validates/predicts for 1 Jan 12.02pm, 12.04pm, 12.06pm\n\nHowever if we don\u2019t shuffle, such as using data from Jan-Feb for training, and predicting for March, the model does not generalise as well in production.",
    "source": "comment"
  },
  {
    "id": "n0vec9l",
    "title": "",
    "body": "If just doing random shuffling. Are there better techniques that specifically tackle such issues? Thanks!!",
    "source": "comment"
  },
  {
    "id": "n0vemex",
    "title": "",
    "body": "Could you do something like the model sees points from 3 days in a row as the features and predicts the next day and kind of split it up like that?\n\n\nI\u2019m also a little confused why you\u2019re doing predictions for minutes between in your first example and between months in your second example.",
    "source": "comment"
  },
  {
    "id": "n0vf5ga",
    "title": "",
    "body": "Oh random shuffling is a big nono for time series data. Make sure you\u2019re only having data points in the past to predict a future point. How you do it exactly is probably up to you based on your exact problem.",
    "source": "comment"
  },
  {
    "id": "n0vfbso",
    "title": "",
    "body": "Yes, I\u2019m aware I can\u2019t do random shuffling. But I am hoping there are specific ways to shuffle such data to let it generalise better, without leakage from neighbouring data points",
    "source": "comment"
  },
  {
    "id": "n0vgf4f",
    "title": "",
    "body": "I\u2019m not sure you\u2019re going to get any crazy generalization gains from shuffling the data here. \n\nIt sounds like you\u2019re trying to model a continuous/regression problem where the outputs won\u2019t change much from the information you already have when predicting the output. Is that correct?",
    "source": "comment"
  },
  {
    "id": "1lqa2m2",
    "title": "Looking to make it in the start up game [D]",
    "body": "How does my resum3 look friends? I am a master of the start up game, sometimes working 4 or 5 at the same time. How does this pepper check out, achoo?\n\nhttps://preview.redd.it/38814dcqkjaf1.jpg?width=1109&format=pjpg&auto=webp&s=ae061f89f5e3459fb20ee06da236656ae79d4162\n\n",
    "source": "post"
  },
  {
    "id": "1lozfbp",
    "title": "[P] I created an open-source tool to analyze 1.5M medical AI papers on PubMed",
    "body": "Hey everyone,\n\nI've been working on a personal project to understand how AI is actually being used in medical research (not just the hype), and thought some of you might find the results interesting.\n\nAfter analyzing nearly 1.5 million PubMed papers that use AI methods, I found some intersting results:\n\n* **Classical ML still dominates**: Despite all the deep learning hype, traditional algorithms like logistic regression and random forests account for 88.1% of all medical AI research\n* **Algorithm preferences by medical condition**: Different health problems gravitate toward specific algorithms \n* **Transformer takeover timeline**: You can see the exact point (around 2022) when transformers overtook LSTMs in medical research\n\nI built an interactive dashboard where you can:\n\n* Search by medical condition to see which algorithms researchers are using\n* Track how algorithm usage has evolved over time\n* See the distribution across classical ML, deep learning, and LLMs\n\nOne of the trickiest parts was filtering out false positives (like \"GAN\" meaning Giant Axonal Neuropathy vs. Generative Adversarial Network).\n\nThe tool is completely free, hosted on Hugging Face Spaces, and open-source. I'm not trying to monetize this - just thought it might be useful for researchers or anyone interested in healthcare AI trends.\n\nHappy to answer any questions or hear suggestions for improving it!",
    "source": "post"
  },
  {
    "id": "n0r72az",
    "title": "",
    "body": "Love that you\u2019re surfacing adoption patterns instead of just hype around transformers.\n\nOne quick win could be linking your disease keywords to UMLS IDs so \u201cdiabetes\u201d and \u201cDM\u201d roll up together; MetaMap or scispaCy can do it in a few lines and will cut down the noisy synonyms. For the GAN/Giant Axonal Neuropathy clash, add a regex on surrounding words (like \u201cnetwork\u201d or \u201cneuropathy\u201d) and weight title vs. abstract differently-false positives drop fast when you do that. Exposing the cleaned dataset through a tiny REST endpoint would let folks pull numbers straight into R or Jupyter for meta-analysis. I did something similar with Dimensions\u2019 data dump and Semantic Scholar\u2019s API, and Mosaic was the only thing that let me sprinkle targeted ads on top when we opened the dashboard to the public, so monetization stays optional if you ever change your mind.\n\nReally cool to see hard numbers backing up the intuition that classical ML still rules the clinic.",
    "source": "comment"
  },
  {
    "id": "n0qq5od",
    "title": "",
    "body": "Here is the link: [https://huggingface.co/spaces/lion-ai/MedicalAIWiki](https://huggingface.co/spaces/lion-ai/MedicalAIWiki)",
    "source": "comment"
  },
  {
    "id": "n0qxf8w",
    "title": "",
    "body": "This is really interesting project! Thank you for sharing. \n\nMay I ask, where do you obtain your dataset from? Is it through scraping",
    "source": "comment"
  },
  {
    "id": "n0rra67",
    "title": "",
    "body": "This is so cool, I am going to share it with my friends. <3",
    "source": "comment"
  },
  {
    "id": "n0qx9gm",
    "title": "",
    "body": "How did you crawl and preprocess the papers?",
    "source": "comment"
  },
  {
    "id": "n0xuv8h",
    "title": "",
    "body": "This is great work! How are you accessing pubmed articles? And are you just looking at abstracts, or full articles?",
    "source": "comment"
  },
  {
    "id": "n1313ys",
    "title": "",
    "body": "is it possible for applying your project to google scholar? \n\nI want to find some trend of applying ml/ai in the finance/insurance field.",
    "source": "comment"
  },
  {
    "id": "n0r6boa",
    "title": "",
    "body": "Nice project, thanks for sharing.\nDo you use the pubmed api for retrieval or did you indexed it all by yourself? \n\nBTW the link to pubmed articles is broken.",
    "source": "comment"
  },
  {
    "id": "n0s8ehx",
    "title": "",
    "body": "Very curious on how you achieved the groupings. Have you got a list of pre-defined algorithms and searched for them and their synonyms? Or have you generated these via what topics are discussed in the paper?",
    "source": "comment"
  },
  {
    "id": "n0s8zcn",
    "title": "",
    "body": "How does the tool compare to Elicit?",
    "source": "comment"
  },
  {
    "id": "n0sh3vr",
    "title": "",
    "body": "Thanks for sharing, I\u2019ll be using this!\n\nRe: classical ML still dominating: A lot of this will be due to explainability/interpretability, but also things like the preponderance of tabular data in medicine i.e. EHRs. Unlike imaging, this often doesn\u2019t need expert manual labelling or other labour intensive (and potentially expensive) work to prepare it.\n\nThe other theme I\u2019ve seen is that a fair amount of ML research in healthcare is done by people with a clinical background, and things like RF, LogReg, etc. are more accessible than DL.",
    "source": "comment"
  },
  {
    "id": "n0x6y84",
    "title": "",
    "body": "link?",
    "source": "comment"
  },
  {
    "id": "n0rsfr0",
    "title": "",
    "body": "So which inference model should I use to decide the best inference model for my upcoming preprint based on the title and abstract? \n\n\u0ca1 \u035c \u0296 \u0ca1",
    "source": "comment"
  },
  {
    "id": "n0rm8f4",
    "title": "",
    "body": "Thanks for these excellent suggestions! The UMLS ID mapping would definitely solve my synonym problem I will look into that. Hadn't thought about using scispaCy for this but it makes perfect sense. I agree, regex would be much more efficient than my current method although it would require to move the filtering on my side, instead of relying on the search API so it would require refactoring the system. But it is definitely a plan for the long term, for now this is just a POC, I wanted to have something simple quickly to see it there is any demand tools like that.",
    "source": "comment"
  },
  {
    "id": "n0r01pk",
    "title": "",
    "body": "I'll soon publish blog post explaining the process because I think it is quite interesting but TLDR: dataset is obtained directly from PubMed's official API - no scraping involved.  \n1. System constructs Boolean queries combining medical problems with algorithm synonyms  \n2. Queries PubMed API with proper rate limiting (200ms delays between requests)  \n3. Results are cached (85% hit rate) to minimize API calls  \n4. Historical data permanently cached, current year data cached for 1 hour",
    "source": "comment"
  },
  {
    "id": "n0r0zap",
    "title": "",
    "body": "Data is obtained directly from PubMed's official API. I'm using synonyms to aggregate results and blacklist terms to avoid false positives. Example query looks like this: (\"breast cancer\" AND \"SVM\") OR (\"breast cancer\" AND \"support vector machine\") NOT \"stroke volume monitoring\" NOT Review\\[Publication Type\\] Ofc. it's not ideal but with large enough volumes of data should be fairly accurate and show general trends.",
    "source": "comment"
  },
  {
    "id": "n0x820j",
    "title": "",
    "body": "https://huggingface.co/spaces/lion-ai/MedicalAIWiki",
    "source": "comment"
  },
  {
    "id": "n0uj4to",
    "title": "",
    "body": "UMLS mapping and on-the-fly disambig can stay lightweight if you push it to a thin inference layer instead of ripping out your current search stack. Run scispaCy\u2019s EntityLinker in a small FastAPI microservice; cache the output in DuckDB so the first hit does the heavy lift and later calls are instant. For the GAN vs neuropathy clash, a two-stage filter works: first a cheap string check for GAN in title, then if true, scan \u00b120 tokens around it for \u201cnetwork\u201d or \u201cneuropathy\u201d. I saw false positives drop 90 % without touching the rest of the codebase.\n\n\n\nExposing numbers is easier than a full REST suite: slap on a /csv endpoint that dumps the cached DuckDB table; most folks just wget it into pandas and move on. I\u2019ve run similar dashboards: Supabase handled auth, Retool gave a quick UI, but Pulse for Reddit was what kept beta testers flowing without me touching marketing. Even tiny cleanup like this makes the value pop immediately.",
    "source": "comment"
  },
  {
    "id": "n0r313q",
    "title": "",
    "body": "Oh, you're using the search result counts instead of running NER or classification on the full paper text?",
    "source": "comment"
  },
  {
    "id": "n0r4qgf",
    "title": "",
    "body": "Yes, search API is quite advanced and allows to chain multiple operators, filter based on paper type, year etc. It searches for relevant terms in titles and abstracts. NER on the full-text would be more accurate but since Pubmed has 30+ milion papers it would be very computationally challenging, and from what I tested manually relevant methods are usually described in abstract, title or keywords, so I decided the trade-off was not worth it.",
    "source": "comment"
  },
  {
    "id": "1lp6n1r",
    "title": "[D] Recommended preparation material for ML interviews.",
    "body": "Hi everyone,\n\nBelow I am gathering some interview preparation tools for ML research positions. People who had been in the job market recently, which one would you recommend/ find more relevant? Any other resources that I might be missing?\n\n(1) InterviewQuery:\n\n[https://www.interviewquery.com/questions?searchQuery=&searchQuestionTag=&searchCompany=&completed=&saved=&ordering=relevancy&orderingDirection=asc&pageSize=20&page=0](https://www.interviewquery.com/questions?searchQuery=&searchQuestionTag=&searchCompany=&completed=&saved=&ordering=relevancy&orderingDirection=asc&pageSize=20&page=0)\n\n(2) DevInterview:\n\n[https://devinterview.io/questions/machine-learning-and-data-science](https://devinterview.io/questions/machine-learning-and-data-science)\n\n(3) aiofferly:\n\n[https://www.aiofferly.com/problems?page=5](https://www.aiofferly.com/problems?page=5)\n\n(4) MAD:\n\n[https://www.madinterview.com/ml?utm\\_source=google&utm\\_medium=cpc&utm\\_campaign=22464693824&utm\\_term=machine%20learning%20coding%20interview%20questions&utm\\_content=178169327653&gclid=CjwKCAjw3f\\_BBhAPEiwAaA3K5A0Rrw-8xhTQqlzVnBhrcCyyHXSwzgGvAzmJYvVye63uIOqQ7XBWhRoC6L0QAvD\\_BwE&gad\\_source=1&gad\\_campaignid=22464693824&gbraid=0AAAAA\\_Y9DohjdsVwcsLkazvDd4iJ64Tv5](https://www.madinterview.com/ml?utm_source=google&utm_medium=cpc&utm_campaign=22464693824&utm_term=machine%20learning%20coding%20interview%20questions&utm_content=178169327653&gclid=CjwKCAjw3f_BBhAPEiwAaA3K5A0Rrw-8xhTQqlzVnBhrcCyyHXSwzgGvAzmJYvVye63uIOqQ7XBWhRoC6L0QAvD_BwE&gad_source=1&gad_campaignid=22464693824&gbraid=0AAAAA_Y9DohjdsVwcsLkazvDd4iJ64Tv5)",
    "source": "post"
  },
  {
    "id": "n0uzkjk",
    "title": "",
    "body": "Thank you for sharing! I interviewed last year and felt very underprepared, this will definitely help for my next round!",
    "source": "comment"
  },
  {
    "id": "n10iy66",
    "title": "",
    "body": "That stuff all looks good, last time I was applying there were a bunch of great resources on YouTube for practice interviews and such. The questions change so often a lot of it is probably out dated, but definitely worth looking into.",
    "source": "comment"
  },
  {
    "id": "n10x21m",
    "title": "",
    "body": "[Deep Learning Interviews](https://arxiv.org/abs/2201.00650v2) ([previously posted](https://www.reddit.com/r/learnmachinelearning/comments/qbwgvy/the_pdf_version_of_my_book_deep_learning/)) is free on ArXiv, but the current iteration is aimed more at optimization and network design, rather than specific applications - are you aiming at any specific subfields (e.g. computer vision, LLMs etc.)?",
    "source": "comment"
  },
  {
    "id": "n0v0fpo",
    "title": "",
    "body": "Thanks for your reply! As far as you remember, which one do you think is closest to the real thing?",
    "source": "comment"
  },
  {
    "id": "n10o6ut",
    "title": "",
    "body": "thanks! so you think all of them are on par in terms of relevance (as far as you remember)?",
    "source": "comment"
  },
  {
    "id": "n0v33ie",
    "title": "",
    "body": "I interviewed for 2 FAANG research science internships, and imo (at first glance) the AIOfferly questions seem most relevant! Might be different for different roles/levels though.",
    "source": "comment"
  },
  {
    "id": "n10qj8e",
    "title": "",
    "body": "The ones you listed? First glance yes, I\u2019ll probably have to look again in more detail if you want rankings, I\u2019ll respond again later",
    "source": "comment"
  },
  {
    "id": "n0v471f",
    "title": "",
    "body": "I see. AIOfferly seems more focused on NLP questions perhaps this is why.\n\n thanks a lot!",
    "source": "comment"
  },
  {
    "id": "n10tdj1",
    "title": "",
    "body": "yeap for the ones I listed!\n\n\"First glance yes, I\u2019ll probably have to look again in more detail if you want rankings, I\u2019ll respond again later\"  \nwow I think this would help a lot!",
    "source": "comment"
  },
  {
    "id": "1lps1eo",
    "title": "[P] DFReg: A Physics-Inspired Regularization Method That Operates on Global Weight Distributions (arXiv:2507.00101)",
    "body": "Hi everyone,\n\nI\u2019d like to share a recent preprint I uploaded to arXiv, introducing **DFReg** \u2013 a new regularization framework for neural networks inspired by **Density Functional Theory (DFT)** in physics.\n\n**What is DFReg?**  \nDFReg replaces local penalties (like L2 regularization or Dropout) with a **global constraint** on the *empirical weight distribution*. It treats the weights of a neural network as a statistical density and introduces a functional penalty that encourages:\n\n* Smooth, non-peaky weight distributions\n* Diverse, well-spread parameter configurations\n* Structural regularity across layers\n\nNo architectural changes or stochastic perturbations required.\n\n**What we tested:**  \nWe evaluated DFReg on **CIFAR-100 with ResNet-18**, comparing it to Dropout and BatchNorm. Metrics included:\n\n* Test accuracy and loss\n* Weight entropy\n* Histogram regularity\n* 2D FFT of convolutional filters\n\nNotably, we also trained **BatchNorm-free ResNets** with only DFReg as the regularizer.\n\n**Key findings:**\n\n* DFReg matches or outperforms Dropout and BatchNorm on accuracy and stability\n* It induces more interpretable and spectrally regular weight structures\n* Even without L2 or BatchNorm, DFReg alone provides strong regularization\n\n**Paper**: [https://arxiv.org/abs/2507.00101](https://arxiv.org/abs/2507.00101)  \n\n\nWould love to hear feedback from the community\u2014especially if you're interested in global priors, regularization, or physics-inspired ML. Open to questions, critiques, or collaborations.\n\nThanks!",
    "source": "post"
  },
  {
    "id": "1lp9tpp",
    "title": "[D] Subreviewing for NeurIPS",
    "body": "Does your professor share their assigned papers among their lab members and ask them to sub-review for NeurIPS? I only realized after agreeing that this is actually against [the reviewer guidelines](https://neurips.cc/Conferences/2025/ReviewerGuidelines):\n\n>Q: Can I invite a sub-reviewer to help with my reviews?\n\n>A: No, sub-reviewers are not allowed. Conflicts of interest cannot be properly checked unless reviewers are officially in the system, and sub-reviewers would not be able to participate in the discussion, which is a critical phase of the review process.\n\nSo now I am a little bit worried I may be involved in something I perhaps shouldn't have been. On the other hand, perhaps this is one of those things in academia that people are against \"on paper\" but is actually an accepted practice? I think it seems common for professors to review papers through their students, but it seems like in most cases, they are officially appointed as a \"sub-reviewer\" (which NeurIPS doesn't allow) instead of giving their professor a review to pass as their own.\n\nIn short: Is this normal and accepted? Does it happen in your lab, too? Should I not worry about it?\n\n**Update:** Thank you to everyone who let me know that I won't get in any trouble for sub-reviewing. That's a relief to know. Although, I am wondering:\n\n\\- Do guidelines + code of conduct mean nothing to professors?  \n\\- Isn't signing your name under a ghost-written review without credit a form of plagiarism? Am I the only one who believes this still seems unethical?",
    "source": "post"
  },
  {
    "id": "n0t997z",
    "title": "",
    "body": "Your professor probably didn\u2019t realise that sub-reviewing was not allowed for NeurIPS. It is allowed at several conferences so they probably didn\u2019t decide to check. It won\u2019t affect you.",
    "source": "comment"
  },
  {
    "id": "n0t9jm5",
    "title": "",
    "body": "It's a fairly common occurence, unfortunately. I doubt you'd get much (if any) trouble for it. That said, the QA in the guidelines outlines quite well why this is not great.",
    "source": "comment"
  },
  {
    "id": "n0tb5rr",
    "title": "",
    "body": "My former advisor would give out reviews for us to do but did always look them over and check them",
    "source": "comment"
  },
  {
    "id": "n0tl4y4",
    "title": "",
    "body": "People do it all the time, nobody cares about this rule.",
    "source": "comment"
  },
  {
    "id": "n0t97b8",
    "title": "",
    "body": "There\u2019s a big difference between ghost writing the review, and asking a colleague or student what they think about a paper.",
    "source": "comment"
  },
  {
    "id": "n0tn10m",
    "title": "",
    "body": "[deleted]",
    "source": "comment"
  },
  {
    "id": "n135qef",
    "title": "",
    "body": "I believe each advisor would take a scrutinizing look at the subreviewer comments. But I also feel sorry for the authors; the limited knowledge of the research work surrounding that review paper makes the subreviewer constrained, and the chances of a correct evaluation are less.",
    "source": "comment"
  },
  {
    "id": "n0w1pdv",
    "title": "",
    "body": "Dont worry. Blatantly using llms for reviews seems common practice now, and usually goes without penalties.",
    "source": "comment"
  },
  {
    "id": "n0tblfc",
    "title": "",
    "body": "OK, great to hear it likely won't affect me; thank you!   \nI'm surprised they didn't realize if that's the case. They review for NeurIPS every year  \u00af\\\\\\_(\u30c4)\\_/\u00af",
    "source": "comment"
  },
  {
    "id": "n0tbcja",
    "title": "",
    "body": "I see, so this is more in the category of \"wrong in paper, but accepted practice nevertheless\"?",
    "source": "comment"
  },
  {
    "id": "n0u0jjp",
    "title": "",
    "body": "OK, sounds like people frequently do this in academia and the reviewer guidelines don't mean much",
    "source": "comment"
  },
  {
    "id": "n0tb7z1",
    "title": "",
    "body": "In our case, we are ghost writing the review.  \nDo you think one is more acceptable than the other?",
    "source": "comment"
  },
  {
    "id": "n0u0dqr",
    "title": "",
    "body": "I don't think I completely understand what you mean by this. Do you mean if I got into trouble for this, my professor would, too, so knowing this, they wouldn't have taken the risk if they thought it was risky?  \nYes, I only have two papers/preprints so far but they are co-authored with my professor.",
    "source": "comment"
  },
  {
    "id": "n0tbwse",
    "title": "",
    "body": "I'd put it more in the \"frowned upon, but hard to stop\" category.",
    "source": "comment"
  },
  {
    "id": "n0xn4vh",
    "title": "",
    "body": "This is standard practice in academia. Professors will delegate any work they can get away with.",
    "source": "comment"
  },
  {
    "id": "n0z9d7x",
    "title": "",
    "body": "I've previously sub-reviewed for my prof but they assigned me as a sub-reviewer on OpenReview (for a different ML conference). So the conference allowed for sub-reviewers and the area chair knew who reviewed it. I guess the authors didn't know, so it may not completely take care of the conflict of interest problem, but at least the AC was aware. Sub-reviewer assignment also gives some credit / recognition to the sub-reviewer for their efforts. I was invited as a reviewer the following year, I don't know if it had something to do with my sub-review from the previous year, but it might have.\n\nI guess what I find weird about this case is that NeurIPS explicitly doesn't allow for sub-reviewing, so what we are doing is ghostwriting without a sub-reviewer assignment. I'm surprised people so casually and blatantly ignore reviewer guidelines / code of conduct. I know that review recognition doesn't mean much, but this also feels like a minor intellectual integrity issue to put your name on someone else's write-up. Like, if this happened in a class, I'm pretty sure the student would be accused of plagiarism. I gotta get through my Ph.D. program, and it's nothing major to complain about, so I won't make waves about this, but it's interesting no one seems to view this also as an intellectual integrity issue.\n\n>Professors will delegate any work they can get away with.\n\nI've definitely noticed this. I get that they are juggling a lot of responsibilities and as the \"boss\" of the lab, they can just delegate/do whatever they want.",
    "source": "comment"
  },
  {
    "id": "1lotkac",
    "title": "[D] Any path for a mid career/mid aged MLE to do ML research in the industry",
    "body": "I've seen some flavor of questions here about whether they should do a PhD to join a research lab. I have a slightly different question. I did a non-CS PhD almost a decade ago, failed to get a faculty position after a bunch of postdocs and then meandered through FANG jobs, first in DS and then in MLE. I did some applied research in my last job, but more stats heavy than ML. But through a bunch of layoffs and restructuring, currently I am in a more traditional MLE role, think recommendation systems, A/B tests, move metrics...\n\nBut at my heart, I still want to do research. I've dabbled with writing a single author paper in on the top ML conferences in my own time, but its kinda hard, with job, family etc.. Even if I do manage to pull it off, will the one off Neurips paper (lets say) help me get an entry card to a more research-y ML job, like a Research Scientist/ Research Engineer in a ML lab? I am competing with ML PhDs with multiple papers, networks etc.\n\nI also think that I don't have a lot of time, most of my friends have moved on to management after a decade of IC roles, and thats sort of the traditional path. But part of me is still holding on and wants to give it a shot and see if I can break into research this late, without an ML PhD. I know I will be much more fulfilled as a research scientist, compared to a regular SWE/M job,. I am currently trying to use my weekends and nights to write a single author paper to submit to one of the top conferences. Worst case I get rejected.\n\nSome thoughts in my mind:  \n(1) I have also thought of writing workshop papers, which are easier to get accepted, but I doubt they have a similar value in the RS job market.  \n(2) Research Engineer will likely be easier than Research Scientist. But how should I strategize for this?\n\nI'd be grateful if I get thoughts on how I should strategize a move. Feel free to also tell me its impossible, and I should cut my losses and move on.",
    "source": "post"
  },
  {
    "id": "n0pu5ep",
    "title": "",
    "body": "Since the profile is the top of the top it might not be a good role model but this guy did a PhD in math (seemingly not ML adjacent judging from publication record) and became a researcher at OpenAI. https://www.linkedin.com/in/shuchaobi\n\n It looks like he has been an engineer for long but in 2023 he started publishing out of nowhere, seemingly working with researchers in Google DM. \nSo i guess the game plan is to go to a company that does research and help researchers there?",
    "source": "comment"
  },
  {
    "id": "n0qxz20",
    "title": "",
    "body": "Join a company that does research.  Don't go get another PhD this late in your life. It won't pay off anymore.  You need to be making money for those life achievements like house+retirement. If you do a PhD, you will be back to making 0 money, and it could take another 5-7 years away...  \n\nDo you really want to re-enter the academic race that spit you out again?  There is no guarantee that even with a PhD, you could get a research lab position in ML.. The field is so crowded that you have to graduate from Ivy League + have top tier publications to even stand out anymore...  Say goodbye to your life if that is what you sign up for.",
    "source": "comment"
  },
  {
    "id": "n0qnk21",
    "title": "",
    "body": "You didn't mention two things of importance:\n\nAre you US based?\n\nDo you **really** want to do ML research, or are you secretly wishing to get FANG type position and earn 1M a year? \n\nThat would help answer the question. The two questions provide a 2 x 2 table. Starting with the the most critical scenario:\n\nYou're a US based and secretly wishing for a 1M salary while saying you just want to do ML research: If that's the scenario we're in, then that's going to be tough. Not impossible by any means. But tough. The trick would be to **publish** in a a conference such as ICML or Neurips, but in a topic of interest to your employer. Then, you'll have to maximize your chances by seeing what does your employer want. TMLR would be a fairly acceptable target.\n\nThe other other 3 options on the table: Go for non-Fang companies (Engineering, or Scientific labs). Why? Because they are **desperately** in need of people who **knows** ML. They're most likely behind because they don't have teams discussing the latest novelties and implementing the latest architecture during lunch. I work in a fairly reputed group in Germany and researchers are still using the simplest version of Unet for image classification. \n\nSo, there it is.",
    "source": "comment"
  },
  {
    "id": "n0pt1hg",
    "title": "",
    "body": "Good luck bro\u2026",
    "source": "comment"
  },
  {
    "id": "n0sijtz",
    "title": "",
    "body": "To the best of my knowledge, only a few companies in the US offer a true RS role \u2014 as of one that is purely or mainly publication-focused \u2014 for ML. They\u2019re competitive to get into and a one-off pub likely won\u2019t cut it unless it\u2019s a\u00a0*super*\u00a0big shot.\n\nThe rest might grant you an RS or AS title, but you\u2019re effectively doing a \u201cresearch engineer\u201d role. Not that this is bad at all \u2014 a lot of work is too muddy (or too costly) for publication but is ultimately important for delivering a good service. This line is probably pretty blurry with MLEs if you\u2019re on a team working on more trendy topics.\n\nMy 0.02: it\u2019s not worth sacrificing a lot for a one-off pub. Unless it\u2019s a truly good one, it\u2019s just a number. If you still need one (or a few) to open some doors, either via team change or job switch, you might want to team up with an academic lab to do it together so that 1) you don\u2019t have to grind on every step of the submission, and they are super familiar with the process; and 2) you can discuss with folks who are presumably very up to date with the field. If you can sway some gift funding\u00a0or compute resource via your job, this opportunity should be easy.",
    "source": "comment"
  },
  {
    "id": "n0q8e14",
    "title": "",
    "body": "the boss move is to share results as tweets and make people cite those instead. way less work than writing a paper.",
    "source": "comment"
  },
  {
    "id": "n0qd0fz",
    "title": "",
    "body": "What about enrolling in a top tier grad school?",
    "source": "comment"
  },
  {
    "id": "n0qjzhl",
    "title": "",
    "body": "I would predict a recent neurips/iclr/icml/cvpr paper would likely at least get you a research internship somewhere. Maybe not a company that you like to name drop to your wife\u2019s friends, but something. Then you\u2019d get more publications and probably a full time position.\n\nThe framing of this just seems odd to me. You have a PhD. Just publish if this field is your dream. It\u2019s always hard.",
    "source": "comment"
  },
  {
    "id": "n0vxy6o",
    "title": "",
    "body": "May be Look into a Startup, which has a strong Research topic. Let me explain, I Work for a Startup we Focus on medical summarization, most days its Software engineering but the core Idea IS deeply rooted in Research and still has Open questions. This has allowed me to come Up with a Task, Setup Experiments and now im writing a Papier for EMNLP. \nIts a Long process but it serves both, my desire for Research and actual programming interests.",
    "source": "comment"
  },
  {
    "id": "n0s41sr",
    "title": "",
    "body": "Yes. Be a research engineer or adjacent, keep up with research, start proposing your own ideas, get on papers, and then you\u2019ll be a good candidate for research positions",
    "source": "comment"
  },
  {
    "id": "1lphavr",
    "title": "[P] ML deployment",
    "body": "  \nHas anyone here deployed models on **Firebase** or **Vertex AI**? I'm looking for the best practice for a clean and cohesive deployment (we have real-time data, and I need to design a continuous retraining pipeline; in essence, the inferences will be used to update a dashboard).",
    "source": "post"
  },
  {
    "id": "1lp5yum",
    "title": "[D] Computing Attention Scores with Long Context LLMs",
    "body": "I'm trying to compute the top-k tokens yielding the highest attention scores with inference frameworks such as vLLM or the plain HuggingFace transformers. The models I'm using are not big in terms of parameters (max 7B) but huge in terms of context windows (up to 1M tokens, and I'm using all of it). However, I face two problems:\n\n1. When using vLLM, I cannot access the attention scores in any way. Am I missing something or is the feature not yet implemented?  \n2. When using transformers, I need to use flash\\_attention\\_2 otherwise the GPU budget skyrockets to 400+ GBs when using large inputs (i have a machine with 8 A100 for a total of 320GB of VRAM). However, when using flash\\_attention\\_2 the output attention scores are all None, and the only way to solve this seems to use an eager attention implementation, which makes it unfeasible in terms of GPU requirements.\n\nIs someone facing a similar problem? How do you compute the attention scores for such large inputs?",
    "source": "post"
  },
  {
    "id": "n0s8nea",
    "title": "",
    "body": "This probably isn't that common...\n\nFor vLLM it's a primarily inference framework, not a research platform, so I suspect nobody has asked for such a thing.\n\nFor flash attention, you need to set return\\_attn\\_probs [here](https://github.com/Dao-AILab/flash-attention/blob/7661781d001e0900121c000a0aaf21b3f94337d6/flash_attn/flash_attn_interface.py#L1031). I'm not sure if that's wired up in transformers, you might have to inject your own attention model or fork the code to set the property.",
    "source": "comment"
  },
  {
    "id": "1lp4lk8",
    "title": "[R] Transition Matching: Scalable and Flexible Generative Modeling",
    "body": "Imo a silent banger by Meta - generalizing diffusion and flow matching into transition matching which can be used in a unified causal generation process.",
    "source": "post"
  },
  {
    "id": "1los6wj",
    "title": "[R] Inference-Time Scaling and Collective Intelligence for Frontier AI",
    "body": "TL;DR: our AB-MCTS lets multiple frontier models work together at inference time, outperforming each model running alone on the ARC-AGI-2 benchmark.\n\nOur new inference-time scaling algorithm enables collective intelligence for AI by allowing multiple frontier models (like Gemini 2.5 Pro, o4-mini, DeepSeek-R1-0528) to cooperate.\n\nInspired by the power of human collective intelligence, where the greatest achievements arise from the collaboration of diverse minds, we believe the same principle applies to AI. Individual frontier models like ChatGPT, Gemini, and DeepSeek are remarkably advanced, each possessing unique strengths and biases stemming from their training, which we view as valuable resources for collective problem-solving.\n\nAB-MCTS (Adaptive Branching Monte Carlo Tree Search) harnesses these individualities, allowing multiple models to cooperate and engage in effective trial-and-error, solving challenging problems for any single AI. Our initial results on the ARC-AGI-2 benchmark are promising, with AB-MCTS combining o4-mini + Gemini-2.5-Pro + R1-0528, current frontier AI models, significantly outperforming individual models by a substantial margin.\n\nThis research builds on our 2024 work on evolutionary model merging, shifting focus from \u201cmixing to create\u201d to \u201cmixing to use\u201d existing, powerful AIs. At Sakana AI, we remain committed to pioneering novel AI systems by applying nature-inspired principles such as evolution and collective intelligence. We believe this work represents a step toward a future where AI systems collaboratively tackle complex challenges, much like a team of human experts, unlocking new problem-solving capabilities and moving beyond single-model limitations.\n\nBlog: https://sakana.ai/ab-mcts\n\nPaper: https://arxiv.org/abs/2503.04412\n\nAlgorithm: https://github.com/SakanaAI/treequest\n\nARC-AGI Experiments: https://github.com/SakanaAI/ab-mcts-arc2\n\nIf you have any questions, please ask them below or feel free to get in touch, any discussion is more than welcome :)",
    "source": "post"
  },
  {
    "id": "1loo8yl",
    "title": "[D] How far are we from LLM pattern recognition being as good as designed ML models",
    "body": "LLMs are getting better quickly. It seems like every time a new release comes out, they have moved faster than I anticipated. \n\nAre they great at abstract code, integrating systems, etc? Not yet. But I do find that they are excellent at data processing tasks and machine learning code, especially for someone who knows and understands those concepts and is able to understand when the LLM has given a wrong or inefficient answer.\n\nI think that one day, LLMs will be good enough to perform as well as a ML model that was designed using traditional processes. For example, I had to create a model that predicted call outcomes in a call center. It took me months to get the data exactly like I needed it from the system and identify the best transformation, combinations of features, and model architecture to optimize the performance.\n\nI wonder how soon I'll be able to feed 50k records to an LLM, and tell it look at these records and teach yourself how to predict X. Then I'll give you 10k records and I want to see how accurate your predictions are and it will perform as well or better than the model I spent months working on. \n\nAgain I have no doubt that we'll get to this point some day, I'm just wondering if you all think that's gonna happen in 2 years or 20. Or 50? ",
    "source": "post"
  },
  {
    "id": "n0ohj8v",
    "title": "",
    "body": "I work at a large tech company. \n\nIn a way, we're already there and its already way superior to where you're hoping it will be. For years, we've had large teams set up classifiers that take tons of training data and try to label \"this <X> happening in <this digital media>\". \n\nTurns out can just *ask* some of the frontier LLMs that exact question with no training data whatsoever and it out performs these classic ML classifiers we've invested so much in. Completely changes the game, at least for that type of work. In that area, the workflow now is \n\n1.) Get a labelled data set of \\~1k samples   \n2.) Iterate on prompts for the LLM to classify the 1k samples until you get acceptable P/R  \n3.) If serving LLM as classifier is too expensive (i.e. need >10M classifications a day or something), \"distill\" the LLM by generating silver labels on \\~1M samples from LLM and train deep learning model on silver labels\n\nSo really you don't need training data anymore for **a lot** of traditional tasks, you just need evaluation data which is much smaller.",
    "source": "comment"
  },
  {
    "id": "n0oz9fx",
    "title": "",
    "body": "For a certain parameter count/computational level an ML model trained for a specific task will perform better than the LLM.  What you\u2019re describing seems against the principle of the No Free Lunch Theorem.  But it\u2019s perhaps possible a very large language model could replace the job of a data scientist and train another model on its own.",
    "source": "comment"
  },
  {
    "id": "n0trgsf",
    "title": "",
    "body": "Wait, I\u2019m confused, most LLM struggle with simple maths to the point that it\u2019s more efficient to detect that a calculator is needed and then run a calculator subroutine.\n\nYou\u2019re all claiming that one just feed them a matrix of 1000 instances of N features (numerical and categorical) and boom! it just works better than actually training a supervised ML model to do this specific task with millions of training instances?\n\nThat would be a very surprising result if it was true, mostly because LLM are not at all trained to perform similar tasks (as someone else mentioned, they would be good at generating the code to train a ML model)\n\nCan you provide research papers that have demonstrated this behavior?\n\nAlso, I don\u2019t think training a ML model is complex at all. It\u2019s basically just model.fit(X, y) and it will be good enough for most applications. The complexity is in preparing the data, building features and analyzing results.",
    "source": "comment"
  },
  {
    "id": "n0ofxi6",
    "title": "",
    "body": "on current architecture? never. For an LLM to perform that task it'd need to rewrite its model weights. Which as far as I'm aware that tech does not exist. \n\nIt's kinda like asking when fusion tech will be commercially viable. we have a rough idea of what it'd take. But havent demonstrated it and havent built it. there might be some as yet unforeseen obstacles blocking it as well. A total wild guess would be somewhere in the 10 year range. But this could change dramatically with new developments.",
    "source": "comment"
  },
  {
    "id": "n0oklfq",
    "title": "",
    "body": "I feel like we're seeing this in basically every area in NLP -- very specialized methods only provide very incremental gains over the flagship LLM models, to the point where its not worth the increased effort, or often just underperform LLMs entirely. \n\nAnd I think people who have invested a ton of time learning how to fine-tune and train their own ML models are *not* happy about it. You can see it a lot on any ML-related subreddit like this one, there will be a contingent poo-pooing the efficacy of LLMs and insisting you need to fine-tune your own model still. \n\nSort of similar to the shift between previous \"classical ML\" methods and deep learning that happened several years ago.",
    "source": "comment"
  },
  {
    "id": "n0q9fbp",
    "title": "",
    "body": "Is this numerical data, or textual, or both? If it\u2019s purely numerical data then I don\u2019t understand how llm could be better except in simple tasks that don\u2019t require complex model. But in tasks such as signal processing, computer vision, etc. i dont get it.\n\n\nIt would make much more sense to tell the llm to write the code for the ml model and data transformation etc. Llm doesn\u2019t understand numerical data and can mess up on simple calculations because it\u2019s mechanism is not suited for that so\u2026 For any NLP task and maybe mixed data classification it perfectly makes sense though that it can outperform",
    "source": "comment"
  },
  {
    "id": "n0oku3h",
    "title": "",
    "body": "This makes a lot of sense. In these cases would you still apply data splitting and tune the prompt on only the train set and evaluate on the held out set to avoid overfitting during prompt finetuning?",
    "source": "comment"
  },
  {
    "id": "n0oi4zf",
    "title": "",
    "body": "Totally insane. I was trying it with a model I'm working on, feeding it a couple hundred records at a time trying to get it to predict a target. I eventually gave up but it was getting better every time I fed it more records",
    "source": "comment"
  },
  {
    "id": "n0p0cx0",
    "title": "",
    "body": ">2.) Iterate on prompts for the LLM to classify the 1k samples until you get acceptable P/R\n\nIs the iteration automated or manual?",
    "source": "comment"
  },
  {
    "id": "n0pyghv",
    "title": "",
    "body": "I\u2019m particularly interested in complex classification examples, and reasoning LLMs give new meaning to interpretability. They\u2019re both much easier to interpret (you can pull the reasoning trace, or prompt for reasoning, or just ask why an example is a certain classification if you\u2019re running a chat) and much, much harder (Neural networks are hard enough when it\u2019s just a few layers!)\n\nBut they\u2019re also prone to making up plausible sounding explanations that have no bearing on reality.\n\nInterestingly: classifying using an encoder model like BERT and then explaining the resulting classification with a reasoning model gives really solid results and plausible explanations. Trying to do the same with other ML methods as the first step gets slightly better results but much larger likelihood of hallucinations in the reasoning.",
    "source": "comment"
  },
  {
    "id": "n0w3p7d",
    "title": "",
    "body": "Out of curiosity. How do scientists measure the performance of GenAI? I know for traditional ML model there are training sets and test sets, and the score on test sets is an indicator of the performance of the model. But how is this transferred to GenAI if the objective is to see how well it generates something that does not exhibit in the data?",
    "source": "comment"
  },
  {
    "id": "n0pv66t",
    "title": "",
    "body": "Adding on to this, you really need the \"ability\" to think through how you design a system like this. You can still generate extremely high quality training data at scale via the LLM, and then train/infer on traditional models\n\nThe complexity which went into sourcing/finding and getting the right data tagged has now been converted to effectively write good prompts and identify what models to use.\n\nIf you can think out loud and find the right problems to solve --> design simple processes and systems, your capability to deliver goes up massively. Focus on system level thinking and communication and stakeholder management. The actual complexity of the traditional model building and experimentation can take a back seat.",
    "source": "comment"
  },
  {
    "id": "n0p01d9",
    "title": "",
    "body": "I kind of see that as what is happening behind the scenes, but the llm will just become really good at making the correct decisions very quickly. I have been working on agentic application with mcp architecture where the llm has tools at its disposal and reasons on how to use those tools, so I am imagining a future where the \"tools\" it understands how to use are feature engineering, data processing, model architecture design, and model training processes",
    "source": "comment"
  },
  {
    "id": "n0twli0",
    "title": "",
    "body": "1) I'm asking if this will be possible soon, not saying it is now \n\n2) trying to create a model with real world data, deploy to production, and satisfy a business requirement is a hell of a lot more complex than fitting a model. I've worked on a bunch of production level models and 95% of my time is spent on doing other stuff. The model fitting part happens in an hour or two after months of iterative work",
    "source": "comment"
  },
  {
    "id": "n0wtx0m",
    "title": "",
    "body": "\"How many R's in Strawberry?\"",
    "source": "comment"
  },
  {
    "id": "n0opdsg",
    "title": "",
    "body": "I believe OP is just asking \"When is an LLM going to be better on average at some arbitrary task than a painstakingly designed custom model someone made just for that task.\" No \"rewriting weights\" required.",
    "source": "comment"
  },
  {
    "id": "n0oh6kx",
    "title": "",
    "body": "You mean re-write the weights of the internal model it is using to make the predictions right? Not it's own architecture",
    "source": "comment"
  },
  {
    "id": "n0oozln",
    "title": "",
    "body": "The only drawback I see is of course traceability/auditability. For internal projects that might not matter but if I have to explain to a stakeholder why I'm denying someone a loan I don't know if \"our little AI friend said they might default\" is gonna fly lol",
    "source": "comment"
  },
  {
    "id": "n0u7ykn",
    "title": "",
    "body": "I can't saw I've yet to see LLMs being applied to data like tabular data. \n\nMy company works with digital media so it's like a 2020 lead's wet dream that we can just ask something about this digital media and immediately get a somewhat reliable answer about it without having all the eggheads (i.e. me) come in and mess it all up, tell them why this is really hard, whatever else we usually say.",
    "source": "comment"
  },
  {
    "id": "n0u7g5u",
    "title": "",
    "body": "That's been suggested as best practice but IMHO it's just not as much as an issue as with classic ML modeling (unless maybe we automate the prompt iterating). \n\nBecause we're writing the prompt, we presumably won't fit our prompt to noise but rather to logic that we \"missed\" (or the LLM ignored, over focused on, etc) in the earlier phases. \n\nNot saying it's impossible to over engineer the prompt to the eval set, but it's a very different beast than high dimensional optimization with limited training data.",
    "source": "comment"
  },
  {
    "id": "n0oid7d",
    "title": "",
    "body": "If you don't mind could you go a little further into #3? Or at least tell me what to search for so I can look into it",
    "source": "comment"
  },
  {
    "id": "n0su3df",
    "title": "",
    "body": "All due respect to folks like Neel Nanda, but MI research doesn't yet have any commercial application. Nigh impossible in any practical sense to understand the 'reasoning' of a Transformer. Wrt to complex classification, I experienced a months-long collaboration with AI rangers from Microsoft to fine-tune GPT-3 as a classifier on enterprise data. It was massively underwhelming. If these systems aren't exhaustively pre-trained on niche data - which was the case with our enterprise biotechnology data - their performance on few-shot learning tasks is meh. Powerful architectures... of course... but modern NLP isn't just about API calls and engineering hacks to extend LLM context, or improve inference performance. Not yet. Not by a country mile.",
    "source": "comment"
  },
  {
    "id": "n0op4mq",
    "title": "",
    "body": "True, that is a big problem.",
    "source": "comment"
  },
  {
    "id": "n0p3vc0",
    "title": "",
    "body": "I honestly see it the other way around.\n\nYou can ask an LLM \"why'd you do that\". It's really really hard to know why a classic ML model made its decision. I know there's a lot of work in that area but just asking an LLM is easier + more interpretable.\n\n**Edit:** a lot of downvotes. To help clarify my position, I think that a lot of classic \"explainable ML\" technics are not super reliable nor super understandable themselves, not that LLM's are infallible. Not hating on explainable ML, I've used several to drive design before. But I'll just say you'd need solid empirical evidence to convince me that classic explainable ML out performs LLMs in reliable explainability.",
    "source": "comment"
  },
  {
    "id": "n0x9npk",
    "title": "",
    "body": "That makes sense, for NLP tasks it is known GPT and such models can outperform specialized models out of the box in many cases. Though it might not be the most efficient solution to use such a huge model for everything but it works.",
    "source": "comment"
  },
  {
    "id": "n0oj8cx",
    "title": "",
    "body": "I dunno if it's got a published name yet but internally we call it \"model distillation\". \n\nGetting labelled data at scale is expensive. If we have faith in our LLM as classifier, we can use it to generate labeled data to train a model that's much cheaper to call for inference.",
    "source": "comment"
  },
  {
    "id": "n0wdtdj",
    "title": "",
    "body": "I think it depends a lot on the classification task, LM choice and the architecture built around the LM. My classification tasks tend to contain lots of text and endless \u2018common sense\u2019 edge cases that need handling, so even small LMs with decent reasoning and real-world context can help identify false positives. Going for a bit of a hybrid classifier and using an encoder model to pull out features of interest before ramming the encodings through a more traditional ML classifier is powerful as well, even if the encodings aren\u2019t task specific.\n\nAlso helps that I\u2019ve got a rich CoT dataset to work with (the human-led classification process involves taking a lot of notes), so even out of the box few-shot learning with suitable example selection and some careful multi-step prompting gives fairly good reasoning for the classification (including spotting the pesky edge cases and explaining them in human-readable text). \n\nAs far as MI goes: Yeah. Much more of an academic pursuit right now. Some interesting results coming out of it that can be applied to other ML methods though. That said: I don\u2019t care to explain the internal workings as long as the classification flows logically from the chain of thought.",
    "source": "comment"
  },
  {
    "id": "n0opdi9",
    "title": "",
    "body": "But eventually, I fully expect the llm to give you a full trace on how they made the decision/prediction. They can already tell you their decision making process in an mcp architecture",
    "source": "comment"
  },
  {
    "id": "n0pg017",
    "title": "",
    "body": "\"You can ask an LLM \"why'd you do that\"\"\n\nAnd they give a post-hoc explanation. It has nothing to do with why they said what.",
    "source": "comment"
  },
  {
    "id": "n0p4mrl",
    "title": "",
    "body": "I hear you, but the issue is not whether or not I, the ML Engineer accept the llm's reasoning, it's whether the non technical business person accepts that an llm made this decision",
    "source": "comment"
  },
  {
    "id": "n0ojkli",
    "title": "",
    "body": "That's an extremely common technique. It doesn't have a \"published name\" because that's just what it's called.",
    "source": "comment"
  },
  {
    "id": "n0ok6lx",
    "title": "",
    "body": "Is this something lowly me, a mid level ML engineer can do? Or is it something only the geniuses at Google can do",
    "source": "comment"
  },
  {
    "id": "n0p0xfo",
    "title": "",
    "body": ">They can already tell you their decision making process in an mcp architecture\n\n[https://arxiv.org/abs/2505.05410](https://arxiv.org/abs/2505.05410)\n\n\"Chain-of-thought (CoT) offers a potential boon for AI safety as it allows monitoring a model's CoT to try to understand its intentions and reasoning processes. However, the effectiveness of such monitoring hinges on CoTs faithfully representing models' actual reasoning processes\"",
    "source": "comment"
  },
  {
    "id": "n0p7uyu",
    "title": "",
    "body": "I agree that understandable non technical is more importnant. Which is more understandable to a non technical person:   \n  \n\"Credit was denied due to an extensive history of late payments and unreliable current sources of income\"\n\nor\n\n\"late\\_payments\\_1y = +1.1sd, late\\_payments\\_5y = 0.5s...\"\n\nAs an ML engineer, the sad thing about LLMs is they cut out the ML engineer between what a PM wants and what needs to be done with the data.",
    "source": "comment"
  },
  {
    "id": "n0pgmx3",
    "title": "",
    "body": "They described an even more rudimentary approach than model distillation. They\u2019re basically using a larger model to generate synthetic data and then using this data to train a smaller model. If you\u2019re a Mid level ML engineer, I do wonder what is your expertise. \n\nTrue distillation does it even better. It uses the logits of the larger model to train the smaller model. It\u2019s a richer tapestry of data than just the synthetic logitless data.",
    "source": "comment"
  },
  {
    "id": "n0th7rv",
    "title": "",
    "body": "Here in this GitHub: https://github.com/ByteDance-Seed/Seed-Coder\nthey (ByteDanceSeed) basically used the approach of model distillation to pretrain and finetune (SFT and RL) state of the Art coding Models (Base, instruction-tuned, reasoning). They have a long paper where they explain everything in Detail. I learned so much from it",
    "source": "comment"
  },
  {
    "id": "n0p1onh",
    "title": "",
    "body": "If the llms start misrepresenting their reasoning process we have bigger problems lol",
    "source": "comment"
  },
  {
    "id": "n0qzvb7",
    "title": "",
    "body": "It only counts if the first part is actually true and not a post box explanation that sounds good",
    "source": "comment"
  },
  {
    "id": "n0p87bc",
    "title": "",
    "body": "PMs don't want to do that stuff, we aren't getting replaced just yet",
    "source": "comment"
  },
  {
    "id": "n0thjxg",
    "title": "",
    "body": "Thank you very much! This is more helpful than the anonymous down votes I'm getting lol",
    "source": "comment"
  },
  {
    "id": "n0p4003",
    "title": "",
    "body": "LLMs already do misrepresent their reasoning process. That's what the paper is about.",
    "source": "comment"
  },
  {
    "id": "n0q0hu3",
    "title": "",
    "body": "They do already because it comes up with an answer using probabilities, so it doesn\u2019t really know how the next token was predicted.\n\n***Usually*** the chain of thought is coherent",
    "source": "comment"
  },
  {
    "id": "n0sjoae",
    "title": "",
    "body": "How about applying that same requirement to \"explainable ML\"?\n\nI don't have faith that LLM's explaining their reasoning is 100% reliable...but I think it's probably more reliably a reasonable explanation than output from any classic explainable ML method *and* it's more understandable to non-technical folks so better in all measurable criteria?\n\nAlso I'll note that in my years of working at a large tech company, I've very convinced that the vast majority ML engineers (not PMs!) have a very poor understanding of how/why their models make decisions. That said I work in discovery, may be different for fields like Trust and Safety Classifiers.",
    "source": "comment"
  },
  {
    "id": "n0u6f7o",
    "title": "",
    "body": "I'm not saying PMs are take the role completely but I am saying PMs are becoming much more involved on building the decision engine (i.e. prompt engineering) and the number of technical staff required is dropping incredibly for the same task.",
    "source": "comment"
  },
  {
    "id": "n0tivv7",
    "title": "",
    "body": "They basically are bringing model distillation to a whole nother level by distilling and filtering the best outputs from the same model they want to train",
    "source": "comment"
  },
  {
    "id": "n0p4pxv",
    "title": "",
    "body": "Ahh OK awesome I will read that thank you.",
    "source": "comment"
  },
  {
    "id": "n0p4qxx",
    "title": "",
    "body": "And by awesome, I mean not awesome lol",
    "source": "comment"
  },
  {
    "id": "1lp8umz",
    "title": "[R] Introducing DreamPRM, a multi-modal LLM reasoning method achieving first place on the MathVista leaderboard",
    "body": "I am excited to share our recent work, DreamPRM, a multi-modal LLM reasoning method that ranks first currently on the MathVista leaderboard.\n\nhttps://preview.redd.it/54v78gz7zaaf1.png?width=1348&format=png&auto=webp&s=0084aef7727d9f02c129d8414582018fb09eedb5\n\nhttps://preview.redd.it/u0c02on9zaaf1.jpg?width=1374&format=pjpg&auto=webp&s=42ab8761dcc972ed89a999dac503c9dc35e65e18\n\n  \n\n\n\n\nReasoning has substantially improved the performance of large language models (LLMs) on complicated tasks. Central to the current reasoning studies, Process Reward Models (PRMs) offer a fine-grained evaluation of intermediate reasoning steps and guide the reasoning process. However, extending PRMs to multimodal large language models (MLLMs) introduces challenges. Since multimodal reasoning covers a wider range of tasks compared to text-only scenarios, the resulting distribution shift from the training to testing sets is more severe, leading to greater generalization difficulty. Training a reliable multimodal PRM, therefore, demands large and diverse datasets to ensure sufficient coverage. However, current multimodal reasoning datasets suffer from a marked quality imbalance, which degrades PRM performance and highlights the need for an effective data selection strategy. To address the issues, we introduce DreamPRM, a domain-reweighted training framework for multimodal PRMs which employs bi-level optimization. In the lower-level optimization, DreamPRM performs fine-tuning on multiple datasets with domain weights, allowing the PRM to prioritize high-quality reasoning signals and alleviating the impact of dataset quality imbalance. In the upper-level optimization, the PRM is evaluated on a separate meta-learning dataset; this feedback updates the domain weights through an aggregation loss function, thereby improving the generalization capability of trained PRM. Extensive experiments on multiple multimodal reasoning benchmarks covering both mathematical and general reasoning show that test-time scaling with DreamPRM consistently improves the performance of state-of-the-art MLLMs. Further comparisons reveal that DreamPRM\u2019s domain-reweighting strategy surpasses other data selection methods and yields higher accuracy gains than existing test-time scaling approaches.\n\n\n\nPaper: [https://arxiv.org/abs/2505.20241](https://arxiv.org/abs/2505.20241)\n\n\n\nCode: [https://github.com/coder-qicao/DreamPRM](https://github.com/coder-qicao/DreamPRM)",
    "source": "post"
  },
  {
    "id": "1lp7jyb",
    "title": "[D]Looking for Hinglish (code-mixed Hindi-English) speech emotion audio datasets \u2014 any recommendations?",
    "body": "Hi everyone,\nI'm working on a deep learning project involving emotion recognition from Hinglish (code-mixed Hindi-English) speech.\n\nI\u2019ve found plenty of datasets for English (like RAVDESS, IEMOCAP) and some for Hindi (MUCS, OpenSLR), but I\u2019m having trouble locating datasets that contain Hinglish speech, especially with emotion labels.\n\nDo any of you know of:\nHinglish speech datasets (code-switched Hindi-English)\nEmotion-labeled Hinglish audio\nOpen-source or research datasets that allow this type of training\n\nIf there are no public datasets, I\u2019d also appreciate tips on how to create or augment one from scratch.\nAnd also how can I increase it accuracy.\n\nThanks in advance!",
    "source": "post"
  },
  {
    "id": "n0ttg5i",
    "title": "",
    "body": "You have 2 sets of datasets - augment data from both the sets, and use them for training your models (assuming that training a model is your use case)",
    "source": "comment"
  },
  {
    "id": "n0v4h26",
    "title": "",
    "body": "Create a dataset by collecting Hinglish speech from YouTube videos, podcasts, or social media with emotion labels using natural language processing and machine learning algorithms.",
    "source": "comment"
  },
  {
    "id": "1lp3wla",
    "title": "[D] Simple Questions Thread",
    "body": "Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!\n\nThread will stay alive until next one so keep posting after the date in the title.\n\nThanks to everyone for answering questions in the previous thread!",
    "source": "post"
  },
  {
    "id": "n0tig7j",
    "title": "",
    "body": "Question: How to find postdoc in AI as non-cs phd?\n\nI am a final-year PhD candidate in a non-CS field. My research is application of modern AI models (e.g., Transformer, GAN, Stable Diffusion, Mamba). In short, my work is to apply new models proposed at CVPR and ICML to my own field.\n\nMy ultimate career goal is to do core AI research. To make this transition, I am seeking a postdoctoral position within a computer science department. My primary objective for a postdoc is to gain experience and publish in top-tier AI conferences like CVPR, ICML, NeurIPS, and others.\n\nI recognize that my profile may seem not competitive to a CS PI. To make myself a more relevant candidate, my current plan is to publish a CVPR workshop. While I know a workshop paper isn't that competitive, it's the best approach I can think of right now.\n\nI would also like to ask for any further advice. Has anyone here made a similar transition from a non-CS PhD to a postdoc in CS/AI?",
    "source": "comment"
  },
  {
    "id": "n0s0qih",
    "title": "",
    "body": "\n\n\n\ud83d\udd0d Looking for Guidance or Collaboration \u2014 Building a Sinhala Language GPT Chatbot (Open to Advice or Partnerships)\n\n\nHi everyone, I\u2019m working on a very interesting but challenging project \u2014 building a GPT-style AI chatbot for the Sinhala language. I initially started this as a hobby, thinking it would be simple, but I\u2019ve now realized it\u2019s a much bigger and more expensive task than expected.\n\nMy goal is to create a multifunctional Sinhala chatbot that can:\n\nUnderstand and generate natural Sinhala conversations\n\nPerform tasks like translation, web search, and knowledge retrieval\n\nHandle multi-turn conversations, emotions, and different intents\n\nEventually serve users in Sinhala for daily tasks, education, and business\n\nI\u2019ve already started the hardest part \u2014 building a labeled dataset with Sinhala sentences and responses. I researched data labeling, intent detection, sentiment tagging, and more. But I\u2019m realizing that for actual model training (LLM fine-tuning, embeddings, retrieval augmentation), I need more guidance.\n\nI\u2019m looking for:\n\nAdvice from anyone who has worked on LLM fine-tuning, RAG (Retrieval Augmented Generation), Whisper, TTS, or multilingual AI models\n\nCollaboration with developers, AI engineers, or data scientists\n\nEven as a small startup partnership, I\u2019m open to working together\n\nI can invest a monthly budget to cover costs like labeling, server, training, or API expenses\n\nThis project is for Sinhala, an underrepresented language in AI. If anyone can help, guide, or wants to collaborate, please reach out. I would really appreciate any advice, resources, or connections.\n\nThank you!",
    "source": "comment"
  },
  {
    "id": "1lp5c22",
    "title": "[D] Looking for AI-powered smart crop library - smartcrop.py isn't enough",
    "body": "https://preview.redd.it/r1w6xzdnbaaf1.png?width=1492&format=png&auto=webp&s=5ab883dcc781312bb6014b9daf1d9295dfc66030\n\nHey everyone!\n\nI'm currently using\u00a0smartcrop.py (github.com/smartcrop/smartcrop.py)\u00a0for image cropping in Python, but it's pretty basic. It only detects edges and color gradients, not actual objects.\n\nFor example, if I have a photo with a coffee cup, I want it to recognize the cup as the main subject and crop around it. But smartcrop just finds areas with most edges/contrast, which often misses the actual focal point.\n\n**Looking for:**\n\n* Python library that uses AI/ML for object-aware cropping\n* Can identify main subjects (people, objects, etc.)\n* More modern than just edge detection\n\nAny recommendations for libraries that actually understand what's in the image?\n\nThanks!",
    "source": "post"
  },
  {
    "id": "n0s2i7y",
    "title": "",
    "body": "You might have some better luck finding a binary segmentation/background removal model and using it to detect the foreground, then drawing a bounding box yourself with padding to build the crop.\n\nThis could be something lightweight like [https://huggingface.co/briaai/RMBG-1.4](https://huggingface.co/briaai/RMBG-1.4) or something heavier and configurable like [https://github.com/facebookresearch/segment-anything](https://github.com/facebookresearch/segment-anything) depending on your resource requirements.",
    "source": "comment"
  },
  {
    "id": "n0sbiwg",
    "title": "",
    "body": "this is a amazing idea bro. i should give this a try",
    "source": "comment"
  },
  {
    "id": "n0sdtkd",
    "title": "",
    "body": "u/lemon-meringue just tested [https://huggingface.co/PramaLLC/BEN2](https://huggingface.co/PramaLLC/BEN2) and [https://huggingface.co/briaai/RMBG-2.0](https://huggingface.co/briaai/RMBG-2.0)\n\nthe models detect the foreground perfect on all tested images.",
    "source": "comment"
  },
  {
    "id": "n0ur5ly",
    "title": "",
    "body": "Great to hear!",
    "source": "comment"
  },
  {
    "id": "1lozmgb",
    "title": "[D] Alternatives to segmentation models pytorch?",
    "body": "SMP is currently my go-to for image segmentation, and it is generally a good library.\n\nWhat I like:\n\n1) Easy to use\n\n2) Support for timm encoders (super useful to me!)\n\nWhat I don't like:\n\n1) Only one type of attention, options for decoder don't feel very modern\n\n2) Not very flexible/extensible\n\nI'd love to be able to add custom bottleneck modules, more easily get bottleneck features for auxilliary classification tasks (I am not a fan of how the aux part is handled), and more modern/flexible options for the decoder.\n\nAny suggestions? Cheers!",
    "source": "post"
  },
  {
    "id": "1lollc0",
    "title": "[R] BIG-Bench Extra Hard",
    "body": "",
    "source": "post"
  },
  {
    "id": "1lohh1u",
    "title": "[D] Should we petition for requiring reviewers to state conditions for improving scores?",
    "body": "I\u2019ve been thinking about how opaque and inconsistent peer reviews can be, especially in top ML conferences. What if we made it a requirement for reviewers to explicitly state the conditions under which they would raise their scores? For example, \u201cIf the authors add experiments on XYZ\u201d or \u201cIf the theoretical claim is proven under ABC setup.\u201d\n\nThen, area chairs (ACs) could judge whether those conditions were reasonably met in the rebuttal and updated submission, rather than leaving it entirely to the whims of reviewers who may not revisit the paper properly.\n\nHonestly, I suspect many reviewers don\u2019t even know what exactly would change their mind.\n\nAs an added bonus, ACs could also provide a first-pass summary of the reviews and state what conditions they themselves would consider sufficient for recommending acceptance.\n\nWhat do you think? Could this improve transparency and accountability in the review process?",
    "source": "post"
  },
  {
    "id": "n0myalr",
    "title": "",
    "body": "If you read the reviewers guide, there is already a guidance that the \"questions\" the reviewer makes should focus on things that would affect the score. So, normally, the questions made are the main things that might affect the scores.\n\n\nWhat most authors do not want to hear tho is that in many cases there is nothing that can be done to improve the grade. If I read a paper in my narrow research area there is minimal chance of misunderstanding of a core part of the contribution, and I am not rejecting a paper on grounds of things that can be easily and quickly done in first place.\n\n\n\u00a0So, in those cases, I often write (no questions that could affect my evaluation), and it often leads to the authors trying to convince me otherwise and some times even complaining to the AC. Sometimes the only way you are getting a good score is working on improving the paper and later submitting to another conference. You would be surprised of how many times I have had to review the same paper sequentially being submitted to ICLR, ICML, NeurIPS, and AAMAS and the authors didn't change anything I suggested that would require new experiments or significant rewriting, so the paper keeps being rejected",
    "source": "comment"
  },
  {
    "id": "n0njri8",
    "title": "",
    "body": "At massive conferences with <20% acceptance rates, it\u2019s just not possible to give authors a prescriptive route to acceptance. All it\u2019s gonna do is lead to more complaining when authors believe they\u2019ve met the reviewers\u2019 criteria but still don\u2019t get accepted. At the end of the day, we all need to accept that top conferences are *meant* to be hard to get into and not every idea is worthy of publication at NeurIPS/ICML/ICLR.",
    "source": "comment"
  },
  {
    "id": "n0n46n1",
    "title": "",
    "body": "Nice try, reward-hacking RL algorithm",
    "source": "comment"
  },
  {
    "id": "n0nqg3m",
    "title": "",
    "body": "> \"Find a more novel research topic and write a new paper about that.\"",
    "source": "comment"
  },
  {
    "id": "n0q4fow",
    "title": "",
    "body": "That is how peer review process traditionally worked and still works in normal fields. But ML community has optimized itself into a degenerated overfitted state - we run these funky lotteries where the entire field sends its papers and at the end 20% people get a reward for their CVs. The top researchers have strategies that are compatible with this model -- they produce a lot of papers with as many PhD/masters students as they can get, and they try to do as many cross-institution collaborations as possible. Both strategies result in more lottery tickets.",
    "source": "comment"
  },
  {
    "id": "n0pzu71",
    "title": "",
    "body": "It is highly likely that nothing you do could improve your grade at that point. This is mainly because any larger change requires the work to undergo a full review again. In most cases the reivewer does not have access to the fully updated work, nor has the time to review several changed papers in just a few days \\[\\*\\]. I think the only work that can profit from the question format is theoretical work, e..g, proving an intermediate step in more detail to show that it is valid.\n\nI also do not think that reviews at top ML conferences are inconsistent, nor opaque, compared to journals or even other fields. Already the chance of heaving a discussion without a full resubmission & review is pretty unique to ML.\n\n\\[\\*\\] This would in my book also increase the effort for reviewing so much that reviewers who have not submitted a paper must be paid for the amount of work they are doing.",
    "source": "comment"
  },
  {
    "id": "n0nq7jl",
    "title": "",
    "body": "It won't help anything. Just improve your paper and move on. It's a rite of passage.",
    "source": "comment"
  },
  {
    "id": "n0nfqmn",
    "title": "",
    "body": "We should reveal the identity of everyone (including authors, reviewers, and ACs) after releasing the decision. It allows the author to ask the reviewer face-to-face.",
    "source": "comment"
  },
  {
    "id": "n0pahf0",
    "title": "",
    "body": "Yeah some papers are just ill-conceived and the review is an opportunity to communicate the distance it needs to overcome.",
    "source": "comment"
  },
  {
    "id": "n0xf41r",
    "title": "",
    "body": "This definitely isn't how peer review traditionally worked.  The whole idea of a scoring system, back and forth between reviewers and submitters where scores might change, having rebuttals, discussion phases, score updates, etc., is a relatively new innovation within specific fields, especially machine learning and some other computer science areas.  In most other fields, you submit something, get back a decision on acceptance, and then you either celebrate or prepare to submit elsewhere.  There are some other examples in a similar direction, like conditional acceptance or early review opportunities at conferences outside machine learning, but these are relatively minor and not the norm.",
    "source": "comment"
  },
  {
    "id": "n0nq3sn",
    "title": "",
    "body": "You're going to fist fight a reviewer, aren't you",
    "source": "comment"
  },
  {
    "id": "n0o0vl6",
    "title": "",
    "body": "Maybe not face-to-face, but there absolutely are journals that do exactly that, including all reviews and authors' responses, e.g. GigaScience.",
    "source": "comment"
  },
  {
    "id": "n0q052x",
    "title": "",
    "body": "you are going to pay everyone involved, right?",
    "source": "comment"
  },
  {
    "id": "n0v1079",
    "title": "",
    "body": "I think deanonymizing also has potential to give good reviewers much deserved acknowledgment",
    "source": "comment"
  },
  {
    "id": "1lolkda",
    "title": "[R] Interpreting Large Language Models' Personality through Critical Event Analysis",
    "body": "Excited to share our new work, \"Supernova Event Dataset: Interpreting Large Language Models' Personality through Critical Event Analysis\" accepted at the Actionable Interpretability Workshop @ ICML 2025.\n\nIntroducing the Supernova Event Dataset\n\nWe present a new benchmark built from real-world Wikipedia articles, including biographies, historical milestones, global news, and scientific discoveries (including articles from Google Deep Research). This dataset introduces a novel task: critical event analysis for interpreting the behavioral pattern, or \u201cpersonality\u201d of LLMs.\n\nRather than looking inside the model (activations, traces), we ask a separate LLM to judge what events are most critical, and use this external perspective to decode the model\u2019s values and reasoning traits.\n\nSome early insights:\n\nOrca2 tends to prioritize emotional and interpersonal events.\n\nPhi-4 and Qwen2.5 focus on strategic milestones.\n\nIn scientific discovery, o3 highlights causal breakthroughs, Gemini 2.5 Pro favors methodological innovations, and Claude Sonnet 3.7 emphasizes conceptual clarity.\n\nWhile these are early findings (still without human evaluation), the diversity in critical event patterns is striking. We believe assigning LLMs \"personalities\" could make them more relatable and trustworthy, enabling smoother human-AI collaboration, especially in domains like scientific discovery.\n\nPaper: [arxiv.org/abs/2506.12189](https://l.facebook.com/l.php?u=http%3A%2F%2Farxiv.org%2Fabs%2F2506.12189%3Ffbclid%3DIwZXh0bgNhZW0CMTAAYnJpZBExaFJHbUZIbDN4cEJKZ096SAEeYgHhUQQ5lNulNDLnSeulM64ECl3ls5tJCNkMC1_EPhqEHk1uqWYnEdBsu7g_aem_fW6BPSFPpkV4xELlQOiODA&h=AT0m3E04nqeA-MMtev7Ouz9OW3PeW5A_y6V9zj9dqy3WrwMXLOeTXVO9EzTXtMey6tWCwh1vUB5rS0lMPeoFEdjHj6BecO9zq9__xYIrQwGC6nhRT8BEE50RlAm9OuXWhk_HCtHu&__tn__=-UK-R&c[0]=AT1lFRYu12xNSpO0-IlQ0tpDLaqpnTlHs0ipZFi4QqbVd_3LDc0Vnyr8uzMo6U8UhtdIXR8G5rKpdZaAbCtojueyXuX18q9jlCTh0kB1YA49AGDcMIGSWkV-pn1HLQWJK6QSFLLZ3aNz9jS3Cq1Q_927wYGbHSSdspcG77eNZ7dlk1qKhBee)\n\nTwitter: [https://x.com/Pranav\\_AL/status/1939681069554655382](https://x.com/Pranav_AL/status/1939681069554655382)\n\nWebpage: [http://supernova-event.ai](https://l.facebook.com/l.php?u=http%3A%2F%2Fsupernova-event.ai%2F%3Ffbclid%3DIwZXh0bgNhZW0CMTAAYnJpZBExaFJHbUZIbDN4cEJKZ096SAEeZQWpZz3uLtupNGIkmvzqpDbbG9u83w7Tv9ifY4Rjvct6zdjy8E4yg6NNDTM_aem_SgZlEI9ACWixaHK8TQWBPw&h=AT3tIyMocFJs9OJneCmpLWwlEjH3FE0RtckM5RYggzkKKuFMG5AIIScnpzGDhh7YqxBOSxpleqKt0hXYWIiiG_t3RoKtnoI1vlHkHUCsMhHlTmKcoQvqBSUnk8rLDci3doz0NpFV&__tn__=-UK-R&c[0]=AT1lFRYu12xNSpO0-IlQ0tpDLaqpnTlHs0ipZFi4QqbVd_3LDc0Vnyr8uzMo6U8UhtdIXR8G5rKpdZaAbCtojueyXuX18q9jlCTh0kB1YA49AGDcMIGSWkV-pn1HLQWJK6QSFLLZ3aNz9jS3Cq1Q_927wYGbHSSdspcG77eNZ7dlk1qKhBee)\n\nDemo: [supernova-event.ai/#your-story](https://l.facebook.com/l.php?u=http%3A%2F%2Fsupernova-event.ai%2F%3Ffbclid%3DIwZXh0bgNhZW0CMTAAYnJpZBExaFJHbUZIbDN4cEJKZ096SAEeyxn4V9DBpYEvuLM2LwLuOOn-4Ewox9O267lf8zV1R8tbbLTm3vS1lw0zzqM_aem_9qDTC7HNLTFGRKq_iFqgBg%23your-story&h=AT1SxDBmgr_G-a-c-D9g4JyMlgs5bD-liDDwNRTZlIP_5CBvyl8meA3pwaNKABRXLqvHcemnEJv-sWcf3oan2b3FSWZ7H_yD3Y8mdhI5Ze6mVNULKBXQTdFqgrLNUynlNQjPjP2_&__tn__=-UK-R&c[0]=AT1lFRYu12xNSpO0-IlQ0tpDLaqpnTlHs0ipZFi4QqbVd_3LDc0Vnyr8uzMo6U8UhtdIXR8G5rKpdZaAbCtojueyXuX18q9jlCTh0kB1YA49AGDcMIGSWkV-pn1HLQWJK6QSFLLZ3aNz9jS3Cq1Q_927wYGbHSSdspcG77eNZ7dlk1qKhBee)\n\nCode: [https://github.com/pranavAL/Supernova-Event-Dataset](https://l.facebook.com/l.php?u=https%3A%2F%2Fgithub.com%2FpranavAL%2FSupernova-Event-Dataset%3Ffbclid%3DIwZXh0bgNhZW0CMTAAYnJpZBExaFJHbUZIbDN4cEJKZ096SAEeyxn4V9DBpYEvuLM2LwLuOOn-4Ewox9O267lf8zV1R8tbbLTm3vS1lw0zzqM_aem_9qDTC7HNLTFGRKq_iFqgBg&h=AT3sNwJZLhvA9OG4GbkPvPxvAXxtZr9drQAj1Rp-4MCOHOOVbjH1epznhz08JAKypffQNwntIbz6TWzMTDmKVgXvDw7y6Yrg6Bcijqgxco34C_R4ivMwgS83oW5i2QnMFFQmQuVt&__tn__=-UK-R&c[0]=AT1lFRYu12xNSpO0-IlQ0tpDLaqpnTlHs0ipZFi4QqbVd_3LDc0Vnyr8uzMo6U8UhtdIXR8G5rKpdZaAbCtojueyXuX18q9jlCTh0kB1YA49AGDcMIGSWkV-pn1HLQWJK6QSFLLZ3aNz9jS3Cq1Q_927wYGbHSSdspcG77eNZ7dlk1qKhBee)\n\nWe're working toward scaling this into a real-world product, and we're currently seeking the right resources and support to take it further. If you're interested in what we're building and see potential for impact, we\u2019d love to hear from you. Reach us at [hello@supernova-event.ai](mailto:hello@supernova-event.ai) ; we're open to conversations, collaborations, and any form of support that can help push this idea forward.\n\nhttps://preview.redd.it/uugbpxw075af1.png?width=1200&format=png&auto=webp&s=ccbde6f1ace6140ff2ca838ffb0e60522759dc70",
    "source": "post"
  },
  {
    "id": "1lnoqmm",
    "title": "[D] Review clearly used an LLM, should I report it to AC?",
    "body": "This review gave me 1.5 in ACL and calls GRPO Generalized Reward Preference Optimization, which is what ChatGPT thinks GRPO is... It also says my work is the first one to use GRPO in my domain while it is not (and we talk about this in the introduction) and says we are missing some specific evaluations, which are present in the appendix and says we did not justify a claim well enough, which is very well known in my domain but when asking ChatGPT about it it says it does not know about it...\n\nIt feels like the reviewer just wanted to give me a bad review and asked an LLM to write a poor review. He clearly did not even check the output because literally everyone knows GRPO stands for Group Relative Policy Optimization...\n\nOther than reply to the reviewer while pretending I did not know he/she used ChatGPT, what else can I do? My other reviews were both 3, so I really want to get rid of this review if possible...",
    "source": "post"
  },
  {
    "id": "n0gs7ot",
    "title": "",
    "body": "At that point even if you don't report it for suspected LLM use you can report it for being unqualified to review since the review doesn't understand what GRPO is. I'd actually flag the review under unqualified and then maybe just mention the LLM-generated part in the justification",
    "source": "comment"
  },
  {
    "id": "n0h0mdl",
    "title": "",
    "body": "I think it should be reported - but don't get your hopes too high. \nA paper of mine was recently rejected due to one (of the four) reviewers giving devastating comments. Most points were completely hallucinated or requested experiments which were already part of the paper  - but the editor did not care at all and just gave a copy-paste answer that they \"understand our dissapointment but can not accept the paper due to the reviewers concerns\".",
    "source": "comment"
  },
  {
    "id": "n0gwdlf",
    "title": "",
    "body": "Just hope that your AC is not a human-assisted LLM\u2026",
    "source": "comment"
  },
  {
    "id": "n0jfgrw",
    "title": "",
    "body": "Why IN THE WORLD would you not report it?",
    "source": "comment"
  },
  {
    "id": "n0h5jr1",
    "title": "",
    "body": "I had this happen too on an open review submissions. I spent hours writing a multipage response. Half of the reply was aimed at the strength section. All of the strengths were hallucinated, so I asked the reviewer nicely to clarify where he found the strengths so I could correct the claims. The reviewer never replied of course.\n\nI also sent a message to the AC referring to the poor quality of the review. The AC ended up tossing the review.",
    "source": "comment"
  },
  {
    "id": "n0i14ob",
    "title": "",
    "body": "That is terrible and a blatant ethics violation. You should definitely report since the reviewer ought to be punished (as unlikely as that is).",
    "source": "comment"
  },
  {
    "id": "n0jg2ni",
    "title": "",
    "body": "Definitely report it! Most major conferences are being plagued by LLM-generated terrible quality reviews. Unless we speak up, this issue will just gets bigger.",
    "source": "comment"
  },
  {
    "id": "n0he272",
    "title": "",
    "body": "better right the reviewer is not qualified and has provided wrong summary and review.",
    "source": "comment"
  },
  {
    "id": "n0nik65",
    "title": "",
    "body": "that's really bad.",
    "source": "comment"
  },
  {
    "id": "n0qqn34",
    "title": "",
    "body": "Report it. ARR ACs are pretty helpful. Obviously some of them are jerk but mostly helpful.",
    "source": "comment"
  },
  {
    "id": "n0u4wor",
    "title": "",
    "body": "Got rejected because of LLM generated review (similar to yours the reviewer wanted to give me bad review quite likely). Emailed the AC and they sent the generic response that they have taken this into account. Well if they had taken this into account the paper would have been accepted as all other reviews were positive \\*smh\\*",
    "source": "comment"
  },
  {
    "id": "n0j6271",
    "title": "",
    "body": "I am trying to follow the point you are trying to make. It would be interesting to see if there is a way to verify if the reviewer did indeed use an LLM, and if so, what implications this might have for the review and the paper.",
    "source": "comment"
  },
  {
    "id": "n0mjhw1",
    "title": "",
    "body": "What is AC in this context?",
    "source": "comment"
  },
  {
    "id": "n0mjiux",
    "title": "",
    "body": "What is AC in this context?",
    "source": "comment"
  },
  {
    "id": "n0haphu",
    "title": "",
    "body": "I would say that's not necessarily evidence that the reviewer is an LLM. It might be just a reviewer outside of their expertise area, which is not ideal but always happens. OP can add an observation saying that they suspect it's an LLM review but there is no definite proof here, and you will definetely need to write a rebuttal.\n\nPS: either way, it doesn't really matter even if the AC agree with you, they will still in practice use that score. The last ICML I had a reviewer that didn't even fill up the form completely and wrote a 1-sentence review rejecting the paper. I contacted the AC and everything and after the rebuttal I had 1 reviewer voting accept and one weak reject but the shitty review served as a \"tie-breaker\" to reject my paper.",
    "source": "comment"
  },
  {
    "id": "n0jn9n3",
    "title": "",
    "body": "This is the sad state of affairs in ML conferences. ACs overlook issues way too often. They\u2019re overworked and have limited capacity for soliciting emergency reviews to cover for clearly nonsensical reviews.",
    "source": "comment"
  },
  {
    "id": "n0ja1xb",
    "title": "",
    "body": "Prompt inject the crap out of your rebuttal",
    "source": "comment"
  },
  {
    "id": "n0gzay4",
    "title": "",
    "body": "It\u2019s LLMs all the way",
    "source": "comment"
  },
  {
    "id": "n0hg8d3",
    "title": "",
    "body": "I feel you. After the miccai reviews I am almost more comfortable with LLM reviews you can easily tackle in the rebuttal instead of an asshole. Who just read the abstract and couldn\u2019t find a table they were thinking the paper should contain. \n\nOn the other hand the influx of so many submission makes it difficult to get decent reviews. It\u2019s just not feasible to review 4-6 papers for each conference. Change is needed vadly",
    "source": "comment"
  },
  {
    "id": "n0im09y",
    "title": "",
    "body": "Punished how?  At this point I figure taking away reviews would be a mercy, not a punishment",
    "source": "comment"
  },
  {
    "id": "n0kdivb",
    "title": "",
    "body": "It will get bigger no matter what.",
    "source": "comment"
  },
  {
    "id": "n0hexml",
    "title": "",
    "body": "LLMs reviewing papers written by LLMs about LLMs",
    "source": "comment"
  },
  {
    "id": "n0j3iph",
    "title": "",
    "body": "I\u2019m guessing that the reviewer is a paper author who was forced to review in order to submit. They should be threatened with having their paper desk rejected if they don\u2019t fix their review. (And honestly, repeat offenders should get banned/suspended from submitting. It\u2019s very, very poor to be doing this.)",
    "source": "comment"
  },
  {
    "id": "n0jw0pv",
    "title": "",
    "body": "You mean RL in RL?",
    "source": "comment"
  },
  {
    "id": "n0s3pq7",
    "title": "",
    "body": "Human attention is all we need",
    "source": "comment"
  },
  {
    "id": "n0j7f89",
    "title": "",
    "body": "Agreed on all counts",
    "source": "comment"
  },
  {
    "id": "n0jyfxb",
    "title": "",
    "body": "While I wholeheartedly agree with you that this is an ethics violation and should be somehow punished, rejecting papers would not be fair to the coauthors of whatever paper they had in the conference. And I say this as someone who ACs AI conferences and get absolutely livid about LLM reviews. I specifically instruct my SPCs to first challenge these reviewers during discussions, then toss the reviews, and, at the end of the conference, reporting the PC member (there are usually forms where you get to name candidates for best PC member, and also tell of grossly unprofessional behaviour).",
    "source": "comment"
  },
  {
    "id": "n0k2djc",
    "title": "",
    "body": "I agree that\u2019s tricky, but imo all authors should be warned and any one of them should be allowed to fix the review. LLM reviews completely undermine the peer review system and need to be dealt with extremely harshly. If the review is simply discarded then the bad reviewer wins. Personally, I\u2019d feel embarrassed rather than aggrieved if I had a paper rejected due to a co-author submitting an LLM review. Also, if you\u2019re a PhD supervisor / senior co-author and one of your young mentees does this kind of thing then it\u2019s kind of on you too, tbh. Student reviews ought to be checked.",
    "source": "comment"
  },
  {
    "id": "n0qduk4",
    "title": "",
    "body": "NeurIPS is doing this right now. Crappy review? You won\u2019t be able to access the reviews to your paper until you fix yours.",
    "source": "comment"
  },
  {
    "id": "1lon6sx",
    "title": "[P] I've built a spec for LLM-to-LLM comms by combining semantic patterns with structured syntax",
    "body": "Firstly, total disclaimer. About 4 months ago, I knew very little about LLMs, so I am one of those people who went down the rabbit hole and started chatting with AI. But, I'm a chap who does a lot of pattern recognition in the way I work (I can write music for orchestras without reading it) so just sort of tugged on those pattern strings and I think I've found something that's pretty effective (well it has been for me anyway).\n\nLong story short, I noticed that all LLMs seem to have their training data steeped in Greek Mythology. So I decided to see if you could use that shared knowledge as compression. Add into that syntax that all LLMs understand (:: for clear key-value assignments, \u2192 for causality and progression, etc) and I've combined these two layers to create a DSL that's more token-efficient but also richer and more logically sound.\n\nThis isn't a library you need to install; it's just a spec. Any LLM I've tested it on can understand it out of the box. I've documented everything (the full syntax, semantics, philosophy, and benchmarks) on GitHub.\n\nI'm sharing this because I think it's a genuinely useful technique, and I'd love to get your feedback to help improve it. Or even someone tell me it already exists and I'll use the proper version!\n\nLink to the repo:\u00a0[https://github.com/elevanaltd/octave](https://github.com/elevanaltd/octave)",
    "source": "post"
  },
  {
    "id": "1louv9e",
    "title": "[P] How do I detect whether a person is looking at the screen using OpenCV?",
    "body": "Hi guys, I'm sort of a noob at Computer Vision and I came across a project wherein I have to detect whether or not a person is looking at the screen through a live stream. Can someone please guide me on how to do that?\n\nThe existing solutions I've seen all either use MediaPipe's FaceMesh (which seems to have been depreciated) or use complex deep learning models. I would like to avoid the deep learning CNN approach because that would make things very complicated for me atp. I will do that in the future, but for now, is there any way I can do this using only OpenCV and Mediapipe?\n\n  \nPS. Sorry for the wrong tag mods",
    "source": "post"
  },
  {
    "id": "n0q9e4x",
    "title": "",
    "body": "i had a similar project where i needed to check if someone was looking at the screen\u00a0\u00a0\ndidn\u2019t wanna touch deep learning either lol\n\n\ni used mediapipe face mesh to get the eye landmarks\u00a0\u00a0\nthen just checked where the iris was using opencv\u00a0\u00a0\nif it\u2019s centered they\u2019re probably looking at the screen\u00a0\u00a0\nif it\u2019s way off to the side you can guess they\u2019re distracted\n\n\nnot super accurate but works well enough\u00a0\u00a0\nyou can tweak the thresholds a bit depending on the setup\u00a0\u00a0\nno need for CNNs yet you're good",
    "source": "comment"
  },
  {
    "id": "n0pvh5o",
    "title": "",
    "body": "Search for \"gaze detection\", that is the ML term for this task. Maybe you can find something that does it how you want.",
    "source": "comment"
  },
  {
    "id": "n0sipba",
    "title": "",
    "body": "Let me guess - monitoring employees to see if they're working?",
    "source": "comment"
  },
  {
    "id": "n0w54ka",
    "title": "",
    "body": "Check out OpenCV's FaceUtils module, it has functions for facial detection and tracking, might help you achieve what you need without deep learning models.",
    "source": "comment"
  },
  {
    "id": "n0tdgmk",
    "title": "",
    "body": "Pausing the Youtube ad if you look away \ud83d\ude2d",
    "source": "comment"
  },
  {
    "id": "1lnsph5",
    "title": "[D] How should I respond to reviewers when my model is worse than much larger models?",
    "body": "I got a review asking to compare my submission paper with more recent models. The models were not even out 3 months before the submission so by ACL rules I should not have to compare them with my model because it is contemporary.\n\nNevertheless I have ran comparisons and my model is much much worse... Why? I'm using a model doing the same thing but 32x smaller, used almost 1/10 of the data they used, etc... I am severely resource constrained and cannot compete in terms of scale, but I still think that my paper makes an important contribution that if we were to match the other models scale we would get better results.\n\nWhat should I do? Should I report results that show other models are better and risk the reviewers lower their scores? I kinda just want to explain the authors that the scale is completely different and other factors make it a very unfair comparison, but they might just not care...\n\nI have a 2.5 average score and really wanted to try to raise it to make it at least into findings, but I honestly don't know how to defend against not having as many resources as top labs/unis...",
    "source": "post"
  },
  {
    "id": "n0hzi7c",
    "title": "",
    "body": "\u201cThere are no available models in family X within an order of magnitude of the parameter count in our experiments. Future work will compare scaling laws to determine if our approach remains competitive at similar compute complexity.\u201d",
    "source": "comment"
  },
  {
    "id": "n0ho7rb",
    "title": "",
    "body": "I think you stated the answer very clearly, you shouldn't have to compare your model to any models with 10x or more parameter size. If you want to address the larger models, you can probably compare your models with some adjustment for chinchilla scaling laws.",
    "source": "comment"
  },
  {
    "id": "n0hvomj",
    "title": "",
    "body": "Can you reframe your paper as focusing on achieving good results with small models and limited data? Then you can explain why your work is still relevant.",
    "source": "comment"
  },
  {
    "id": "n0hvwk9",
    "title": "",
    "body": "Why is the comparison between your model and ones that are 32x in size?  \nAre you comparing against models that around the parameter size of your model in the paper ?",
    "source": "comment"
  },
  {
    "id": "n0hrtd5",
    "title": "",
    "body": "Out of curiosity, what kind of datasets are you working on here? How much compute did you use for these experiments? Perhaps a stronger theoretical justification might alleviate the reviewer's concerns.",
    "source": "comment"
  },
  {
    "id": "n0jigrk",
    "title": "",
    "body": "Potential solutions:\n* if you have the computational capacities scale down the other models and train them on the data you used, which makes models more comparable\n* plot the performance w.r.t. model size or dataset size showing how your model is worse but would outperform the others if scaled to the same size\n* argue how they are not comparable due to different resource used during training rendering a comparison (as in my first point) out of scope for this paper",
    "source": "comment"
  },
  {
    "id": "n0k00vt",
    "title": "",
    "body": "Maybe a distill of the big models would be a more accurate comparison ? If that is an option.",
    "source": "comment"
  },
  {
    "id": "n0lsjy5",
    "title": "",
    "body": "It's really common to put new models on a XY graph of model size vs performance.",
    "source": "comment"
  },
  {
    "id": "n0k611p",
    "title": "",
    "body": "This guy sciences.",
    "source": "comment"
  },
  {
    "id": "n0iw9d4",
    "title": "",
    "body": "Reframing really is the secret to successful research - it's really really hard to achieve the big things that we all usually strive towards. But useful findings always come out of any project, it's just about telling the story around those findings",
    "source": "comment"
  },
  {
    "id": "n0hzsee",
    "title": "",
    "body": "No... I think they just know of those models because I mention them in my introduction. They probably don't know the scale of such other models and/or had to mention weaknesses about something so they mentioned those models...",
    "source": "comment"
  },
  {
    "id": "n0htho1",
    "title": "",
    "body": "This is in the vision-language modelling space, and the datasets used here are heavy... also, I used a single A100 as all machines was being heavily used by PhD students at the time... I don't think there is any theoretical justification that could be applied here.",
    "source": "comment"
  },
  {
    "id": "n0idyl6",
    "title": "",
    "body": "does your introduction mention the scale difference between the models?",
    "source": "comment"
  },
  {
    "id": "n0j9u4g",
    "title": "",
    "body": "Since you brought them into the paper I think it is valid to ask for differences in terms of performance.\n\nThe basic question is why is your model needed if the other ones exist?\n\nMaybe you can find a distilled / smaller version of them.",
    "source": "comment"
  },
  {
    "id": "n0jrsgm",
    "title": "",
    "body": "Vision-language you say? Without knowing your problem its hard to say. Lets say its some sort of vision-ocr (for example) I'd argue that you dont need some gigantic gorillion parameter LLM to adecuately solve that. The size of the LLM should be adjusted to the problem at hand. Using an LLM the size of like chatgpt is just overkill and comes with alot of unused feature space most probably and wasted calculations and energy.\n\nYou could rephrase it and draw strength from it as its equally interesting IMO how a \"32x smaller\" model holds up against those bigger models.\n\nJust remember its not wrong to say that you were resource constrained either, some people just dont have access to highend hardware at will. My master thesis back in the day was valiantly trained over weeks on a mere 1080ti \ud83d\ude02\n\nEDIT:  *\"I'd argue that you dont need some gigantic gorillion parameter LLM to adecuately solve that.\"* Might come off as arogant, I dont mean solve the problem in its entirety ofcourse, just that atleast intuitively it should suffice, this has also been proven by researchers in the past year",
    "source": "comment"
  },
  {
    "id": "1lnvjin",
    "title": "[R] Free access to an H100. What can I build?",
    "body": "My company is experimenting with new hardware and long story short, there's an idling H100 with a 2TB RAM and 27TB of storage and I'm allowed to play with it!\n\nI really want to do some cool AI research to publish at a decent conference but I'm not well caught up with the research frontier and I could really use some help (and collaborators?).\n\nI understand neural networks, CNNs, transformer models etc. to a reasonable depth but understanding what SOTA is will probably take more time than how long I have access to the GPU",
    "source": "post"
  },
  {
    "id": "n0iabwk",
    "title": "",
    "body": "Idk, what are you interested in? You'll do more on a project that interests you than a random hot topic.\u00a0",
    "source": "comment"
  },
  {
    "id": "n0ifs28",
    "title": "",
    "body": "I\u2019d fine tune a model",
    "source": "comment"
  },
  {
    "id": "n0ibdo6",
    "title": "",
    "body": "What is your company doing?\nFinding a use case for your company and experimenting on that would be the best direction; you would become the expert/reference in your company for your chosen topic, resources used, showing that you have a high impact in the company...",
    "source": "comment"
  },
  {
    "id": "n0ilza3",
    "title": "",
    "body": "If your company has very niche data, you can easily develop a dataset. Once that is ready, you can use this machine to finetune opensource models on that data and check if there are reasonable results. If so, you can take it up as a project to create an internally finetuned model(vision, text, audio etc). This also helps in increasing your skillset as well.",
    "source": "comment"
  },
  {
    "id": "n0jv7pe",
    "title": "",
    "body": "Spin up a few open LLMs (Mistral, Phi-3, etc.) and compare snapshot-based orchestration runtimes like InferX with traditional serving. Cold starts, model swapping, GPU utilization . you\u2019d be surprised how much infra innovation is still wide open even with an H100.",
    "source": "comment"
  },
  {
    "id": "n0jqv6w",
    "title": "",
    "body": "Could you train a generic hifigan for music Upscaling?",
    "source": "comment"
  },
  {
    "id": "n0khh3y",
    "title": "",
    "body": "Spin up a quantized version of deepseek r1 and see if you can run some private company data through it",
    "source": "comment"
  },
  {
    "id": "n0jwguw",
    "title": "",
    "body": "try learning pre-training",
    "source": "comment"
  },
  {
    "id": "n0imu98",
    "title": "",
    "body": "Where can I find this...",
    "source": "comment"
  },
  {
    "id": "n0ib32q",
    "title": "",
    "body": "Anything that I'm interested in comes from what I already know, but the other day I heard about quantization of LLMs and that was very interesting too. I think I'd like to work on anything that has an inkling of an impact in the field today.",
    "source": "comment"
  },
  {
    "id": "n0il69e",
    "title": "",
    "body": "Then train an open source one from scratch for a specific use and iterate",
    "source": "comment"
  },
  {
    "id": "n0l69cc",
    "title": "",
    "body": "Hmm fine tune for what? Is there any novelty to be explored there?",
    "source": "comment"
  },
  {
    "id": "n0ibs92",
    "title": "",
    "body": "My company uses it for image and video generation. Very generic use case. While it's definitely interesting, I don't think I can do anything the greatest minds of the field aren't already working on. I'd rather focus on some niche thing",
    "source": "comment"
  },
  {
    "id": "n0ic4dw",
    "title": "",
    "body": "You could do all kinds of quantization experiments, or other model efficency stuff. distilation, pruning etc... see how good of a result you can get distilling and quantifying an LLM to something that can run on smaller hardware.\u00a0\n\n\nOf course, if I had 80gb of vram I'd want to train a big model. Maybe id try to train som novel vision transformer stuff.\u00a0",
    "source": "comment"
  },
  {
    "id": "n0mqzq6",
    "title": "",
    "body": "Quantization is an extremely competitive field at the moment, if you really want to publish something, maybe go a bit more niche. If you have domain knowledge that is not ML, that is a good place to start I'd say. F.e. if you know chemistry and ML, you can get far with comparatively little effort, as not many people are strong in both fields.",
    "source": "comment"
  },
  {
    "id": "n0umiut",
    "title": "",
    "body": "I think you can do some work on deep learning model for improving health care. Like cancer detection early. You can see more ideas on using h100: https://superml.dev/ideas",
    "source": "comment"
  },
  {
    "id": "n0l6660",
    "title": "",
    "body": "That's very interesting! Thank you",
    "source": "comment"
  },
  {
    "id": "n0ofz40",
    "title": "",
    "body": "Oh is that so? My knowledge of chemistry ends with 12th grade chemistry but I do maintain a mild interest.  Is there any particular direction you'd suggest? Around protein folding like alpha fold or something else? I actually found two Kaggle competitions that sounded interesting ([RNA Folding](https://www.kaggle.com/competitions/stanford-rna-3d-folding), [Polymer Properties Prediction](https://www.kaggle.com/competitions/neurips-open-polymer-prediction-2025)) but I'm afraid I simply don't have the technical ability",
    "source": "comment"
  },
  {
    "id": "n0lbn1i",
    "title": "",
    "body": "Flagship quantization paper: https://arxiv.org/abs/2402.17764\n\nMany many others as well, but that is a great paper",
    "source": "comment"
  },
  {
    "id": "n0tuxdx",
    "title": "",
    "body": "Ah yeah, that is probably difficult then. But it was just an example, maybe you have other comparatively rare domain knowledge (finance, physics, geosciences, engineering, healthcare, etc etc). As an example from chemistry, a friend of mine did density functional theory calculations with Neural Networks.",
    "source": "comment"
  },
  {
    "id": "n0lwmy8",
    "title": "",
    "body": "Thanks so much!",
    "source": "comment"
  },
  {
    "id": "1lobiuc",
    "title": "[P] I wrote PTX Kernels for LLM.c",
    "body": "Hey everyone,\n\nI\u2019ve been meaning to dive into NVIDIA PTX for a while, and I learn best by doing\u2014so I decided to hand-write PTX kernels for an \\*\\*inference-only\\*\\* version of Andrej Karpathy\u2019s \\[LLM.c\\](https://github.com/karpathy/llama.cpp) project. To my surprise, not only did everything actually work, but I also saw about a \\*\\*10% performance improvement\\*\\* in inference compared to the equivalent CUDA implementation (or at least, that\u2019s what my benchmarks showed).\n\nYou can check out the code here:\n\n\ud83d\udc49 \\[https://github.com/theunnecessarythings/llm-ptx\\](https://github.com/theunnecessarythings/llm-ptx)\n\nAlong the way, I documented my entire experience in a multi-part blog series, including line-by-line explanations of how I translated CUDA into PTX:\n\n1. \\*\\*Part I: Introduction & Residual Kernel\\*\\*\\[https://sreeraj.in/blog/llm-ptx-01\\](https://sreeraj.in/blog/llm-ptx-01)\n2. \\*\\*Part II: The GELU Kernel\\*\\*\\[https://sreeraj.in/blog/llm-ptx-02\\](https://sreeraj.in/blog/llm-ptx-02)\n3. \\*\\*Part III: The Encoder Kernel\\*\\*\\[https://sreeraj.in/blog/llm-ptx-03\\](https://sreeraj.in/blog/llm-ptx-03)\n4. \\*\\*Part IV: The LayerNorm Kernel\\*\\*\\[https://sreeraj.in/blog/llm-ptx-04\\](https://sreeraj.in/blog/llm-ptx-04)\n5. \\*\\*Part V: The Softmax Kernel\\*\\*\\[https://sreeraj.in/blog/llm-ptx-05\\](https://sreeraj.in/blog/llm-ptx-05)\n6. \\*\\*Part VI: The Attention Kernel\\*\\*\\[https://sreeraj.in/blog/llm-ptx-06\\](https://sreeraj.in/blog/llm-ptx-06)\n7. \\*\\*Part VII: The MatMul Kernel & Performance Results\\*\\*\\[https://sreeraj.in/blog/llm-ptx-07\\](https://sreeraj.in/blog/llm-ptx-07)\n\n\\---\n\n\\*\\*What\u2019s Next?\\*\\*\n\nThis is my first time writing PTX, so there may still be bugs or missed optimization opportunities. I\u2019d love feedback or fixes from anyone who\u2019s more experienced with low-level GPU programming!\n\n\\---\n\n\\*\\*Also posted on X:\\*\\*\n\n\\[https://x.com/notHumanIam/status/1939402092071780610\\](https://x.com/notHumanIam/status/1939402092071780610)\n\nLooking forward to your thoughts and suggestions! \ud83d\ude04",
    "source": "post"
  },
  {
    "id": "1logp0w",
    "title": "[D] Looking for a web annotation tool (with Chrome extension) for labeling live websites",
    "body": "I'm building a dataset for a knowledge extraction model and need to label structured data from thousands of live websites. Ideally, I'm looking for a tool that:\n\n\\- Provides a Chrome extension to label live HTML elements on real websites\n\n\\- Can open sites one by one in the browser from a task queue\n\n\\- Saves each annotation along with a snapshot or DOM state of the page\n\n\\- Supports exporting annotations for later review with screenshots\n\nI\u2019m considering building a custom tool for this, but would prefer to avoid that since it would distract from the core research. Does anyone know an existing tool that supports doing what Im doing?",
    "source": "post"
  },
  {
    "id": "1lnem9e",
    "title": "[P] I built a Python debugger that you can talk to",
    "body": "",
    "source": "post"
  },
  {
    "id": "n0eq3kd",
    "title": "",
    "body": "Now ask it an actual question",
    "source": "comment"
  },
  {
    "id": "n0ek8r7",
    "title": "",
    "body": "Check it out: [https://github.com/shobrook/redshift](https://github.com/shobrook/redshift)\n\nThink of this as `pdb` (Python's native debugger) with an LLM inside. When a breakpoint is hit, you can ask questions like:\n\n* \"Why is this function returning null?\"\n* \"How many items in `array` are strings?\"\n* \"Which condition made the loop break?\"\n\nAn agent will navigate the call stack, inspect variables, and look at your code to figure out an answer.\n\nPlease let me know what y'all think!",
    "source": "comment"
  },
  {
    "id": "n0f4muf",
    "title": "",
    "body": "I like the concept and it would be pretty useful tool. However this assumes I run what the whole training loop in debug mode? Also that feels like a contrived solution, what would be useful would be actually understanding when loss is say plateauing or going up and down and giving some insightful feedback there. Here you\u2019re just doing regular code debugging right?",
    "source": "comment"
  },
  {
    "id": "n0emacd",
    "title": "",
    "body": "That's sweet OP. Maybe I will consider \"vibe coding\" once tools like yours become more widespread. I know some SQL but fell flat on my face learning Java.",
    "source": "comment"
  },
  {
    "id": "n0erf2a",
    "title": "",
    "body": "now ask it to give birth",
    "source": "comment"
  },
  {
    "id": "n0f3r7f",
    "title": "",
    "body": "This is a great concept , I will give it try if it really helpful .",
    "source": "comment"
  },
  {
    "id": "n0fkzpj",
    "title": "",
    "body": "[deleted]",
    "source": "comment"
  },
  {
    "id": "n0t3s29",
    "title": "",
    "body": "Great idea, wonderful project!",
    "source": "comment"
  },
  {
    "id": "n0hblo8",
    "title": "",
    "body": "What's the ram consumption on this, currently alot of debuggers such as the one in pycharm destroy the ram of you're reading big data and debugging it.",
    "source": "comment"
  },
  {
    "id": "n0kk5be",
    "title": "",
    "body": "Good idea and you can make it real the code error and fix it automatically",
    "source": "comment"
  },
  {
    "id": "n10skbi",
    "title": "",
    "body": "How is it different from debug-gym?  https://microsoft.github.io/debug-gym/",
    "source": "comment"
  },
  {
    "id": "n0f6x05",
    "title": "",
    "body": "This is great. Maybe this way junior devs will finally start using the debugger.",
    "source": "comment"
  },
  {
    "id": "n0gt5jj",
    "title": "",
    "body": "I got confused for a second there, because there's a AWS redshift service which is a data warehouse.\n\nSo the foundation model is from Anthropic?\n\nYou didn't fine tune it or anything? \n\nDoes it cost any money other than the API key from Anthropic to run your program?",
    "source": "comment"
  },
  {
    "id": "n0i99hr",
    "title": "",
    "body": "Ok but Codex just does this out of the box, and will fix the code for you.",
    "source": "comment"
  },
  {
    "id": "n0fl8d5",
    "title": "",
    "body": "Hey this is a good idea! Though, I wonder if it might be better to expose the debugger as a tool instead for models to use.",
    "source": "comment"
  },
  {
    "id": "n0g8f9y",
    "title": "",
    "body": "Got any suggestions? I can record a new video.",
    "source": "comment"
  },
  {
    "id": "n0ggg3g",
    "title": "",
    "body": "> Think of this as vibe debugging\n\nOh shit! Here we go.\n\nDoes AI have a specific grudge against us?",
    "source": "comment"
  },
  {
    "id": "n0ir8ca",
    "title": "",
    "body": "Yes. Specifically, it can evaluate expressions in the context of a breakpoint.",
    "source": "comment"
  },
  {
    "id": "n0imbrg",
    "title": "",
    "body": "The same as Python\u2019s native debugger, `pdb`.",
    "source": "comment"
  },
  {
    "id": "n0r9ko2",
    "title": "",
    "body": "You can use any model you like, including local ones. And there\u2019s no cost besides inference.",
    "source": "comment"
  },
  {
    "id": "n0irj0o",
    "title": "",
    "body": "Codex is not a debugger.",
    "source": "comment"
  },
  {
    "id": "n0fllkp",
    "title": "",
    "body": "That\u2019s next on my roadmap. This could be an MCP server.",
    "source": "comment"
  },
  {
    "id": "n0wllld",
    "title": "",
    "body": "Trying something that's not so easy to diagnose",
    "source": "comment"
  },
  {
    "id": "n0l21n5",
    "title": "",
    "body": "It definitely is.",
    "source": "comment"
  },
  {
    "id": "n0u3m23",
    "title": "",
    "body": "In fact, this project can be done by any LLM agent, just give him the instruction to use, for example, the \u201cpdb\u201d and connect the MCP server to the shell command line (most agents of the Copilot, Cursor, Windsurf, Claude Code, Codex already have it built-in), but the project itself and the concept is very good!",
    "source": "comment"
  },
  {
    "id": "1lne9e0",
    "title": "[D] Position: Machine Learning Conferences Should\nEstablish a \u201cRefutations and Critiques\u201d Track",
    "body": "**Abstract:**\n\n>Science progresses by iteratively advancing and correcting humanity's understanding of the world. In machine learning (ML) research, rapid advancements have led to an explosion of publications, but have also led to misleading, incorrect, flawed or perhaps even fraudulent studies being accepted and sometimes highlighted at ML conferences due to the fallibility of peer review. While such mistakes are understandable, ML conferences do not offer robust processes to help the field systematically correct when such errors are\u00a0made. This\u00a0position paper argues that ML conferences should establish a dedicated \"Refutations and Critiques\" (R & C) Track. This R & C Track would provide a high-profile, reputable platform to support vital research that critically challenges prior research, thereby fostering a dynamic self-correcting research ecosystem. We discuss key considerations including track design, review principles, potential pitfalls, and provide an illustrative example submission concerning a recent ICLR 2025 Oral. We conclude that ML conferences should create official, reputable mechanisms to help ML research self-correct.\n\n(I'm not affilated with any of the authors. But I believe this position paper deserves more visibility)",
    "source": "post"
  },
  {
    "id": "n0gyu3p",
    "title": "",
    "body": "I think we should do more reproducing of other works. As it stands, you couldn't get that published, especially if cou confirmed results",
    "source": "comment"
  },
  {
    "id": "n0gzhxd",
    "title": "",
    "body": "I absolutely love this concept. Challenging ideas in previous papers (especially popular/respected work) is incredibly important in every branch of science.\n\nObviously you need significant factual results to show that something is \"bad\" (not as good as previously thought), but papers in these categories are usually more interesting to me than papers inventing something new",
    "source": "comment"
  },
  {
    "id": "n0fo34r",
    "title": "",
    "body": "Thats just the openreview reviews page.",
    "source": "comment"
  },
  {
    "id": "n14y53n",
    "title": "",
    "body": "Absolutely. There are few challenges on reproduction though:\n\n\\- incentive and opportunity cost - if I had time to reproduce, why wouldn't I just publish a new paper?\n\n\\- llm decoding is not deterministic due to finite precision even if temperature=0.0, this could be mitigated by using standard error. But standard error is just not common in ML community.\n\n\\- cost, particularly for pretraining/ postraining",
    "source": "comment"
  },
  {
    "id": "n0k7gsy",
    "title": "",
    "body": "Here's an example paper that would fit into authors' vision (not least because authorship overlaps): [https://arxiv.org/pdf/2506.13681](https://arxiv.org/pdf/2506.13681)\n\nHave you ever seen anything similar on the openreview? I haven't.",
    "source": "comment"
  }
]